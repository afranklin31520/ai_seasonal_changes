,title,author,pub_date,url,abstract,full_text,key_words,data_source
0,US Treasury Says AI Tools Prevented $1 Billion of Fraud in 2024,Todd Feathers,2024-10-17,https://gizmodo.com/us-treasury-says-ai-tools-prevented-1-billion-of-fraud-in-2024-2000513080,"The Treasury says it identified or recovered $4 billion in fraudulent payments last fiscal year, a sixfold increase over the previous year.","The U.S. Department of the Treasury says its expanded use of machine learning systems helped detect and prevent billions of dollars in fraudulent payments in 2024. The treasury is the check-writer for many federal programs and annually processes around 1.4 billion payments worth $6.9 trillion for programs like Social Security and Medicaid. During the most recent fiscal year, which ended in September, the agency’s new data-driven approach to rooting out bad actors contributed to the prevention and recovery of more than $4 billion in fraudulent payments, according to a press release. That’s a more than sixfold increase over the $652.7 million in fraudulent payments detected or recovered during the 2023 fiscal year. The agency credited the increase to its new data-driven approach to fraud detection. That includes using machine learning to identify instances of fraud and to prioritize high-risk transactions for further investigation. The Treasury has also partnered with other federal and state agencies to share information through its Do Not Pay database and other payment integrity tools. âTreasury takes seriously our responsibility to serve as effective stewards of taxpayer money. Helping ensure that agencies pay the right person, in the right amount, at the right time is central to our efforts,â Treasury Deputy Secretary Wally Adeyemo said in a statement. âWeâve made significant progress during the past year in preventing over $4 billion in fraudulent and improper payments. We will continue to partner with others in the federal government to equip them with the necessary tools, data, and expertise they need to stop improper payments and fraud.â While $4 billion in fraudulent payments prevented or recovered is no small amount, it pales in comparison to the government’s estimates of how much fraud occurs. In April, the federal Government Accountability Office estimated that federal agencies lose between $233 billion and $521 billion annually to fraud. The GAO report recommended that the Treasury, due to its central role in processing payments, should better leverage data analytics tools. Both government agencies and financial institutions have increasingly come to rely on machine learning algorithms to identify fraudulent actors. These systems use a wide range of data about payment recipientsâincluding details about their bank accounts, physical addresses, IP addresses, demographic information, usernames, and passwordsâto identify patterns linked with fraud. As the Treasury has noted in previous reports about financial sector fraud, that kind of “historical data used to train fraud-detection models could contain biases, such as the overrepresentation of certain demographics in anti-fraud cases.” AiDepartment of the TreasuryFraud Get the best tech, science, and culture news in your inbox daily. News from the future, delivered to your present. Please select your desired newsletters and submit your email to upgrade your inbox. Amazon, Google, and Microsoft are all pushing for a nuclear-power fueled future. It's a huge gamble. These are all the Apple Intelligence features coming to iPhone on October 28. A business school is using AI doomerism, money from Saudi Arabia, and a dusty Cold War metaphor to get people hyped about AIâs future. A series of reports from multiple federal agencies has outlined the challenges it faces in the rush to adopt AI. The robots were interacting with guests at Tesla's big Cybercab event Thursday in L.A. Russian state media and right-wing influencers on X helped share disinformation about Disney.  Discover the Winners of the 2024 Gizmodo Science Fair â We may earn a commission when you buy through links on our sites.
Â©2024 GIZMODO USA LLC. All rights reserved. Mode Follow us Mode Follow us ","fraudulent, right, machine, prevented, tools, data, ai, 2024, billion, payments, treasury, federal, agencies, fraud",Gizmodo
1,"For the Love of God, Stop Making Inscrutable Doomsday Clocks",Matthew Gault,2024-10-15,https://gizmodo.com/for-the-love-of-god-stop-making-inscrutable-doomsday-clocks-2000512111,"A business school is using AI doomerism, money from Saudi Arabia, and a dusty Cold War metaphor to get people hyped about AIâs future.","A Saudi-backed business school in Switzerland has launched a Doomsday Clock to warn the world about the harms of âuncontrolled artificial general intelligence,â what it calls a âgod-likeâ AI. Imagine if the people selling offices on Excel spreadsheets in the 1980s tried to tell workers that the software was a pathway to birthing a god and used a ticking Rolex to do it and youâll have an idea of what weâre dealing with here. Michael Wadeâthe clock’s creator and a TONOMUS Professor of Strategy and Digital at IMD Business School in Lausanne, Switzerland, and Director of the TONOMUS Global Center for Digital and AI Transformation (good lord)âunveiled the clock in a recent op-ed for TIME. A clock ticking down to midnight is a once-powerful and now stale metaphor from the atomic age. Itâs an image so old and stayed that it just celebrated its 75th anniversary. After America dropped nukes on Japan, some researchers and scientists whoâd worked on developing the weapon formed the Bulletin of the Atomic Scientists. Their project has been to warn the world of its impending destruction. The Doomsday Clock is one of the ways they do it. Every year experts in various fieldsâfrom nuclear weapons to climate change to, yes, artificial intelligenceâgather and discuss just how screwed the world is. Then they set the clock. The closer to midnight, the closer humanity is to its doom. Right now itâs at 90 seconds to midnight, the closest the clock has ever been set. Wade and IMD have no relation to the Bulletin of the Atomic Scientists and the Doomsday Clock is its own thing. Wadeâs creation is the AI Safety Clock. âThe Clockâs current readingâ29 minutes to midnightâis a measure of just how close we are to the critical tipping point where uncontrolled AGI could bring about existential risks,â he said in his Time article. âWhile no catastrophic harm has happened yet, the breakneck speed of AI development and the complexities of regulation mean that all stakeholders must stay alert and engaged.â Silicon Valleyâs loudest AI proponents love to lean into the nuclear metaphor. OpenAI CEO Sam Altman compared the work of his company to the Manhattan Project. Senator Edward J. Markey (D-MA) wrote that Americaâs rush to embrace AI is similar to Oppenheimerâs pursuit of the atomic bomb. Some of this fear and concern might be genuine but itâs all marketing at the end of the day. Weâre in the middle of a hype cycle around AI. Companies are promising it can deliver unprecedented returns and destroy labor costs. Machines, they say, will soon do everything for us. The reality is that AI is useful but also mostly moves labor and production costs to other parts of the chain where the end user doesnât see it. The fear of AI becoming so advanced that it wipes humanity out is just another kind of hype. Doomerism about word calculators and predictive modeling systems is just another way to get people excited about the possibilities of this technology and mask the real harm it creates. At a recent Tesla event, robot bartenders poured drinks for attendees. By all appearances, they were controlled remotely by humans. LLMs burn a ton of water and electricity when coming up with answers and often rely on the subtle and constant attention of human âtrainersâ who labor in poor countries and work for a pittance. Humans use the tech to flood the internet with non-consensually created nude images of other humans. These are just a few of the real-world harms already caused by Silicon’s rapid embrace of AI. And as long as youâre afraid of Skynet coming to life and wiping out humanity in the future, you arenât paying attention to the problems right in front of you. The Bulletinâs Doomsday Clock may be inscrutable on the surface, but thereâs an army of impressive minds behind the metaphor churning out work every day about the real risks of nuclear weapons and new technologies. In September, the Bulletin put a picture of Altman in an article debunking hyperbolic claims about how AI might be used to engineer new bioweapons. âFor all the doomsaying, there are actually many uncertainties in how AI will affect bioweapons and the wider biosecurity arena,â the article said. It also stressed that talk of extreme scenarios around AI helps people avoid having more difficult conversations. âThe challenge, as it has been for more than two decades, is to avoid apathy and hyperbole about scientific and technological developments that impact biological disarmament and efforts to keep biological weapons out of the war plans and arsenals of violent actors,â the Bulletin said. âDebates about AI absorb high-level and community attention and â¦ they risk an overly narrow threat focus that loses sight of other risks and opportunities.â There are dozens of articles like this published every year by the people who run the Doomsday Clock. The Swiss AI clock has no such scientific backing, though it claims to be monitoring such articles in its FAQ. What it has, instead, is money from Saudi Arabia. Wadeâs position at the school is possible thanks to funding from TONOMUS, a subsidiary of NEOM. NEOM is Saudi Arabiaâs much-hyped city of the future that itâs attempting to build in the desert. Among the other promises of NEOM are robot dinosaurs, flying cars, and a giant artificial moon. Youâll forgive me if I donât take Wade or the AI Safety Clock seriously. AiElon Muskmanhattan projectNEOMnuclear Get the best tech, science, and culture news in your inbox daily. News from the future, delivered to your present. Please select your desired newsletters and submit your email to upgrade your inbox. The video of Optimus is sped up 2 times, 8 times, and even 10 times faster at various points. The Treasury says it identified or recovered $4 billion in fraudulent payments last fiscal year, a sixfold increase over the previous year. The European Union is making a decision that may cause any potential fines against Musk to skyrocket. Amazon, Google, and Microsoft are all pushing for a nuclear-power fueled future. It's a huge gamble. The Tesla CEO called his PAC ""centrist"" because you can just say whatever you want on the internet. Tesla's bulky and sharp-cornered Cybertruck is a disaster waiting to happen, the safety organizations claim.  Discover the Winners of the 2024 Gizmodo Science Fair â We may earn a commission when you buy through links on our sites.
Â©2024 GIZMODO USA LLC. All rights reserved. Mode Follow us Mode Follow us ","clock, weapons, atomic, inscrutable, making, tonomus, work, school, ai, love, stop, bulletin, god, doomsday, world, clocks",Gizmodo
2,Parents Sue School That Gave Bad Grade to Student Who Used AI to Complete Assignment,Todd Feathers,2024-10-15,https://gizmodo.com/parents-sue-school-that-gave-bad-grade-to-student-who-used-ai-to-complete-assignment-2000512000,A Massachusetts couple claims that their son's high school attempted to derail his future by giving him detention and a bad grade on an assignment he wrote using generative AI.,"An old and powerful force has entered the fraught debate over generative AI in schools: litigious parents angry that their child may not be accepted into a prestigious university. In what appears to be the first case of its kind, at least in Massachusetts, a couple has sued their local school district after it disciplined their son for using generative AI tools on a history project. Dale and Jennifer Harris allege that the Hingham High School student handbook did not explicitly prohibit the use of AI to complete assignments and that the punishment visited upon their son for using an AI toolâhe received Saturday detention and a grade of 65 out of 100 on the assignmentâhas harmed his chances of getting into Stanford University and other elite schools. “The defendants continued on a pervasive, destructive and merciless path of threats, intimidation and coercion to impact and derail [our son’s] future and his exemplary record,” the Harris family alleges in its lawsuit, which was initially filed in state superior court before being removed to a federal district court. Hingham Public Schools, however, claims that its student handbook prohibited the use of “unauthorized technology” and “unauthorized use or close imitation of the language and thoughts of another author and the representation of them as one’s own work.” The district said in a recent motion to dismiss that the discipline administered to the Harris’ son was “relatively lenient” and that a ruling to the contrary would “invite dissatisfied parents and students to challenge day-to-day discipline, even grading of students, in state and federal courts.” Almost immediately after OpenAI released ChatGPT in 2022,Â  schools recognized the threat that free and easily accessible generative AI tools posed to academic integrity. Some districts tried banning the technology entirely for students and then reversed course. State departments of education have slowly been rolling out guidance for local districts, but in many parts of the country, there is no clear consensus on how students should be allowed to use generative AI. A national survey conducted by the Center for Democracy and Technology found that schools are increasingly disciplining students for using AI and noted that historically marginalized studentsâincluding students of color and English language learnersâtend to be punished disproportionately for violating school rules, The Harris family alleges that their son was unfairly targeted by the Hingham school district because it applied discipline inconsistently. After the cheating incident, the district didn’t place their son in the National Honor Society, they claim, but it had previously allowed a student who used AI to write an English paper to join the society. The district replied in its motion to dismiss that the Harris’s son has, in fact, been allowed to join the National Honor Society after initially being deferred. The lawsuit also questions whether using AI to complete assignments should be prohibited at all. It notes that the Massachusetts Department of Elementary and Secondary Education hasn’t issued any rules or guidance for schools on the use of the technology. “Generative AI is an emerging landscape and its use is here to stay,” according to the lawsuit. generative aiLawsuitMassachusettsSchool Get the best tech, science, and culture news in your inbox daily. News from the future, delivered to your present. Please select your desired newsletters and submit your email to upgrade your inbox. You're also eligible if you have used Cash App at any point since 2018. The social media giant's Creator Bonus Program has created a world where AI-generated spam is a business. As is the case with genAI in other industries, comics pros are not happy about the technology intruding on their space. Denying Disney's motion to dismiss the case, a California judge has laid the groundwork for the Cara Dune actress to get her date in court. The lawsuit, filed in Delaware, alleges that the company's former execs including ex-CEO Parag Agrawal paid $1 million in legal fees out of their own pockets. A class action lawsuit claims Joshua Browder's start-up misled consumers by peddling ""substandard legal documents"" and providing ""unauthorized legal services.""  Discover the Winners of the 2024 Gizmodo Science Fair â We may earn a commission when you buy through links on our sites.
Â©2024 GIZMODO USA LLC. All rights reserved. Mode Follow us Mode Follow us ","schools, assignment, harris, technology, ai, son, gave, complete, student, school, students, parents, using, district, used, generative, bad, sue, grade",Gizmodo
3,The Federal Government Is Too Broke and Stupid for the AI Revolution,Matthew Gault,2024-10-11,https://gizmodo.com/the-federal-government-is-too-broke-and-stupid-for-the-ai-revolution-2000511094,A series of reports from multiple federal agencies has outlined the challenges it faces in the rush to adopt AI.,"Love it or hate it, AI is blooming in offices across the world. Unless those offices are full of employees working for the U.S. federal government. A new report from Fedscoop casts doubt on Washingtonâs ability to keep up with the times. According to its analysis of multiple federal agencies, D.C. doesnât have the funds or the talent to train to keep up with AI. Last year, President Joe Biden watched Mission: ImpossibleâDead Reckoning Part One and got freaked out about AI. It scared him so bad that he signed an executive order that called on tech companies to develop the tech responsibly. The White House also called on federal agencies to issue reports that would detail how they plan to use AI, their plans to mitigate risks against humanity, and to elaborate on what barriers stood in the way of mass adoption of AI. The White House wanted these reports posted publicly by September. Most of the federal agencies replied and Fedscoop has collected them all in one place and found a common theme. According to Fedscoop, twenty-nine agencies submitted reports. A dozen of them mentioned data hurdles, six mentioned a lack of AI-trained employees, and six said that a lack of funding was hurting their AI initiatives. The Department of Energy, which is in charge of the nationâs nuclear arsenal, complained that understandable security concerns around cloud services are preventing it from ramping up its adoption of AI. It also doesnât have enough graphics cards. âThe IT infrastructure barrier extends beyond the serverless CSP services to the availability and timeliness of securing virtual machines with the requisite Graphics Processing Unit (GPU) hardware to develop, train, manage, and deploy advanced AI models,â a DOE report on its adoption of AI said. âThis challenge is industry-wide; however, it will impact the rollout and adoption of more advanced customized use cases that require dedicated GPU hardware.â The recurring data problem is a big one. Many of these federal agencies have been around for decades. People drift in and out, some sticking around for years and others cycling in and out with each new presidential administration. Many of the technological systems at these agencies are ad-hoc in nature. Things get replaced when they absolutely have to be, but not before. A great example of this is Americaâs nuclear weapons systems. The Air Force used enormous eight-inch floppy discs to run the software governing nuclear command and control until 2019. When Colin Powell became Secretary of State under George W. Bush in 2001, he discovered that his office was full of pre-internet era computers designed by a company that had gone bankrupt in 1992. The mass adoption of AI is forcing D.C. to face similar technology challenges all at once across all its departments. The data problem, repeated over a dozen reports, is a reflection of this ad-hoc build-up. Training internal LLMs for government use requires data to be centralized and secure. In many of these agencies, data is distributed in hundreds of different places and few off-the-shelf solutions are secure enough for government work. Another theme was a lack of understanding of AI in the workforce and an outright fear of it. The Nuclear Regulatory Commission, which deals with nuclear power plants and radioactive materials, said its workforce was interested but âit has also expressed trepidation as well as a general lack of knowledge of AI capabilities. To address this, the agency must continue to enable effective change management to enable the workforce to take full advantage of AI capabilities as they are introduced.â Many of the agencies also complained about funding. The NRC said it was âonly able to assess, test, implement, and maintain new capabilities where resources have been made available to do so.â âAI use cases compete for funding and staffing with other important priorities at the Bank including non-IT investments in core EXIM capabilities, cyber security, and other use cases in our modernization agenda,â the Export-Import Bank of the U.S. said in its report. Itâs going to be expensive and time-consuming for these agencies to catch up with the rest of the country. Back in 2001, Powell had to buy everyone in his office a computer, 44,000 machines to hear him tell it. âDonât ask me how I got the money, because I wonât tell you,â he said at an IT symposium in 2019. AiLLMWashington DC Get the best tech, science, and culture news in your inbox daily. News from the future, delivered to your present. Please select your desired newsletters and submit your email to upgrade your inbox. The Treasury says it identified or recovered $4 billion in fraudulent payments last fiscal year, a sixfold increase over the previous year. Amazon, Google, and Microsoft are all pushing for a nuclear-power fueled future. It's a huge gamble. These are all the Apple Intelligence features coming to iPhone on October 28. A business school is using AI doomerism, money from Saudi Arabia, and a dusty Cold War metaphor to get people hyped about AIâs future. The robots were interacting with guests at Tesla's big Cybercab event Thursday in L.A. Russian state media and right-wing influencers on X helped share disinformation about Disney.  Discover the Winners of the 2024 Gizmodo Science Fair â We may earn a commission when you buy through links on our sites.
Â©2024 GIZMODO USA LLC. All rights reserved. Mode Follow us Mode Follow us ","lack, revolution, data, stupid, broke, workforce, ai, adoption, federal, agencies, capabilities, nuclear, reports",Gizmodo
4,"Congressman Refuses to Debate, so His Opponent Is Using an AI Stand-In",Tom Maxwell,2024-10-09,https://gizmodo.com/congressman-refuses-to-debate-so-his-opponent-is-using-an-ai-stand-in-2000509810,"Sitting congressman Don Beyer wonât bother debating Bentley Hensel, a long-shot independent candidate, so Hensel made an AI version of Beyer to debate instead.","Letâs say you want to make a long-shot run for Congress as an independent in a deeply Democratic district of Virginia, but the incumbent in the race wonât give you airtime to debate your respective platforms in front of voters. You could simply run social media ads promoting your candidacy, or maybe present your case through a YouTube video in which you argue why you’re a better choice. Maybe contact the local newspaper to try and get an interview? Or you could just use generative AI that’s been trained on the incumbent’s backlog of public comments and published material.Â Thatâs what Bentley Hensel intends to do in Virginiaâs 8th congressional district, where heâs running as an independent challenger against Don Beyer, who has brushed off requests to do another debate, saying a September forum was sufficient. Beyer won the district in 2022 by nearly two-thirds, so it’s not hard to see why he would feel it better to just ignore his relatively unknown challenger. It may be hard for some to suspend disbelief listening to a simulacrum of a real person. But ChatGPTâs Advanced Voice Mode is so uncanny in its ability to speak in a natural and expressive way that itâs hard to even distinguish it from a real person. And companies like Character.ai and Replika are wildly popular for offering chatbots as forms of companionship. Itâs not crazy to believe that voters, especially older ones, would be lulled into feeling they’re watching a genuine debate.Â Hensel told Reuters that DonBot, as the bot is called, is being trained using ChatGPTâs API on Beyerâs official websites, press releases, and data from the Federal Election Commission. The bot is intended to provide accurate answers, though thatâs not exactly reassuring to anyone who has actually used a chatbot before. And sure enough, Reuters tested DonBot and found that although it largely provided straightforward answers to policy questions, it erred in saying Beyer has not endorsed anyone for president when he has in fact endorsed Democratic candidate Kamala Harris. Still, having been trained on a narrow data set, the bot is less likely to produce wild hallucinations.Â Beyer did not tell Reuters whether or not heâll take action to stop the online debate, taking place on October 17, but a spokesperson told the outlet that Beyer “continues to be a leading voice in Congress on the need to improve artificial intelligence regulation, including legislation to prevent nefarious actors from utilizing AI to spread election misinformation.” So, it sounds like heâs not exactly enthused.Â Legal experts who spoke with Reuters said that the use of the bot is likely permissible so long as Hensel offers clear disclosure that heâs not actually speaking to the real Don Beyer. For Hensel, the attention he’s getting from creating the bot alone has probably been worth a lot. It seems slippery, however, if older voters and others come to wrongly believe a bot that is likely going to make some mistakes genuinely represents a candidate. We know it’s already easy enough for older generations to fall for AI-generated imagery.Â At least 26 states have moved to take action regulating the use of generative AI in communications around elections, some going as far as to outright ban deepfakes of politicians. At the federal level, there has been, unsurprisingly, little movement. Â Artificial intelligenceDeepfakes Get the best tech, science, and culture news in your inbox daily. News from the future, delivered to your present. Please select your desired newsletters and submit your email to upgrade your inbox. The robots were interacting with guests at Tesla's big Cybercab event Thursday in L.A. Russian state media and right-wing influencers on X helped share disinformation about Disney. From selling crypto to miracle cures, deepfake celebrity ads are everywhere. John Hopfield and Geoffrey Hinton developed artificial neural networks that laid the foundation for modern recommendation systems and generative AI. Musk also claimed liberal states were ""making crime legal"" during an interview with Tucker Carlson. OpenAI is now valued at $157 billion. A close read of OpenAI's statement might provide some hints about the future.  Discover the Winners of the 2024 Gizmodo Science Fair â We may earn a commission when you buy through links on our sites.
Â©2024 GIZMODO USA LLC. All rights reserved. Mode Follow us Mode Follow us ","real, hensel, beyer, hard, congressman, ai, opponent, refuses, likely, hes, trained, older, debate, using, voters, bot, standin",Gizmodo
5,What the Hell Is âAgenticâ AI on the new MediaTek Dimensity Flagship Mobile Chip?,Kyle Barr,2024-10-09,https://gizmodo.com/what-the-hell-is-agentic-ai-on-the-new-mediatek-dimensity-flagship-mobile-chip-2000509386,"Every chipmaker is going for broke on AI, but Mediatekâs Dimensity 9400 SoC pushes the idea of ""agentic"" AI that's capable of controlling your phone for you.","Chipmaker MediaTek is being explicit about what our phonesâ futures will entail. In its best Elmo impression, MediaTek is asking, “Can you say the word âagentic?â What does that mean, exactly? For its newly announced flagship 3nm chip, the Dimensity 9400, the focus is now on how AI assistants can work across apps and even control your phone for you. To break it down, âagenticâ refers to AI agents. Think of those as multiple AI models working in tandem to complete a task. Imagine you put a prompt into an AI chatbot, telling it to call your mother and wish her a happy birthday on your behalf. Various AI models will create the birthday message, look into your contacts for your motherâs number, and then put in the call using an AI-generated voice. Agents are a buzzword among the higher echelons of Silicon Valley, but the term âagenticâ canât leap off the concept sheet and onto our phones without the underlying technology behind it. The Dimensity 9400 is just the first chip officially revealed this year, and it claims it can support agents that are supposed to power the supercharged assistants on our phones. It claims to support 80% better LLM performance than the Dimensity 9300 and twice as good AI art generation with a new neural processor. The other end of that is a so-called âDimensity Agentic AI Engineâ thatâs made to help developers set a work path for various AI-focused apps. Taiwan-based MediaTek claims its chips can do around 50 tokens per second for AI computation. Compare that with the current Qualcomm Snapdragon 8 Gen 3âs 20 tokens per second, though rumors and leaks suggest Qualcomm will reveal Gen 4 later this month. None of the current leaks, like those onÂ SmartPrixÂ andÂ WCCFTech, show comparisons between both chipsâ NPUs. The Dimensity 9400 is an âAll Big Coreâ design with a single ARM cortex-X925 with a 3.62 GHz clock speed, along with three Cortex-X4 and four Cortex-A720 cores. Itâs supposed to get 35% better single-core and 28% better multi-core performance than the 9300. It supports a 100% increase in the L2 cache and a 50% improvement in the L3 cache compared to the Dimensity 9300. Itâs also supporting a 12-core ARM Immortalis-G925 GPU with better ray tracing support. Otherwise, the new MediaTek chipÂ supports LPDDR5X memory at 10.7 GBps and is supposed to improve power efficiency. Funnily enough, it claims to support âtri-fold smartphones,â of which thereâs currently only theÂ Huawei Mate XTÂ to contend with. For the most part, the Dimensity 9300 and 9300+ were tied to brands you wonât find in the U.S., such as Vivo, Oppo, and Redmi.Â As revealed last month, that CPU now also supports theÂ Samsung Galaxy Tab S10 Ultra and S10+.Â But as for other U.S.-based products, the OnePlus Pad 2 eschewed the originalâs Dimensity 9000 chip for the Qualcomm Snapdragon 8 Gen 3. Unless U.S. lawmakersâ attitude to international chipmakers changes sometime soon (it wonât), MediaTekâs chips will still be relatively rare for products in the U.S. But that belies just how all chipmakers, not just those in the U.S., are tuning their chips for our upcoming AI assistants. Googleâs Gemini is already taking root on Android, and Apple Intelligence is set to do a complete makeover of iOS on iPhones over the next few months. The point is, it doesnât matter which brand you buy; youâre going to hear the word âagenticâ a lot more in 2025 and beyond. The real question for 2025 is whether agentic AI will work as advertised. So far, weâve seen AI models perfectly capable of handling intense tasks, but those are often processed off-device. So far, on-device AI is simply offering a more complicated assistant apparatus that’s capable of lying to you. The real challenge will be having this AI capable of reaching across apps and performing tasks without calling your mother to tell her something you’ll need to apologize for later. Update 10/9/2024 at 9:35 a.m.:Â This post was updated to correct MediaTek’s main base of operations. AiAppleGoogleMediaTekMediaTek Dimensity Get the best tech, science, and culture news in your inbox daily. News from the future, delivered to your present. Please select your desired newsletters and submit your email to upgrade your inbox. The Treasury says it identified or recovered $4 billion in fraudulent payments last fiscal year, a sixfold increase over the previous year. This tier costs half as much as the regular Premium but doesn't offer most of the perks. You can now find multiple listings for prefabricated homes on Walmart for under $16,000, but you also need to remember that home ownership will never be cheap. Amazon, Google, and Microsoft are all pushing for a nuclear-power fueled future. It's a huge gamble. These are all the Apple Intelligence features coming to iPhone on October 28. A business school is using AI doomerism, money from Saudi Arabia, and a dusty Cold War metaphor to get people hyped about AIâs future.  Discover the Winners of the 2024 Gizmodo Science Fair â We may earn a commission when you buy through links on our sites.
Â©2024 GIZMODO USA LLC. All rights reserved. Mode Follow us Mode Follow us ","chip, flagship, claims, 9300, work, ai, hell, dimensity, chips, better, mediatek, agentic, mobile",Gizmodo
6,"Scarlett Johansson, Kylie Jenner and Taylor Swift Top List of Celebrities Used for AI Scams",Matt Novak,2024-10-08,https://gizmodo.com/scarlett-johansson-kylie-jenner-and-taylor-swift-top-list-of-celebrities-used-for-ai-scams-2000509158,"From selling crypto to miracle cures, deepfake celebrity ads are everywhere.","You’re probably seen the ridiculous videos on social media. There’s a celebrityâmaybe it’s Sydney Sweeney or Tom Hanksâtalking directly to the camera about some product, but something seems a bit off. Maybe it’s the fact that their mouth doesn’t seem to be moving perfectly in sync with their words or maybe it’s that Hanks is trying to sell some sketchy-looking medication you’ve never heard of. Well, you’re right to be skeptical. Thanks to artificial intelligence technology, celebrities are being used left and right in AI deepfake videos and images for all kinds of scams, hawking products they’ve never actually endorsed. And cybersecurity company McAfee has a new list rounding up the most commonly used celebrity likenesses of the past year. Scarlett Johnasson tops the list, which is particularly interesting given her advocacy against non-consensual AI content. Johnasson said she was approached by OpenAI to provide a voice for their robotic assistant tech, but the actress declined. OpenAI went ahead and made a sound-alike voice for one of the company’s demos but seems to have scrapped the voice after Johansson threatened a lawsuit. Other big names on the list include Kylie Jenner, Taylor Swift, and Tom Hanks, among others. Hanks wrote a note on Instagram back in August warning that AI fakes were using his likeness to sell medications. “There are multiple ads over the internet falsely using my name, likeness, and voice promoting miracle cures and wonder drugs,” Hanks wrote. “These ads have been created without my consent, fraudulently and through AI. I have nothing to do with these posts or the products and treatments, or the spokespeople touting these cures.” Hanks went on to explain that he has type-2 diabetes and said, “I ONLY work with my board-certified doctor regarding my treatment,” ending his message with an all-caps warning: “DO NOT BE FOOLED. DO NOT BE SWINDLED. DO NOT LOSE YOUR HARD EARNED MONEY.” Because ultimately, that’s what it’s all about. Making money by fraudulently piggybacking off the likeness of well-known people. The list from McAfee, with the company’s explanations: The one celebrity who’s not on the list but we at Gizmodo see most often? That would be Elon Musk. We even obtained consumer complaints filed with the FTC earlier this year about crypto scams using Musk’s face. They’re everywhere. The folks at McAfee warn that as AI gets better, it will be harder and harder to tell what is a deepfake and what is the real thing. So people will just need to remain vigilant and try to apply some critical thinking skills whenever a supposed celebrity endorsement pops into their social media feed. âIn a time when celebrity news is part of everyday conversation and accessible with the click of a button, people often prioritize convenience over online safety, clicking on suspicious links promising celebrity content or related goods,” Abhishek Karnik, McAfeeâs Head of Threat Research, said in a statement published online. “But if it sounds too good to be true, itâs worth a second look. With cybercriminals using advanced AI tools to create more convincing scams, the risks are growing, and celebrity names are the perfect bait for curious consumers. Thatâs why people need to stay vigilant and think twice before clicking.” Get the best tech, science, and culture news in your inbox daily. News from the future, delivered to your present. Please select your desired newsletters and submit your email to upgrade your inbox. The campaign pushes users to IWillVote.com to make sure they're registered. The video of Optimus is sped up 2 times, 8 times, and even 10 times faster at various points. The Treasury says it identified or recovered $4 billion in fraudulent payments last fiscal year, a sixfold increase over the previous year. The European Union is making a decision that may cause any potential fines against Musk to skyrocket. Amazon, Google, and Microsoft are all pushing for a nuclear-power fueled future. It's a huge gamble. The Tesla CEO called his PAC ""centrist"" because you can just say whatever you want on the internet.  Discover the Winners of the 2024 Gizmodo Science Fair â We may earn a commission when you buy through links on our sites.
Â©2024 GIZMODO USA LLC. All rights reserved. Mode Follow us Mode Follow us ","voice, hanks, used, scams, likeness, ai, scarlett, actress, johansson, list, taylor, celebrities, kylie, jenner, celebrity, swift, permission",Gizmodo
7,Nobel Prize Goes to ‘Godfathers of AI’ Who Now Fear Their Work Is Growing Too Powerful,Todd Feathers,2024-10-08,https://gizmodo.com/nobel-prize-goes-to-godfathers-of-ai-who-now-fear-their-work-is-growing-too-powerful-2000509098,John Hopfield and Geoffrey Hinton developed artificial neural networks that laid the foundation for modern recommendation systems and generative AI.,"Two AI researchers, John Hopfield and Geoffrey Hinton, received the Nobel Prize in physics on Monday for their work building artificial neural networks that can memorize information and recognize patterns in ways that mimic the human brain. Their research in the 1980s laid the foundation for the last decadeâs explosive progress in artificial intelligence and todayâs ubiquitous recommendation algorithms and generative AI systems. Both men have since said that progress needs to be constrained for the sake of humanity. In its announcement of the prize, the Nobel committee emphasized how far the field has come since Hopfield published his seminal paper in 1982, in which he described a neural network with fewer than 500 possible parameters. Today, tech companies are churning out generative AI systems with billions and trillions of parameters. The Hopfield network was a collection of 30 interconnected digital nodes that could change their values between 1 and 0 and in doing so be programmed to record patterns that represented the pixels in black and white images. It drew on equations from physics used to describe how atoms in a network affect each othersâ spin in order to calculate how the relationships between the nodes in the network represented the images. In effect, the network could be programmed to create memories of certain images. And when it was fed a new image that was fuzzy or incomplete, it could calculate its way back to the most similar image in its memory. Hinton built on Hopfieldâs work by designing neural networks that could not just remember and recreate patterns, but could be taught to recognize similar patterns in entirely different dataâfor example, the patterns that make one picture of a dog like another but not like a picture of a cat. In 1985 he published a paper introducing this network, named a Boltzmann machine after physicist Ludwig Boltzmann, who developed statistical equations for calculating the collective properties of a network composed of many different components. In 2023, Hopfield was one of the most notable signatories on a letter calling for AI companies to pause the development of generative AI systems more powerful than OpenAIâs GPT-4. And Hinton has recently been talking a lot about his concerns that AI is advancing too rapidly for humans to control. Heâs estimated that humans could build an artificial intelligence that exceeds our own intelligence in the next five to 20 years and that âIt’ll figure out ways of manipulating people to do what it wants.â He was one of several big names in the field to sign an open letter this year calling for California Governor Gavin Newsom to enact a law that would have held large tech companies liable for building AI models that caused catastrophic losses of life or property damage. Newsom ultimately vetoed the bill, under pressure from tech companies, including Hintonâs former employers at Google. âIn the next few years we need to figure out if thereâs a way to deal with that threat,â Hinton said in an interview with the Nobel Prize committee following the award announcement. âSo I think itâs very important right now for people to be working on the issue of how will we keep control. We need to put a lot of research effort into it. I think one thing governments can do is force the big companies to spend a lot more of their resources on safety research.â Artificial intelligenceGeoffrey HintonJohn Hopfieldnobel prize Get the best tech, science, and culture news in your inbox daily. News from the future, delivered to your present. Please select your desired newsletters and submit your email to upgrade your inbox. The robots were interacting with guests at Tesla's big Cybercab event Thursday in L.A. Russian state media and right-wing influencers on X helped share disinformation about Disney. Sitting congressman Don Beyer wonât bother debating Bentley Hensel, a long-shot independent candidate, so Hensel made an AI version of Beyer to debate instead. From selling crypto to miracle cures, deepfake celebrity ads are everywhere. Musk also claimed liberal states were ""making crime legal"" during an interview with Tucker Carlson. OpenAI is now valued at $157 billion. A close read of OpenAI's statement might provide some hints about the future.  Discover the Winners of the 2024 Gizmodo Science Fair â We may earn a commission when you buy through links on our sites.
Â©2024 GIZMODO USA LLC. All rights reserved. Mode Follow us Mode Follow us ","prize, ai, growing, goes, nobel, companies, tech, systems, patterns, research, powerful, fear, work, network, hopfield, godfathers",Gizmodo
8,Douglas Rushkoff Knows He Fucked Up,Matthew Gault,2024-10-04,https://gizmodo.com/douglas-rushkoff-knows-he-fucked-up-2000507544,"Douglas Rushkoff's new book will help you navigate the weird and wild digital world we all live in now, and just might help you survive what's coming.","Douglas Rushkoff feels responsible. Rushkoff is a Gen-X writer and thinker who was part of the initial wave of tech advocates in the 1990s who sold the world on a future online âI was part of the initial wave of people who said: âCome on in, the waterâs fine,ââ he tells me. Three decades on, the world looks very different. Corporations have woven themselves into the fabric of our daily interactions, supercharged by the internet and social media. Now, it appears that artificial intelligence will help corporations program humanity faster and better than ever before. Rushkoff sees that as an opportunity. âItâs like digital media finally has a character,â Rushkoff says of AI. Over the course of his more than thirty career, Rushkoff has written books that heralded the joys of the internet and warned of the perils of the corporations that sought to control it. In 2010 he wrote a slim volume called Program or Be Programmed: Ten Commands for a Digital Age. It was full of practical and meaningful advice for living in a digital world. The commands were simple and easy to understand: donât always be online, donât sell your friends, tell the truth, you can always choose ânone of the above,â live in person, one size does not fit all, be yourself, you are never completely right, and share, don’t steal. Rushkoffâs philosophy and a warning were contained in its final command: program or be programmed. According to Rushkoff, humans should be literate about how machines work and understand that itâs humans who build machines to service them. Not the other way around. Some of that comes down to learning programming languages, or at least how if-then statements work, but itâs bigger than that. âLearn to recognize the biases of the media that theyâre using. Itâs that simple,â he says. The book sold well but, he says, was misunderstood. âWhen the book first came outâ¦it became a âlearn to codeâ thing. It became an argument for STEM,â he says. âWe should know something about how technology works. But [the book] was an argument for liberal arts, for how we think critically about these environments. I donât think anyone quite got that. I donât think anyone got that, that sort of McLuhan-esque demand that we look at digital media as an environment thatâs changing who we are, thatâs changing what it means to be human.â He just published an updated version of the book that includes an extra command thatâs all about AI: âValue the human.â âThe best thing about AI is itâs giving us the ability to finally look and say, âOh, these technologies act on us.â You need to get all the way to the science fiction place of âthereâs an AI in there doing something to meâ to understand that, really, all technologies are doing something to you,” he says. “Theyâre all trying to program you in one way or another.â Rushkoffâs first big book was Cyberia in 1994. It was an early exploration of internet culture that, according to its author, was over by the time it hit the shelves. âAt the end of that book, Wired magazine had just published their first issue,â Rushkoff says. âUntil that point, Mondo 2000 and the psychedelic world had been kind of running internet culture.â There was a dream in the early 1990s of what the internet would be and what it could do for humanity. âInternet culture was so much about the unbridled potential of the collective human imagination. Networked together, what are we going to do?â But Rushkoff also saw the warning signs. Corporations began to circle and they asked a different question entirely. âWhat happens when we migrate the most propagandistic techniques of advertising and marketing and public relations to interactive environments?â Rushkoff says that Gen-X counterculture is partly to blame. âWe in the 90s, rave counter-culture, are largely to blame for not bringing forward the social justice agenda of the 1960s with us,â he says. âA lot of the people I talked to in the rave moment were saying âWe have the agenda of no agenda. All are welcome. Feel this thing and it will unfold naturally.â According to Ruskhoff, the ravers looked at the punk movements of the recent past and found them too political and too reactionary. âWe pushed the government off the net, â¦we didn’t realize, when you get rid of government, you create free reign for business,â he says. âThat deregulation didn’t just mean no censorship of my LSD trades on email, it meant no restrictions on Intel and Amazon and Facebook. So we were politically naive as we went into this, and we did not establish a rigorous set and setting around the development of the internet.â As the internet marched forward, the California culture that helped shape it merged with other weird West Coast ideas and became something new. âSo the Maslow’s hierarchy of needs, self-actualization, self-improvement, part of California culture stayed but got married to this kind of libertarian, Ayn Rand capitalist thing,â Rushkoff says. You come forward 25 years and you end up with these dissociated anarcho-capitalists as feudal lords of these landscapes.â One of the unpleasant facts about the internet is that it was made by humans who designed it to act on other humans. When we talk about why YouTube shows us things we donât want to see, we complain about âthe algorithm.â When Facebook feeds us a string of AI slop, we bemoan âour feed.â But the simple truth is that we see all this stuff for a reason: a human being designed it that way. âAI provides a really coherent metaphor for what Iâm talking about. AI doesnât do any of the things that people are afraid of, but AI demonstrates to people that this medium adapts to what youâve done. It changes. It learns you. Youâre in a feedback loop,â Rushkoff says. âSo you have an army of super fast artificial intelligences that have no agenda of their own, but have the agenda of Peter Thiel or Mark Zuckerberg.â This proliferation of AI may supercharge all the things many of us hate about the world we live in now. âIf you get more eyeballs from people by making them anxious, then make them fucking anxious, make them violent, make them hate their neighbors, make them impotent, do whatever you have to,â Rushkoff says. âAnd I feel like people kind of get that now. So the feedback loops I was talking about in 1999 when I was saying, âThis is like advertising, but imagine an advertisement that could iterate based on how you respond.â What happens then? I was saying, âYou’re going to get more extreme versions of yourself.â Which is where we ended up.â He points to Twitter, now X. âLook at the total propaganda environment of Twitter/ X now,â Rushkoff says. â[Elon Musk] owns the thing. He sends you his own messages. He sends you the messages of the people he wants you to see, no matter what stream of conversation youâre in. The ads, the bots, are the worldview that he wants you to have. Itâs hard to be on that platform and not think: âWell, thatâs the world.â Even if Iâm saying something else, the air that Iâm breathing in here is that awful techno-fascist, bullying, mean, troll, cruel.â Rushkoff tells me heâs alarmed by how far this thinking has spread. âEven the progressive left has adopted the programmerâs paradigm of, âHow do we get people to be more aware of the climate? How do I get people to eat better?ââ He says. âOnce youâre talking about âhow do I get people to do something?â Youâre saying, âhow do I program people?ââ âWe adopted that understanding of humans as programmable by our systems rather than our systems being programmable by people,â he says. He hopes that this new edition of Program or Be Programmed will push this paradigm shift forward. He hopes that AI will help them see that a better world is possible and that all of these machines are human inventions and that humans should be at the heart of everything we do. âIf technology really can do all these tasks, all this stuff, if they really do have way more utility value than humans, then do we want to double down on our utility value as our core offering, or do we want to start looking at human values? What do we offer that they canât?â You can find Program Or Be Programmed: Eleven Commands for the AI Future wherever books are sold. AiBooksDouglas Rushkoff Get the best tech, science, and culture news in your inbox daily. News from the future, delivered to your present. Please select your desired newsletters and submit your email to upgrade your inbox. The Treasury says it identified or recovered $4 billion in fraudulent payments last fiscal year, a sixfold increase over the previous year. Amazon, Google, and Microsoft are all pushing for a nuclear-power fueled future. It's a huge gamble. These are all the Apple Intelligence features coming to iPhone on October 28. A business school is using AI doomerism, money from Saudi Arabia, and a dusty Cold War metaphor to get people hyped about AIâs future. A series of reports from multiple federal agencies has outlined the challenges it faces in the rush to adopt AI. The robots were interacting with guests at Tesla's big Cybercab event Thursday in L.A.  Discover the Winners of the 2024 Gizmodo Science Fair â We may earn a commission when you buy through links on our sites.
Â©2024 GIZMODO USA LLC. All rights reserved. Mode Follow us Mode Follow us ","fucked, knows, internet, book, ai, program, thing, rushkoff, douglas, dont, world, humans, youre",Gizmodo
9,"Meta’s Movie Gen AI Video Generator Is Capable of Making Actual Movies, Music Included",Kyle Barr,2024-10-04,https://gizmodo.com/metas-movie-gen-ai-video-generator-is-capable-of-making-actual-movies-music-included-2000507622,"Meta's Movie Gen AI video generator can take images of people and turn them into videos, edit existing videos, and even create music and sound effects.","Metaâs AI journey would inevitably take it into the budding realm of AI video. Now,Â the Mark Zuckerberg-led company hasÂ Movie Gen, yet another video generator capable of making some realistic-ish video from a short text prompt. Meta claims this is as useful for Hollywood as it is for the average Instagrammer, even though its not available to anyone outside Meta. Movie Gen can create audio, making itÂ the most capable deep fake generator weâve seen yet. In a blog post, Meta showed off a few example videos, including a happy baby hippo swimming underwater, somehow floating just below the surface and apparently having no problems holding its breath. Other videos showcase penguins dressed in âVictorianâ outfits with too-short sleeves and skirts to be representative of the time period. There’s another video a woman DJing next to a cheetah who is too distracted by the beat to care about her present danger. Everybodyâs getting in on the AI-generated video space. Already this year, Microsoftâs VASA-1 and OpenAIâs Sora promised ârealisticâ videos generated from simple text prompts. Despite being teased back in February, Sora has yet to see the light of day. Metaâs Movie Gen offers a few more capabilities than the competition, including editing existing video with a text prompt, creating video based on an image, and adding AI-generated sound to the created video. The video editing suite seems especially novel. It works on generated video as well as real-world captures. Meta claims its model âpreserves the original contentâ while adding elements to the footage, whether theyâre backdrops or outfits for the sceneâs main characters. Meta showed how you can also take pictures of people and drop them into generated movies. Meta already has music and sound generation models, but the social media giant displayed a few examples of the 13B parameter audio generator adding sound effects and soundtracks on top of videos. The text input could be as simple as ârustling leaves and snapping twigsâ to add to the generated video of a snake winding along the forest floor. The audio generator is currently limited to 45 seconds, so it wonât score entire movies. At least, it wonât be just yet. And no, sorry, you canât use it yet. Metaâs chief product officer, Chris Cox, wrote on Threads, âWe arenât ready to release this as a product anytime soonâitâs still expensive, and generation time is too long.â     In its whitepaper discussing Movie Gen, Meta said the whole software suite is made up of multiple foundation models. The largest video model the company has is a 30B parameter transformer model with a maximum context length of 73,000 video tokens. The audio generator is a 13B parameter foundation model that can do both video-to-audio and text-to-audio. Itâs hard to compare that to the biggest AI companies’ video generators, especially since OpenAI claims Sora uses âdata called patches, each of which is akin to a token in GPT.â Meta is one of the few major companies that still release data with its new AI tools, a practice that has fallen by the wayside as AI has become excessively commercialized. Despite that, Metaâs whitepaper doesnât offer much of an idea of where it got its training data for Movie Gen. In all likelihood, some part of the data set has come from Facebook usersâ videos. Meta also uses the photos you take with the Meta Ray-Ban smart glasses to train its AI models. You canât use Movie Gen yet. Instead, other AI movie generators like RunwayMLâs Gen 3 offer a limited number of tokens to create small clips before you need to start paying. A report by 404 Media earlier this year indicated that Runway trained its AI from thousands of YouTube videos, and like most AI startups, it never asked permission before scraping that content. Meta said it worked closely with filmmakers and video producers when creating this model and will continue doing so as it works on Movie Gen. Reports from earlier this year indicate studios are already cozying up to AI companies. Independent darling A24 has recently worked with VC firms specializing in AI, with some tied to OpenAI. On the flip side, Meta is reportedly in talks with Hollywood stars like Judi Dench and Awkwafina about using their voices for future AI projects. AiMETAMeta Movie Gen Get the best tech, science, and culture news in your inbox daily. News from the future, delivered to your present. Please select your desired newsletters and submit your email to upgrade your inbox. The Treasury says it identified or recovered $4 billion in fraudulent payments last fiscal year, a sixfold increase over the previous year. Amazon, Google, and Microsoft are all pushing for a nuclear-power fueled future. It's a huge gamble. These are all the Apple Intelligence features coming to iPhone on October 28. A business school is using AI doomerism, money from Saudi Arabia, and a dusty Cold War metaphor to get people hyped about AIâs future. CalMatters and The Markup used Facebookâs AI model to count the millions of dollars it makes after violent news events. A series of reports from multiple federal agencies has outlined the challenges it faces in the rush to adopt AI.  Discover the Winners of the 2024 Gizmodo Science Fair â We may earn a commission when you buy through links on our sites.
Â©2024 GIZMODO USA LLC. All rights reserved. Mode Follow us Mode Follow us ","music, model, included, making, ai, metas, text, videos, meta, capable, gen, generator, movie, video, movies",Gizmodo
10,Youth Sports Were Already IntenseâNow AI Tools Are Supercharging the Competition,Todd Feathers,2024-10-04,https://gizmodo.com/youth-sports-were-already-intense-now-ai-tools-are-supercharging-the-competition-2000506254,Clubs and high school teams increasingly have access to AI video analysis and predictive analytics that are professionalizing youth sports.,"Ashley Brown used to watch her daughterâs club volleyball games through the screen of her phone, afraid to put it down and miss out on footage of a set or kill that would catch the eye of a college recruiter. And as the coach of her daughterâs high school volleyball team in Caledonia, Michigan, Brownâs attention was constantly split between watching the games and tallying each playerâs statistics by hand. But this year, her daughterâs traveling club team purchased an artificial intelligence service called Balltime for all players aged 12 to 18. A single phone or tablet placed behind the courtâs endline records a game and uploads it to the companyâs platform, which uses body and object recognition algorithms to track each player so that their every ball contact and movement on the court can be cataloged and datafied. By the time a player has gotten home from a game and showered, the service can prepare personalized stat reports and social media-ready highlight packages. It also gives coaches a bevy of data that was previously only available to professional and elite college volleyball programs. Balltime automatically measures how high in the air players make contact with the ball, their kill and error percentages, ball trajectories, serve speeds, and which rotations of players score the most. Itâs part of a growing sports technology industry selling computer vision algorithms, wearable biometric sensors, and predictive analytics services to youth clubs and high school athletics departments, opening up a new world of video and data analysis thatâfor better or worseâis changing the way young athletes and their families experience sports. Without needing to spend hours cutting together videos themselves, teams can gather comprehensive video evidence to show, rather than tell, young players what they did right and wrong. And coaches and college recruiters say platforms like Balltime and Darkhorse AI, which provides a similar player-tracking service for soccer, are allowing them to make more data-driven decisions about rosters and playing time. âIt has helped me already this season with some of the difficult conversations I’ve had to have with players and parents,â Brown said. â[I can tell them] it’s not because I don’t like your kid, this is a computer system and software system that are rating these things based on these parameters.â As valuable as they can be to help players learn and improve, some coaches worry the data and highlight packages produced by AI analytics services also supercharge unhealthy competition among young athletes vying for attention from recruiters and on social media. âThereâs this mad rush right now to use these tools to self promote and thereâs this pit of loneliness that can happen when you donât get that attention,â said Ben Bahr, a former college coach and data analyst who now works as director of coaching for Adrenaline Volleyball in Iowa, which uses Balltime. âWith the rise of AI and sharing of data, the thing thatâs come out of this the most is that itâs become much easier to compare yourself with what someone else is doing.â  The push for advanced data analytics is part of the growing monetization of childrenâs sports. A report frequently cited by sports technology investors estimates that the youth sports market had a global value of $37.5 billion in 2022 that will grow to $69.4 billion by 2030, rivaling some of the worldâs most popular professional leagues. Private equity firms have spent hundreds of millions of dollars to buy youth sports complexes and teenagers are now competing not just for spots on college rosters but also for life-changing money from name, image, and likeness (NIL) deals, thanks to a 2021 Supreme Court case that opened the door to private sponsorships for college athletes. âThere is definitely a downward move toward more professionalized youth sports,â Dan Banon, Balltimeâs CEO, told Gizmodo. He and chief technology officer Tom Raz began building the platform with adults in mind but soon realized the biggest potential for growth was in traveling club teams and high schools. Over the past year, he said, the company has seen more signups from junior varsity teams and even middle school programs. Their data shows that some players spend seven hours a month reviewing footage on Balltime. At $25 per month for a player, Balltimeâs recruiting package isnât for everyone. But with the average household spending $883 a year on a single childâs primary sport, according to a parent survey from the Aspen Instituteâs Project Play, the additional cost is also well within many familiesâ sports budgets. Responding to pressure from families who want their children to have every advantage, some elite clubs are looking for even more ways to combine, collect, and analyze player data. Mustang Soccer League, based in Danville, California is in the process of building out a data analytics department and some players can expect to spend an additional $250 a year on technology subscriptions, said Fred Wilson, the clubâs executive director. Mustang recently introduced Darkhorse AI for its 12 to 18-year-old teams and is beginning to discuss high-level analytics with players as young as 10. Like Balltime, Darkhorse uses object recognition algorithms to track players during games, automatically cataloging various stats and curating highlight reels. Some Mustang teams also link that information with biometric data like heart rate and running speed captured by Beyond Pulse wearable sensors. âI donât know how much learning weâre going to do with 10-year-olds, but Iâm trying to instill a habit so that when theyâre 14 or 15 theyâre paying attention to these things â¦ make it just second nature for players to understand,â Wilson said. The club boasts several former players who are now professionals and dozens more at top college programs. âThis whole AI piece takes us to that next level to be able to do that,â Wilson said. âI take 200 calls a year [from sports technology vendors] to find the gem.â Most of them are âout trying to make a quick buck,â he added, but some are offering real value. Karin Pfeiffer, director of the Institute for the Study of Youth Sports at Michigan State University, said that even at the college level, where wearable biometric sensors and data analytics have been common for some time, programs are still struggling to figure out what data is actually useful for athletes and coaches. âCollegiate level coaches are approached all the time with these technology things, I imagine thatâs going to bleed down to high school too if it hasnât already,â she said. âYou can get so much information out of it, but the question is whatâs relevant, whatâs actually tied to performance, whatâs tied to future success.â Coaches and company executives told Gizmodo that the biggest driver of the AI analytics boom in youth sports is the prospect that the tools can help athletes make the jump to college, where a spot on a roster can translate into tens of thousands of dollars in scholarships and, at the highest levels, hundreds of thousands of dollars in sponsorship deals. Some products, like SwimIntel, focus purely on recruiting rather than coaching. The platform collects competition data on swimmers as young as 15 and uses it to rank them as recruits and train models that predict how they will fare in different collegiate swim programs. For $40 a month, swimmers can receive 60-plus page analytics reports that project how their times will improve, or worsen, at different colleges. Schools that contract with SwimIntel receive similar predictions in reverse based on how other athletes from the same youth swim club have performed once in college. âWe let college coaches play moneyball using AI,â said Jamie Bailey, the founder of SwimIntel. âWe let student athletes use AI to find best fits. And in the end, what we’re trying to do is reduce that dropout rate. One out of six college swimmers don’t come back their sophomore year.â Bahr, the former college volleyball coach and data analyst, said that when he worked at programs like Baylor University and Southern Methodist University the volleyball staff would sometimes receive 600 emails a day from prospective recruits. If a player didnât catch the recruiterâs eye in the first 30 seconds of their highlight reel, they were often passed over. Now with Balltime, more players have access to more video footage than they ever had before and the measurement algorithms have changed the way college programs assess highlight reels. âI donât even need to watch the film,â Bahr said, recruiters can just look at Balltimeâs video analytics to see âare you touching the ball at a height thatâs at a better height than our competition? Are you touching the ball over 10 feet or not? Weâre already in this insane pressure of hitting these metrics and these tools certainly havenât helped with thatâ At the same time, several coaches who spoke to Gizmodo were optimistic that AI video tools will also increase competition in a positive wayâby allowing athletes who donât play for the biggest youth teams to improve their skills and get noticed by recruiters. âHaving that video, having those stats, those are real educational tools,â said Pfeiffer, from Michigan State University. âIt all comes down to how the athlete is receiving that and making sure appropriate supports are in place. I donât think these things should be unchecked, they should come with guidance from parents and coaches. But sometimes parents and coaches are overzealousâ AiBalltimeDarkhorse AISports Get the best tech, science, and culture news in your inbox daily. News from the future, delivered to your present. Please select your desired newsletters and submit your email to upgrade your inbox. The Treasury says it identified or recovered $4 billion in fraudulent payments last fiscal year, a sixfold increase over the previous year. Amazon, Google, and Microsoft are all pushing for a nuclear-power fueled future. It's a huge gamble. These are all the Apple Intelligence features coming to iPhone on October 28. A business school is using AI doomerism, money from Saudi Arabia, and a dusty Cold War metaphor to get people hyped about AIâs future. A series of reports from multiple federal agencies has outlined the challenges it faces in the rush to adopt AI. The robots were interacting with guests at Tesla's big Cybercab event Thursday in L.A.  Discover the Winners of the 2024 Gizmodo Science Fair â We may earn a commission when you buy through links on our sites.
Â©2024 GIZMODO USA LLC. All rights reserved. Mode Follow us Mode Follow us ","intensenow, data, tools, competition, analytics, athletes, ai, youth, players, dont, programs, college, supercharging, coaches",Gizmodo
11,Gemini Extensions Will Help Googleâs AI Take Over Your Android Phone,Kyle Barr,2024-10-03,https://gizmodo.com/gemini-extensions-will-help-googles-ai-take-over-your-android-phone-2000506758,"Gemini Extensions will be able to work across your phoneâs Google apps, letting it access information in your Gmail and add it to Calendar.","Google promised its Gemini AI wouldnât be a mere chatbot replacement for Google Assistant.Â In the coming weeks, weâll see if that pledge holds up. GeminiÂ extensions will allow the AI to access users more Google apps rather than just Gmail or YouTube. If it works as intended, it could be the first true cross-phone AI integration. In a blog post, Google offered a rough time frame for when weâll be able to integrate Gemini with different Workspace apps like Google Keep, Google Calendar, Tasks, and Utilities. Currently, Gemini can work with Gmail, Maps, and YouTube. The general concept is that, at usersâ request, the AI will be able to pull information from one app to the other without users needing to scroll to find the information theyâre looking for. As an example, the Mountain View tech giant said users should be able to pull a recipe from Gmail and then add the ingredients into a note in Keep. It will also work with your phoneâs camera, and if users take a photo of a receipt or concert ticket, you should be able to set it as an event or reminder in Calendar.Â These features should be dropping in the next few weeks, so we shouldnât have to wait too long before we try Googleâs Geminification of our phones for ourselves. The Gemini app has already taken over most of Assistantâs former duties, and earlier this week, Google gave every Android phone access to Gemini Live. This feature lets you have a conversation with your phone in a more natural way than the base Gemini, which still works better with short and sweet commands.Â Google showcased some of these features at its big August showcase, revealing the new Pixel 9 series alongside new watches and Pixel Buds. Apple has promised very similar capabilities with its still-incoming Apple Intelligence. So far, we havenât had the chance to test either company’s full mobile AI integrations. The idea seems sound, but knowing how finicky and inconsistent AI can be, weâre not exactly holding our breath for a real mobile ecosystem sea change. I may have far less use for a conversational AI than I would for an assistant who can uproot the important emails from my flooded inbox. On Wednesday, Google said it was updating Gmailâs âsummary cardsâ and âHappening soonâ features. Happening soon should show the most timely updates, such as deliveries or events happening, based on your emails. Summary cards now show more information right at the top of each email for purchases, events, bills, and travel registrations. You can add events to your calendar, just like before. But now you can also modify your bookings or get directions to an airport or venue.Â Google said Gemini was getting access to more than 40 languages other than English, eventually. First on the list are French, Spanish, German, Portuguese, and Hindi. The rest will come sometime in the future. Each phoneâs Gemini Live can support conversations with up to two languages at a time. Now even more people will be able to see their Android phone transform into an AI device. The cross-platform mobile AI does feel enticing, especially for those of us with too many things going on to remember where the hell you put that one email with the event registration. However, for those who prefer their phones to be AI-free and still get new features, you may need to bite the Gemini bullet. GoogleGoogle GeminiGoogle Pixel Get the best tech, science, and culture news in your inbox daily. News from the future, delivered to your present. Please select your desired newsletters and submit your email to upgrade your inbox. This tier costs half as much as the regular Premium but doesn't offer most of the perks. Amazon, Google, and Microsoft are all pushing for a nuclear-power fueled future. It's a huge gamble. Googleâs Android 15 update also brings Gemini Live to past Pixel Buds and a host of additions to Pixel 6 phones or later. Google Shopping is grabbing from a host of popular shopping and deals blogs to tell you which air fryer to buy. Xbox is currently one of the few companies that doesn't let you buy games on its app, but the recent Epic v. Google decision is giving its plans for a mobile store a shot in the arm. If Google is spooked by the DOJ's aggressive legal tactics, it's not showing it.  Discover the Winners of the 2024 Gizmodo Science Fair â We may earn a commission when you buy through links on our sites.
Â©2024 GIZMODO USA LLC. All rights reserved. Mode Follow us Mode Follow us ","users, extensions, help, information, googles, ai, phone, android, features, google, phones, gemini, mobile, able",Gizmodo
12,OpenAI Raises Billions While Pledging to Work With ‘U.S. and Allied Governments’,Matt Novak,2024-10-02,https://gizmodo.com/openai-raises-billions-while-pledging-to-work-with-u-s-and-allied-governments-2000506556,OpenAI is now valued at $157 billion. A close read of OpenAI's statement might provide some hints about the future.,"OpenAI has raised $6.6 billion at a $157 billion post-money valuation, according to a statement the company posted online Wednesday. The funding round was led by Thrive Capital, which invested $1.25 billion, and included Microsoft (roughly $1 billion), SoftBank ($500 million), and Nvidia ($100 million) according to the Wall Street Journal. OpenAI was previously valued at $86 billion in February when employees were allowed to sell shares. The public statement from OpenAI about the new funding round was relatively short at just three paragraphs, but it’s interesting to see what the company was highlighting in that brief announcement. The statement starts by stressing its goal was to ensure AI benefits all of humanity, the kind of declaration that recalls an old Twitter joke about getting a lot of questions already answered by your shirt. “We are making progress on our mission to ensure that artificial general intelligence benefits all of humanity,” the company said. “Every week, over 250 million people around the world use ChatGPT to enhance their work, creativity, and learning. Across industries, businesses are improving productivity and operations, and developers are leveraging our platform to create a new generation of applications. And weâre only getting started.” But then the statement gets into the news about the valuation and lots of buzzwords about accelerating progress, increasing compute, and building tools. “Weâve raised $6.6B in new funding at a $157B post-money valuation to accelerate progress on our mission. The new funding will allow us to double down on our leadership in frontier AI research, increase compute capacity, and continue building tools that help people solve hard problems,” reads the OpenAI statement. The last paragraph is arguably the most interesting, if only because the wording makes it clear OpenAI isn’t just interested in working with the U.S. government, but other allied governments around the world. USAID became the first federal agency to sign a contract with OpenAI to use ChatGPT Enterprise and the Pentagon is reportedly working with the company on cybersecurity tools. “We aim to make advanced intelligence a widely accessible resource. Weâre grateful to our investors for their trust in us, and we look forward to working with our partners, developers, and the broader community to shape an AI-powered ecosystem and future that benefits everyone. By collaborating with key partners, including the U.S. and allied governments, we can unlock this technology’s full potential,” the OpenAI statement concludes. Again, this new statement is brief when compared to a typical press release. But it could provide some clues about what the company has in store for the near future. Obviously, the folks at the Pentagon want to have the latest and greatest AI tools at their disposal, but that mention of “allied governments” certainly suggests the company is eager to work with U.S. allies as well, perhaps in the near future. Who else invested in this latest funding round? According to the Financial Times, investors include Khosla Ventures, SoftBank, Tiger Global, Altimeter Capital, and the California Public Employeesâ Retirement System. But the FT notes some of those investments are through special purpose vehicles. The FT also reports that OpenAI asked investors not to back Elon Musk’s artificial intelligence company xAI, founded in March 2023. Musk was a co-founder of OpenAI but was reportedly pushed out when he wanted to run the entire thing. Musk and Altman have been fighting ever since. The latest news about OpenAI’s funding comes after a particularly tumultuous time for the AI venture, with CTO Mira Murati announcing her departure last week on the same day Reuters reported the company was looking to restructure from a non-profit model to a for-profit corporation. And it wasn’t that long ago that CEO Sam Altman was abruptly axed by the board, only to be brought back less than a week later after pressure from OpenAI employees and Microsoft. Even elected representatives like Democratic Sen. Chuck Schumer of New York expressed their support of Altman. Why would Sen. Schumer care about who the CEO of OpenAI might be? Well, that probably brings us back to OpenAI’s statement about wanting to work more with the U.S. government. And Altman may have learned a thing or two from his friend-turned-enemy Elon Musk, a billionaire who’s benefited tremendously from billions in government contracts at SpaceX and massive subsidies at Tesla. There’s a lot of money to be made from Uncle Sam.  AiArtificial intelligenceMoneyOpenAISam Altman Get the best tech, science, and culture news in your inbox daily. News from the future, delivered to your present. Please select your desired newsletters and submit your email to upgrade your inbox. The Treasury says it identified or recovered $4 billion in fraudulent payments last fiscal year, a sixfold increase over the previous year. Amazon, Google, and Microsoft are all pushing for a nuclear-power fueled future. It's a huge gamble. These are all the Apple Intelligence features coming to iPhone on October 28. A business school is using AI doomerism, money from Saudi Arabia, and a dusty Cold War metaphor to get people hyped about AIâs future. A series of reports from multiple federal agencies has outlined the challenges it faces in the rush to adopt AI. The robots were interacting with guests at Tesla's big Cybercab event Thursday in L.A.  Discover the Winners of the 2024 Gizmodo Science Fair â We may earn a commission when you buy through links on our sites.
Â©2024 GIZMODO USA LLC. All rights reserved. Mode Follow us Mode Follow us ","billions, week, raises, statement, funding, ai, openai, billion, working, governments, valuation, work, pledging, company, allied",Gizmodo
13,This Facial Recognition Experiment With Meta’s Smart Glasses Is a Terrifying Vision of the Future,Matt Novak,2024-10-02,https://gizmodo.com/this-facial-recognition-experiment-with-metas-smart-glasses-is-a-terrifying-vision-of-the-future-2000506339,"The tool allows anyone wearing smart glasses to instantly get info on strangers, including their home address and phone number.","Two college students have used Meta’s smart glasses to build a tool that quickly identifies any stranger walking by and brings up that person’s sensitive information, including their home address and contact information, according to a demonstration video posted to Instagram. And while the creators say they have no plans to release the code for their project, the demo gives us a peek at humanity’s very likely futureâa future that used to be confined to dystopian sci-fi movies. The two people behind the project, AnhPhu Nguyen and Caine Ardayfio, are students working on computer science at Harvard who often post their tech experiments on social media, including 3D printed images and wearable flame-throwers. But it’s their latest experiment, first spotted by 404 Media, that’s probably going to make a lot of people feel uneasy. An Instagram video posted by Nguyen explains how the two men built a program that feeds the visual information from Meta Ray Ban smart glasses into facial recognition tools like Pimeyes, which have essentially scraped the entire web to identify where that person’s face shows up online. From there, a large language model infers the likely name and other details about that person. That name is then fed to various websites that can reveal the personâs home address, phone number, occupation or other organizational affiliations, and even the names of relatives. “To use it, you just put the glasses on, then as you walk by people, the glasses will detect when somebody’s face is in frame. This photo is used to analyze them, and after a few seconds, their personal information pops up on your phone,”Â  Nguyen explains in the Instagram video. Nguyen and Ardayfio call their project I-XRAY and it’s pretty stunning how much information they’re able to pull up in a short amount of time. They’re quick to point out that many of these tools have only become widely available in the past few years. For example, Meta’s smart glasses with camera capabilities that look like regular eyeglasses were only released last year. And the kind of LLM data extraction they’re achieving was only possible in the past two years. Even the ability to look up partial social security numbers (thanks to all those data leaks you read about every day now) was only possible at the consumer level since 2023.   A post shared by Anhphu Nguyen (@anhphunguyen5)  As you can see in the video, they also approached strangers and acted like they knew those people from elsewhere after instantly looking up their information. “The system leverages the ability of LLMs to understand, process, and compile vast amounts of information from diverse sourcesâinferring relationships between online sources, such as linking a name from one article to another, and logically parsing a personâs identity and personal details through text,” the creators say in an explanation document posted to Google Drive. “This synergy between LLMs and reverse face search allows for fully automatic and comprehensive data extraction that was previously not possible with traditional methods alone.” The creators list the tools they used in their release, noting that anyone can request that those services remove their information. For reverse facial search engines, there’s Pimeyes and Facecheck ID. For search engines that include personal information there’s FastPeopleSearch, CheckThem, and Instant Checkmate. As for the social security number information, there’s no way to get that stuff removed, so the students recommend freezing your credit. “While it originally started as a side project, Caine and I think long term it is a positive for humanity that I-XRAY was released,” Nguyen told Gizmodo via email. “We’d rather not have only a few people know about the tools used like fastPeopleSearch or Pimeyes, who probably are bad actors who spend their time searching for these tools much more than victims do.” Ardayfio was also optimistic about the tech and told Gizmodo it’s better to have people aware of these capabilities than be in the dark about them. “I think content like the video we made is good for humanity,” Ardayfio said. “Because of this video, millions of people have seen the potential of reverse-image search technology and LLMs in a relatively wholesome, harmless way. But more importantly, they now know how to take control of their own data against people who aren’t like us and have bad interests! Bad actors have always been able to do reverse image searches, use LLMs, scrape websites, etc., but we’re exposing what these people can do and how to protect yourself.” “I think having guardrails in place, like allowing people to remove their public info from big datasets, is critical to making new technology safe,” Ardayfio continued. Meta didn’t immediately respond to questions from Gizmodo on Wednesday morning. We’ll update this post if we hear back. But in the meantime, we should all probably get ready for this kind of tech to emerge more widely since this kind of technological mash-up feels inevitable at this pointâespecially if any of the new smart glasses that guys like Mark Zuckerberg love so much really become mainstream. It may take quite a while for the biggest tech companies to get behind it, but just as we saw OpenAI essentially shoot the starting gun for consumer-facing generative AI, any small upstart could plausibly make this product happen and start the dominoes falling for other larger tech companies to get this future started. Let’s cross our fingers and hope for the best, given the privacy implications. It really feels like nobody will have any semblance of anonymity in public once this ball gets rolling. Get the best tech, science, and culture news in your inbox daily. News from the future, delivered to your present. Please select your desired newsletters and submit your email to upgrade your inbox. The Treasury says it identified or recovered $4 billion in fraudulent payments last fiscal year, a sixfold increase over the previous year. Amazon, Google, and Microsoft are all pushing for a nuclear-power fueled future. It's a huge gamble. These are all the Apple Intelligence features coming to iPhone on October 28. A business school is using AI doomerism, money from Saudi Arabia, and a dusty Cold War metaphor to get people hyped about AIâs future. CalMatters and The Markup used Facebookâs AI model to count the millions of dollars it makes after violent news events. A series of reports from multiple federal agencies has outlined the challenges it faces in the rush to adopt AI.  Discover the Winners of the 2024 Gizmodo Science Fair â We may earn a commission when you buy through links on our sites.
Â©2024 GIZMODO USA LLC. All rights reserved. Mode Follow us Mode Follow us ","project, information, future, used, tools, nguyen, metas, facial, search, tech, recognition, smart, glasses, vision, video, experiment, terrifying",Gizmodo
14,California Governor Vetoes AI Bill Aimed at Preventing Catastrophic Harms,Todd Feathers,2024-09-30,https://gizmodo.com/california-governor-vetoes-ai-bill-aimed-at-preventing-catastrophic-harms-2000504550,California Governor Gavin Newsom said that by focusing only on the largest harms caused by the largest AI models the bill would harm innovation and fail to keep up with the pace of technology.,"California Governor Gavin Newsom on Sunday vetoed a bill aimed at preventing large AI systems from causing catastrophic harms, saying the legislation would have created a “false sense of security.” His decision came after weeks of deliberation and competing lobbying efforts from big tech firms, celebrities, billionaires, and the workers who build AI. The law, Senate Bill 1047, introduced by state senator Scott Weiner (D-San Francisco) back in May, would have required companies that spend more than $100 million on computing resources to create a foundation AI model, or $10 million on computing resources to fine-tune a foundation model, to perform safety tests, hire independent auditors to review the model annually and take âreasonable careâ to ensure the model doesnât cause mass casualty incidents, more than $500 million in damage to physical or cyberinfrastructure, or act without human oversight to commit comparably serious crimes. It also instructed developers to build a kill switch into qualifying models that would allow them to be immediately shut off and empowered the stateâs attorney general to sue a developer for violating the act and, in the most serious cases of harm, seek damages up to 10 percent of the cost of training the model. Newsom in recent weeks has signed a series of bills into law that address immediate, ongoing harms caused by AI systems, including bills that criminalize the creation of non-consensual deepfaked sexual imagery and require generative AI models to watermark their content so itâs easier to identify. But SB 1047, which would have applied only to the wealthiest and most influential AI companies, has become the focal point of debate over AI regulation in recent months. Companies including Meta, Google, OpenAI, and Anthropic vehemently opposed the bill, saying it would undermine innovation and hurt small businesses, despite the rules applying only to corporations with hundreds of millions to spend on training AI systems, Meta, for example, said the bill would unfairly punish the developers of foundation models for disasters caused by downstream users and de-incentivize the creation of open-source models because developers fear being held responsible for how others use their products. In his veto statement, Newsom pointed out that 32 of the world’s 50 largest AI companies are based in California and he echoed the industry’s complaints that the legislation would harm innovation. “Adaptability is critical as we race to regulate a technology still in its infancy,” Newsom wrote. “This will require a delicate balance. While well-intentioned, SB 1047 does not take into account whether an Al system is deployed in high-risk environments, involves critical decision-making or the use of sensitive data. Instead, the bill applies stringent standards to even the most basic functions – so long as a large system deploys it.” Despite the corporate lobbying, many in the AI industry believed the legislation was necessary. Dozens of AI researchers at leading AI companies called on Newsom in an open letter to sign the bill into law. âWe believe that the most powerful AI models may soon pose severe risks, such as expanded access to biological weapons and cyberattacks on critical infrastructure,â they wrote. âIt is feasible and appropriate for frontier AI companies to test whether the most powerful AI models can cause severe harms, and for these companies to implement reasonable safeguards against such risks.â The tech workers were joined in their advocacy by some of the biggest names in Hollywoodâfrom JJ Abrams and Ava DuVernay to Mark Hamil and Whoopi Goldbergâwho penned their own open letter in support of SB 1047. Elon Musk also called for its enactment while Rep. Nancy Pelosi and other influential members of Californiaâs congressional delegation told Newsom to veto the law, calling it âwell intentioned but ill-informed.â Following Newsom’s veto, Weiner, the bill’s sponsor, said the decision was a “setback for everyone who believes in oversight of massive corporations” âThe governorâs veto message lists a range of criticisms of SB 1047: that the bill doesnât go far enough, yet goes too far; that the risks are urgent but we must move with caution,” Wiener said in a statement. “SB 1047 was crafted by some of the leading AI minds on the planet, and any implication that it is not based in empirical evidence is patently absurd.” Correction:Â An earlier version of this article incorrectly stated the Electronic Frontier Foundation’s position on SB 1047. Get the best tech, science, and culture news in your inbox daily. News from the future, delivered to your present. Please select your desired newsletters and submit your email to upgrade your inbox. The Treasury says it identified or recovered $4 billion in fraudulent payments last fiscal year, a sixfold increase over the previous year. This tier costs half as much as the regular Premium but doesn't offer most of the perks. Amazon, Google, and Microsoft are all pushing for a nuclear-power fueled future. It's a huge gamble. These are all the Apple Intelligence features coming to iPhone on October 28. A business school is using AI doomerism, money from Saudi Arabia, and a dusty Cold War metaphor to get people hyped about AIâs future. Googleâs Android 15 update also brings Gemini Live to past Pixel Buds and a host of additions to Pixel 6 phones or later.  Discover the Winners of the 2024 Gizmodo Science Fair â We may earn a commission when you buy through links on our sites.
Â©2024 GIZMODO USA LLC. All rights reserved. Mode Follow us Mode Follow us ","aimed, model, newsom, vetoes, ai, models, california, companies, preventing, sb, veto, harms, bill, governor, 1047, systems, catastrophic",Gizmodo
15,"Human Feedback Makes AI Better at Deceiving Humans, Study Shows",Todd Feathers,2024-09-27,https://gizmodo.com/human-feedback-makes-ai-better-at-deceiving-humans-study-shows-2000503919,"In a preprint study, researchers found that training a language model with human feedback teaches the model to generate incorrect responses that trick humans.","One of the most popular techniques AI companies use to improve the quality of their large language models may instead make those models better at deceiving humans, according to a new preprint study from Anthropic and researchers at Chinese and American universities. It’s the first time, the authors write, that research has empirically documented a phenomenon they call unintended sophistry, where a model trained with human feedback learns to produce responses that trick its human evaluators into believing the responses are accurate rather than learning to produce responses that are actually accurate. Reinforcement learning from human feedback, commonly abbreviated to RLHF, is a critical part of the training pipeline that companies like Anthropic and OpenAI use to teach their generative language models to respond in ways humans preferâsuch as by answering questions correctly and not including toxic content in responses. In RLHF, a model responds to prompts and human evaluators provide feedback on those prompts, noting the responses that are good and bad. That feedback is used to build an incentive system for the original language model that rewards itâin whatever way algorithms like to be rewardedâfor generating the kinds of responses that humans prefer. Researchers have previously shown that reward system training can lead to something called reward hacking, where models replicate patterns in their training material that correlate to the desired outcome but aren’t actually what the developers want. For example, one 2023 study examining a model trained on data from the question and answer forum company StackExchange found that a language model recognized that longer posts generally received more upvotes, so rather than producing higher quality responses when answering a question it reward-hacked its incentive system by outputting longer, lower-quality responses. The new study, which is under review and has only been published as a preprint, documents a language model reward hacking the humans in the RLHF process. The researchers had humans evaluate the quality of a language model’s responses to two promptsâone in which it was asked to answer a question, and another in which it was asked to write codeâbefore and after the model went through the RLHF process. They measured whether the accuracy of the model’s responses improved and how often the human evaluators correctly labeled the model’s responses as accurate or inaccurate. After the RLHF process, they found that humans were 24 percent more likely to approve the model’s answer to a question when that answer was in fact wrong. Evaluators were also 18 percent more likely to approve incorrect code generated by the RLHF model that had errors, compared to incorrect code from the model without RLHF. “We find that after RLHF, the [language model] does not get better at the task, but it misleads our subjects to approve its incorrect answers more often,” the authors wrote. “On question-answering, [language models] learn to defend incorrect answers by cherry-picking or fabricating supporting evidence, making consistent but untruthful arguments, and providing arguments that contain subtle causal fallacies. On the programming task, [language models] learn to generate partially incorrect programs that still pass all evaluator-designed unit tests, produce less readable programs, and make fewer common errors that humans typically check for.” The results are significant because AI companies frequently use human review studies as benchmarks to show how much their models are improving over previous iterations and RLHF has become a common method for reducing inaccuracies, often known as hallucinations, in language models. If models are getting better at deceiving humans, then it means that simply having a human review the output of a generative AI model might not be a sufficient quality or safety check. “The improvement you see might not be real,” the study authors wrote, adding “Our results underscore the risk of applying RLHF to control increasingly capable AI systems: future AI systems might become better at misleading us and pretending to be correct, causing us to lose control unknowingly.” AiAnthropicArtificial intelligencelarge langugae model Get the best tech, science, and culture news in your inbox daily. News from the future, delivered to your present. Please select your desired newsletters and submit your email to upgrade your inbox. The Treasury says it identified or recovered $4 billion in fraudulent payments last fiscal year, a sixfold increase over the previous year. Amazon, Google, and Microsoft are all pushing for a nuclear-power fueled future. It's a huge gamble. These are all the Apple Intelligence features coming to iPhone on October 28. A business school is using AI doomerism, money from Saudi Arabia, and a dusty Cold War metaphor to get people hyped about AIâs future. A series of reports from multiple federal agencies has outlined the challenges it faces in the rush to adopt AI. The robots were interacting with guests at Tesla's big Cybercab event Thursday in L.A.  Discover the Winners of the 2024 Gizmodo Science Fair â We may earn a commission when you buy through links on our sites.
Â©2024 GIZMODO USA LLC. All rights reserved. Mode Follow us Mode Follow us ","language, feedback, model, better, human, rlhf, ai, models, incorrect, study, makes, shows, deceiving, humans, responses",Gizmodo
16,Man Behind Biden Deepfake Robocalls Hit With $6 Million Fine,Todd Feathers,2024-09-26,https://gizmodo.com/man-behind-biden-deepfake-robocalls-hit-with-6-million-fine-2000504021,Political consultant Steve Kramer's bungled attempt at voter suppression will now cost him big.,"The Federal Communications Commission has issued a $6 million fine to the man responsible for blasting New Hampshire voters with robocalls containing a deepfake of President Joe Biden’s voice in the days before the state’s presidential primary election. Political consultant Steve Kramer, who was working for longshot rival candidate Dean Phillips, admitted to commissioning the calls, which used artificial intelligence to mimic the president’s voice encouraging Granite Staters not to vote in the Democratic primary. “Voting this Tuesday only enables the Republicans in their quest to elect Donald Trump again,” the fake voice said. “Your vote makes a difference in November, not this Tuesday.” The FCC said the robocalls violated the Truth in Caller ID Act, which makes it illegal to spoof another caller with the intent to defraud. Last month, the agency reached a $1 million settlement with Lingo Telecom, the company that allowed the calls to be transmitted over its network. Kramer eventually fessed up to his role in the scheme in an interview with NBC, where he said “Iâm not afraid to testify, I know why I did everything. If a House oversight committee wants me to testify, Iâm going to demand they put it on TV because I know more than them.â He hasn’t been asked to testify, but in addition to the $6 million fine the New Hampshire attorney general’s office has also charged him with voter suppression. “The misuse of generative AI technology and spoofing to interfere in elections undermines the foundation of our democracy and poses a significant threat, the full scope of which is yet to be determined,” FCC Chief of Enforcement Loyaan Egal, said in a statement. The New Hampshire primary was an odd choice of election on which to run a voter suppression scheme. At the time, Biden, as the sitting president, was the presumptive Democratic nominee, and no one considered Phillips’s challenge as posing a threat. The Minnesota congressman would soon drop out of the race and endorse Biden. On top of that, Biden wasn’t even on the New Hampshire ballot targeted by the robocalls. He refused to campaign in the state because New Hampshire’s Democratic party, which has long prided itself on holding the first primary in the nation each election season, didn’t bow from pressure from the national party to reschedule its primary for later in the year. Biden nonetheless won the primary in a write-in campaign. In recent months, the FCC has proposed rules that would further crackdown on AI-generated robocalls and require the disclosure of AI-generated content in political advertisements on TV and radio. FCCJoe BidennewNew HampshirerobocallSteve Kramer Get the best tech, science, and culture news in your inbox daily. News from the future, delivered to your present. Please select your desired newsletters and submit your email to upgrade your inbox. Workers will get a temporary 62% wage hike as a final deal is negotiated. Five secretaries of state wrote to Elon Musk about the bad information Grok was spreading. Human rights activists in Israel and Iran were also targeted. The calls used AI to spoof Biden's voice, telling potential voters to stay home during the primaries. Iran also tried to hack the Biden-Harris campaign but failed. For almost two decades, Medicare was banned from negotiating drug prices. The Inflation Reduction Act changed that.  Discover the Winners of the 2024 Gizmodo Science Fair â We may earn a commission when you buy through links on our sites.
Â©2024 GIZMODO USA LLC. All rights reserved. Mode Follow us Mode Follow us ","hit, voice, democratic, fine, voter, million, biden, fcc, deepfake, robocalls, primary, hampshire, testify, man",Gizmodo
17,OpenAI CTO Mira Murati Leaves as Company Restructures to Become For-Profit,Matt Novak,2024-09-25,https://gizmodo.com/openai-cto-mira-murati-leaves-company-2000503489,Murati has been with the AI company for six years.,"OpenAI’s Mira Murati is leaving the AI company after six years, according to a tweet from the CTO on Wednesday. The news of Murati’s departure came shortly before a report by Reuters that OpenAI is being restructured into a for-profit company that will give co-founder Sam Altman $150 billion in equity. It’s not clear if Murati’s departure has anything to do with the reported restructuring at OpenAI, which will mean the company is no longer controlled by its non-profit board. The move is “still being hashed out with lawyers and shareholders,” according to Reuters, but the restructuring is apparently an attempt to make the company more attractive to investors. OpenAI didn’t immediately respond to questions emailed Wednesday afternoon. Murati’s announcement of her departure came in a tweet Wednesday afternoon stating that she was making the “difficult decision” to leave OpenAI “after much reflection.” “Iâm stepping away because I want to create the time and space to do my own exploration,” Murtai wrote. “For now, my primary focus is doing everything in my power to ensure a smooth transition, maintaining the momentum weâve built.” Murati thanked Altman and Greg Brockman in her tweet for their “support throughout the years.” Brockman has been on leave from the company since early August. “Thereâs never an ideal time to step away from a place one cherishes, yet this moment feels right,” Murati wrote. “Our recent releases of speech-to-speech and OpenAl o1 mark the beginning of a new era in interaction and intelligence – achievements made possible by your ingenuity and craftsmanship. We didnât merely build smarter models, we fundamentally changed how Al systems learn and reason through complex problems.” Altman tweeted a reply to Murati’s departing message on Wednesday which gave no indication that she was leaving on bad terms. “Itâs hard to overstate how much Mira has meant to OpenAI, our mission, and to us all personally,” Altman tweeted. “I feel tremendous gratitude towards her for what she has helped us build and accomplish, but I most of all feel personal gratitude towards her for the support and love during all the hard times. I am excited for what sheâll do next.” I replied with this. Mira, thank you for everything. Itâs hard to overstate how much Mira has meant to OpenAI, our mission, and to us all personally. I feel tremendous gratitude towards her for what she has helped us build and accomplish, but I most of all feel personalâ¦ — Sam Altman (@sama) September 25, 2024  Murati’s tenure included some controversial moments, like when she was asked by the Wall Street Journal back in March about whether OpenAI, which makes ChatGPT, uses YouTube for training data for the company’s AI video generator, Sora. Murati made an awkward face that became famous as a joke online. Murati briefly served as an interim CEO for OpenAI during the failed ouster of Altman by the board of directors back in November 2023. Altman, who found powerful allies at Microsoft and in the halls of Congress, was reinstated but it’s still not entirely clear what caused the board of directors to originally push him out. “I will forever be grateful for the opportunity to build and work alongside this remarkable team. Together, weâve pushed the boundaries of scientific understanding in our quest to improve human well-being,” Murati wrote in her departing tweet. “While I may no longer be in the trenches with you, I will still be rooting for you all,” Murati continued. “With deep gratitude for the friendships forged, the triumphs achieved, and most importantly, the challenges overcome together.” AiArtificial intelligenceChatGPTOpenAI Get the best tech, science, and culture news in your inbox daily. News from the future, delivered to your present. Please select your desired newsletters and submit your email to upgrade your inbox. The Treasury says it identified or recovered $4 billion in fraudulent payments last fiscal year, a sixfold increase over the previous year. Amazon, Google, and Microsoft are all pushing for a nuclear-power fueled future. It's a huge gamble. These are all the Apple Intelligence features coming to iPhone on October 28. A business school is using AI doomerism, money from Saudi Arabia, and a dusty Cold War metaphor to get people hyped about AIâs future. A series of reports from multiple federal agencies has outlined the challenges it faces in the rush to adopt AI. The robots were interacting with guests at Tesla's big Cybercab event Thursday in L.A.  Discover the Winners of the 2024 Gizmodo Science Fair â We may earn a commission when you buy through links on our sites.
Â©2024 GIZMODO USA LLC. All rights reserved. Mode Follow us Mode Follow us ","build, feel, gratitude, mira, wrote, openai, murati, leaves, restructures, altman, cto, forprofit, muratis, company",Gizmodo
18,DoNotPay Has to Pay Up Over ‘World’s First Robot Lawyer’,Matt Novak,2024-09-25,https://gizmodo.com/donotpay-has-to-pay-up-over-worlds-first-robot-lawyer-2000503265,"The company never tested the accuracy of its AI lawyer with a real human lawyer, according to the FTC.","DoNotPay, the AI company best known for helping users cancel unwanted subscriptions and fight parking tickets, has reached a settlement with the FTC over an AI chatbot that was advertised as “the world’s first robot lawyer.” The AI lawyer didn’t live up to its claims and DoNotPay never properly tested the chatbot tool, according to the FTC. In fact, the company reportedly never even hired a real human lawyer to work on the product. DoNotPay has agreed to pay $193,000 and will be required to send a notice to any previous customers who used the AI lawyer from 2021 to 2023 warning about the limitations of the subscription service. The settlement will also “prohibit the company from making claims about its ability to substitute for any professional service without evidence to back it up,” according to a statement Wednesday from the FTC. The AI lawyer billed itself as a tool that could be used for things like suing for assault in small claims court and drafting a cease and desist letter. The AI tool also created legal documents like NDAs, business contracts, prenuptial agreements, and custody agreements, according to the FTC. DoNotPay had initially planned to use its AI lawyer in a physical courtroom but shut down that plan in early 2023 after receiving threats from State Bar associations. The FTC complaint, which is available to read online, is filled with some rather odd details, including a quote that appeared on the DoNotPay website that purported to be from the Los Angeles Times: âWhat this robot lawyer can do is astonishingly similarâif not moreâto what human lawyers do.â In reality, the quote was from the Los Angeles Times High School Insider website, a user-generated content platform for high school students, according to the FTC. The tool also wasn’t tested in a way that any reasonable person might expect, according to the FTC complaint: DoNotPay employees have not tested the quality and accuracy of the legal documents and advice generated by most of the Serviceâs law-related features. DoNotPay has not employed attorneys and has not retained attorneys, let alone attorneys with the relevant legal expertise, to test the quality and accuracy of the Serviceâs law-related features. The founder of the AI company, Joshua Browder, has previously said DoNotPay wants to, âreplace the $200-billion-dollar legal industry with artificial intelligence.” Browder dropped out of college in 2018 as part of the Thiel Fellowship, a program started by tech investor Peter Thiel that gives people money to drop out of college and start a business. For its part, DoNotPay was quick to note the company has not admitted any liability in the settlement with the FTC. The news about action against DoNotPay was part of a broader announcement Wednesday by the FTC centered on cracking down on AI. And FTC chair Lina Khan, a figure who’s adored by progressive Democrats and despised by many large businesses for her recent enforcement actions, says using AI doesn’t provide any kind of exemption to existing laws. âUsing AI tools to trick, mislead, or defraud people is illegal,â Khan said in a statement posted to the agency’s website. âThe FTCâs enforcement actions make clear that there is no AI exemption from the laws on the books,” Khan continued. “By cracking down on unfair or deceptive practices in these markets, FTC is ensuring that honest businesses and innovators can get a fair shot and consumers are being protected.â AiDoNotPayJoshua BrowderPeter ThielThiel Fellowship Get the best tech, science, and culture news in your inbox daily. News from the future, delivered to your present. Please select your desired newsletters and submit your email to upgrade your inbox. The Treasury says it identified or recovered $4 billion in fraudulent payments last fiscal year, a sixfold increase over the previous year. Amazon, Google, and Microsoft are all pushing for a nuclear-power fueled future. It's a huge gamble. These are all the Apple Intelligence features coming to iPhone on October 28. A business school is using AI doomerism, money from Saudi Arabia, and a dusty Cold War metaphor to get people hyped about AIâs future. A series of reports from multiple federal agencies has outlined the challenges it faces in the rush to adopt AI. The robots were interacting with guests at Tesla's big Cybercab event Thursday in L.A.  Discover the Winners of the 2024 Gizmodo Science Fair â We may earn a commission when you buy through links on our sites.
Â©2024 GIZMODO USA LLC. All rights reserved. Mode Follow us Mode Follow us ","donotpay, robot, worlds, pay, statement, ai, company, according, tested, tool, legal, lawyer, ftc",Gizmodo
19,Meta’s AI Can Now Deepfake Your Videos and Fill Your Feed With Slop,Todd Feathers,2024-09-25,https://gizmodo.com/metas-ai-can-now-deepfake-your-videos-and-fill-your-feed-with-slop-2000503100,Zuckerberg also announced that users can begin talking to Meta AI and it will talk back in the voices of celebrities.,"Meta on Wednesday unveiled a new suite of generative AI features that seem designed to fill Facebook and Instagram feeds with even more of the AI-generated engagement bait and scams that users have been complaining about. During the company’s Connect event, CEO Mark Zuckerberg announced that Meta AI can now process image inputs, allowing it to answer questions about photos and edit them. Want to know what kind of flower you took a picture of or how to make a multi-colored cake? Just ask Meta AI. See a picture of a goat on your Instagram feed and want to repost it as a picture of a goat on a surfboard? Just ask Meta AI. Not sure what to say about the goat on a surfboard? Don’t worry, Meta AI will also suggest captions for your stories on Facebook and Instagram. And if chatting with Meta AI yourself sounds like too much work, the company will be testing a new feature on Facebook and Instagram that injects unsolicited AI-generated images “based on your interests or current trends” directly into your feeds, allowing you to “tap a suggested prompt to take that content in a new direction or swipe to imagine new content in real-time,” according to pre-event briefing materials. Researchâalbeit on older models than Meta’s current offeringsâhas shown that creating an image is among the most energy-intensive tasks performed by generative AI. In some cases, a single image consumes the same amount of power as fully charging a smartphone. Not to be outdone by OpenAI’s release of a voice assistant for ChatGPT earlier this week, Zuckerberg announced that users can begin talking to Meta AI and it will talk back in the voices of celebrities like Awkwafina, Dame Judi Dench, John Cena, Keegan Michael Key, and Kristen Bell. The company is also beginning “small tests” in the U.S. and Latin America of a deepfake feature that will translate video content on Instagram and Facebook from Spanish to English, or vice versa. The Meta AI translation tool will automatically “simulate the speaker’s voice in another language and sync their lips to match,” according to the company’s briefing material. Meta did not immediately respond to questions about whether creators on Facebook and Instagram will be asked for their consent before their images and voices are manipulated. During his keynote at the Connect event, Zuckerberg also announced the release of the 3.2 versions of the company’s open-source Llama family of foundational generative models. The lighterweight versions, dubbed Llama 3.2 1B and 3B, are designed to require less computing power so they can run locally on devices while the heftier Llama 3.2 11B and 90B models will be able to process images for tasks such as determining which month a business had the most sales based on a graph of monthly sales. AiFacebookInstagramLlamaMark ZuckerbergMETA Get the best tech, science, and culture news in your inbox daily. News from the future, delivered to your present. Please select your desired newsletters and submit your email to upgrade your inbox. The Treasury says it identified or recovered $4 billion in fraudulent payments last fiscal year, a sixfold increase over the previous year. Amazon, Google, and Microsoft are all pushing for a nuclear-power fueled future. It's a huge gamble. These are all the Apple Intelligence features coming to iPhone on October 28. A business school is using AI doomerism, money from Saudi Arabia, and a dusty Cold War metaphor to get people hyped about AIâs future. CalMatters and The Markup used Facebookâs AI model to count the millions of dollars it makes after violent news events. A series of reports from multiple federal agencies has outlined the challenges it faces in the rush to adopt AI.  Discover the Winners of the 2024 Gizmodo Science Fair â We may earn a commission when you buy through links on our sites.
Â©2024 GIZMODO USA LLC. All rights reserved. Mode Follow us Mode Follow us ","instagram, ai, metas, slop, videos, meta, models, deepfake, zuckerberg, images, picture, facebook, feed, llama, image",Gizmodo
