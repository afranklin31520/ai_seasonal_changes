,title,author,url,summary,full_text,key_words,pub_date
0,AI winter is well on its way,,https://blog.piekniewski.info/2018/05/28/ai-winter-is-well-on-its-way/,"Deep learning (does not) scaleOne of the key slogans repeated about deep learning is that it scales almost effortlessly. Initially it was thought that end-to-end deep learning could somehow solve this problem, a premise particularly heavily promoted by Nvidia. In such situation it is easy to learn spurious relations, as exemplified by adversarial examples in deep learning. For those who missed it, he has excellent blog posts/papers ""Deep learning: A critical appraisal"" and ""In defense of skepticism about deep learning"", where he very meticulously deconstructs the deep learning hype. I respect Gary a lot, he behaves like a real scientist should, while most so called ""deep learning stars"" just behave like cheap celebrities.","Deep learning has been at the forefront of the so called AI revolution for quite a few years now, and many people had believed that it is the silver bullet that will take us to the world of wonders of technological singularity (general AI). Many bets were made in 2014, 2015 and 2016 when still new boundaries were pushed, such as the Alpha Go etc. Companies such as Tesla were announcing through the mouths of their CEO's that fully self driving car was very close, to the point that Tesla even started selling that option to customers [to be enabled by future software update].  We have now mid 2018 and things have changed. Not on the surface yet, NIPS conference is still oversold, the corporate PR still has AI all over its press releases, Elon Musk still keeps promising self driving cars and Google CEO keeps repeating Andrew Ng's slogan that AI is bigger than electricity. But this narrative begins to crack. And as I predicted in my older post, the place where the cracks are most visible is autonomous driving - an actual application of the technology in the real world.  The dust settled on deep learning  When the ImageNet was effectively solved (note this does not mean that vision is solved), many prominent researchers in the field (including even typically quiet Geoff Hinton) were actively giving press interviews, publicizing stuff on social media (e.g. Yann Lecun, Andrew Ng, Fei-Fei Li to name a few). The general tone was that we are in front of a gigantic revolution and from now on things can only accelerate. Well years have passed and the twitter feeds of those people became less active, as exemplified by Andrew Ng below:  2013 - 0.413 tweets per day  2014 - 0.605 tweets per day  2015 - 0.320 tweets per day  2016 - 0.802 tweets per day  2017 - 0.668 tweets per day  2018 - 0.263 tweets per day (until 24 May)  Perhaps this is because Andrew's outrageous claims are now put to more scrutiny by the community as indicated in a tweet below:  Visibly the sentiment has quite considerably declined, there are much fewer tweets praising deep learning as the ultimate algorithm, the papers are becoming less ""revolutionary"" and much more ""evolutionary"". Deepmind hasn't shown anything breathtaking since their Alpha Go zero [and even that wasn't that exciting, given the obscene amount of compute necessary and applicability to games only - see Moravec's paradox]. OpenAI was rather quiet, with their last media outburst being the Dota 2 playing agent [which I suppose was meant to create as much buzz as Alpha Go, but fizzled out rather quickly]. In fact articles began showing up that even Google in fact does not know what to do with Deepmind, as their results are apparently not as practical as originally expected... As for the prominent researchers, they've been generally touring around meeting with government officials in Canada or France to secure their future grants, Yann Lecun even stepped down (rather symbolically) from the Head of Research to Chief AI scientist at Facebook. This gradual shift from rich, big corporations to government sponsored institutes suggests to me that the interest in this kind of research within these corporations (I think of Google and Facebook) is actually slowly winding down. Again these are all early signs, nothing spoken out loud, just the body language.  Deep learning (does not) scale  One of the key slogans repeated about deep learning is that it scales almost effortlessly. We had the AlexNet in 2012 which had ~60M parameters, we probably now have models with at least 1000x that number right? Well probably we do, the question however is - are these things 1000x as capable? Or even 100x as capable? A study by openAI comes in handy:  So in terms of applications for vision we see that VGG and Resnets saturated somewhat around one order of magnitude of compute resources applied (in terms of number of parameters it is actually less). Xception is a variation of google inception architecture and actually only slightly outperforms inception on ImageNet, arguably actually slightly outperforms everyone else, because essentially AlexNet solved ImageNet. So at 100 times more compute than AlexNet we pretty much saturated architectures in terms of vision, or image classification to be precise. Neural machine translation is a big effort by all the big web search players and no wonder it takes all the compute it can take (and yet google translate still sucks, though has gotten arguably better). The latest three points on that graph, interestingly show reinforcement learning related projects, applied to games by Deepmind and OpenAI. Particularly AlphaGo Zero and slightly more general AlphaZero take ridiculous amount of compute, but are not applicable in the real world applications because much of that compute is needed to simulate and generate the data these data hungry models need. OK, so we can now train AlexNet in minutes rather than days, but can we train a 1000x bigger AlexNet in days and get qualitatively better results? Apparently not...  So in fact, this graph which was meant to show how well deep learning scales, indicates the exact opposite. We can't just scale up AlexNet and get respectively better results - we have to fiddle with specific architectures, and effectively additional compute does not buy much without order of magnitude more data samples, which are in practice only available in simulated game environments.  Self driving crashes  By far the biggest blow into deep learning fame is the domain of self driving vehicles (something I anticipated for a long time, see e.g. this post from 2016). Initially it was thought that end-to-end deep learning could somehow solve this problem, a premise particularly heavily promoted by Nvidia. I don't think there is a single person on Earth who still believes that, though I could be wrong. Looking at last year California DMV disengagement reports Nvidia car could not drive literally ten miles without a disengagement. In a separate post I discuss the general state of that development and comparison to human driver safety, which (spoiler alert) is not looking good. Since 2016 there were also several Tesla AutoPilot incidents [1, 2, 3] a few of which were fatal [1, 2]. Arguably Tesla Autopilot should not be confused with self driving (it is), but at least at the core it relies on the same kind of technology. As of today, aside from occasional spectacular errors [1], it still cannot stop at an intersection, recognize a traffic light, or even navigate through a roundabout. That is in May 2018, several months after the promised coast to coast Tesla autonomous drive (did not happen although the rumor is they've tried but could not get it to work without ~30 disengagements). Several months ago (in February 2018) Elon Musk repeated in a conference call when asked about the coast to coast drive that:  ""We could have done the coast-to-coast drive, but it would have required too much specialized code to effectively game it or make it somewhat brittle and that it would work for one particular route, but not the general solution. So I think we would be able to repeat it, but if it’s just not any other route, which is not really a true solution. (…)  I am pretty excited about how much progress we’re making on the neural net front. And it’s a little – it’s also one of those things that’s kind of exponential where the progress doesn’t seem – it doesn’t seem like much progress, it doesn’t seem like much progress, and suddenly wow. It will feel like well this is a lame driver, lame driver. Like okay, that’s a pretty good driver. Like ‘Holy Cow!’ this driver’s good. It’ll be like that,”  Well, looking at the graph above (from OpenAI) I seem not be seeing that exponential progress. Neither is it visible in miles before disengagement for pretty much every big player in this field. In essence the above statement should be interpreted: ""We currently don't have the technology that could safely drive us coast to coast, though we could have faked it if we really wanted to (maybe...). We deeply hope that some sort of exponential jump in capabilities of neural networks will soon happen and save us from disgrace and massive lawsuits"".  But by far the biggest pin punching through the AI bubble was the accident in which Uber self driving car killed a pedestrian in Arizona. From the preliminary report by the NTSB we can read some astonishing statements:  Aside from general system design failure apparent in this report, it is striking that the system spent long seconds trying to decide what exactly it sees in front (whether that be a pedestrian, bike, vehicle or whatever else) rather than making the only logical decision in these circumstances, which was to make sure not to hit it. There are several reasons for it: first, people will often verbalize their decisions post factum. So a human will typically say: ""I saw a cyclist therefore I veered to the left to avoid him"". Huge amount of psychophysical literature will suggest a quite different explanation: a human saw something which was very quickly interpreted as a obstacle by fast perceptual loops of his nervous systems and he performed a rapid action to avoid it, long seconds later realizing what happened and providing a verbal explanation"". There are tons of decisions we make every day that are not verbalized, and driving includes many of them. Verbalization is costly and takes time and reality often does not provide that time. These mechanisms have evolved for a billion years to keep us safe, and driving context (although modern) makes use of many such reflexes. And since these reflexes have not evolved specifically for driving, they may induce mistakes. A knee jerk reaction to a wasp buzzing in a car may have caused many crashes and deaths. But our general understanding of 3d space, speed, ability to predict the behavior of agents, behavior of physical objects traversing through our path are the primitive skills, that were just as useful 100 million years ago as they are today and they've been honed really sharp by evolution.  But because most of these things are not easily verbalizable, they are hard to measure, and consequently we don't optimize our machine learning systems on these aspects at all [see my earlier post for benchmark proposals that would address some of these capabilities]. Now this would speak in favor of Nvidia end-to-end approach - learn image->action mapping, skipping any verbalization, and in some ways this is the right way to do it but... the problem is that the input space is incredibly high dimensional, while the action space is very low dimensional. Hence the ""amount"" of ""label"" (readout) is extremely small compared to the amount of information coming in. In such situation it is easy to learn spurious relations, as exemplified by adversarial examples in deep learning. A different paradigm is needed and I postulate prediction of the entire perceptual input along with the action as a first step to make a system able to extract the semantics of the world, rather than spurious correlations [read more about my first proposed architecture called Predictive Vision Model].  In fact if there is anything at all we learned from the outburst of deep learning, is that (10k+ dimensional) image space has plenty enough spurious patterns in it, that they actually generalize across many images and make the impression like our classifiers actually understand what they are seeing. Nothing could be further from the truth, as admitted even by the top researchers who are heavily invested in this field. In fact many top researchers should not be too outraged by my observations, Yann Lecun warned about overexcitement and AI winter for a while, even Geoffrey Hinton - the father of the current outburst of backpropagation - admitted in an interview that this likely is all a dead end, and we need to start over. At this point though, the hype is so strong that nobody will listen, even to the founding fathers of the field.  Gary Marcus and his quest against the hype  I should mention that more top tier people are recognizing the hubris and have the courage to openly call it. One of the most active in that space is Gary Marcus. Although I don't think I agree with everything that Gary proposes in terms of AI, we certainly agree that it is not yet as powerful as painted by the deep learning hype-propaganda. In fact it is not even any close. For those who missed it, he has excellent blog posts/papers ""Deep learning: A critical appraisal"" and ""In defense of skepticism about deep learning"", where he very meticulously deconstructs the deep learning hype. I respect Gary a lot, he behaves like a real scientist should, while most so called ""deep learning stars"" just behave like cheap celebrities.  Conclusion  Predicting the A.I. winter is like predicting a stock market crash - impossible to tell precisely when it happens, but almost certain that it will at some point. Much like before a stock market crash, there are signs of the impending collapse, but the narrative is so strong that it is very easy to ignore them, even if they are in plain sight. In my opinion there are such signs of a huge decline in deep learning (and probably in AI in general as this term has been abused 'ad nauseam' by corporate propaganda) already visible. Visible in plain sight, yet hidden from the majority by an increasingly intense narrative. How ""deep"" will this winter be? I have no idea. What will come next? I have no idea. But I'm fairly positive it is coming, perhaps sooner rather than later.  If you found an error, highlight it and press Shift + Enter or click here to inform us.  Related  Comments  comments","driving, tweets, winter, general, coast, compute, way, alexnet, fact, learning, ai, deep",2018-05-28 00:00:00
1,The Prospect of an AI Winter,,https://www.erichgrunewald.com/posts/the-prospect-of-an-ai-winter/,"The Prospect of an AI WinterWilliam Eden forecasts an AI winter. The Prospect of a New AI Winter #What does a speculative bubble look like from the inside? Meaning: plausibly, even if expectations for LLMs and other AI systems are mostly unmet, there still won’t be an AI winter comparable to previous winters as investment plateaus rather than declines. Reasons Why There Could Be a Winter After All #Everything I’ve written so far is premised on something like “any AI winter would be caused by AI systems’ ceasing to get more practically useful and therefore profitable”. At this point I would put only 5% on an AI winter happening by 2030, where AI winter is operationalised as a drawdown in annual global AI investment of ≥50%.","The Prospect of an AI Winter  William Eden forecasts an AI winter. He argues that AI systems (1) are too unreliable and too inscrutable, (2) won’t get that much better (mostly due to hardware limitations) and/or (3) won’t be that profitable. He says, “I’m seeing some things that make me think we are in a classic bubble scenario, and lots of trends that can’t clearly continue.”  I put 5% on an AI winter happening by 2030, with all the robustness that having written a blog post inspires, and where AI winter is operationalised as a drawdown in annual global AI investment of ≥50%. (I reckon a winter must feature not only decreased interest or excitement, but always also decreased funding, to be considered a winter proper.)  There have been two previous winters, one 1974-1980 and one 1987-1993. The main factor causing these seems to have been failures to produce formidable results, and as a consequence wildly unmet expectations. Today’s state-of-the-art AI systems show impressive results and are more widely adopted (though I’m not confident that the lofty expectations people have for AI today will be met).  I think Moore’s Law could keep going for decades. But even if it doesn’t, there are many other areas where improvements are being made allowing AI labs to train ever larger models: there’s improved yields and other hardware cost reductions, improved interconnect speed and better utilisation, algorithmic progress and, perhaps most importantly, an increased willingness to spend. If 1e35 FLOP is enough to train a transformative AI (henceforth, TAI) system, which seems plausible, I think we could get TAI by 2040 (>50% confidence), even under fairly conservative assumptions. (And a prolonged absence of TAI wouldn’t necessarily bring about an AI winter; investors probably aren’t betting on TAI, but on more mundane products.)  Reliability is definitely a problem for AI systems, but not as large a problem as it seems, because we pay far more attention to frontier capabilities of AI systems (which tend to be unreliable) than long-familiar capabilities (which are pretty reliable). If you fix your gaze on a specific task, you usually see a substantial and rapid improvement in reliability over the years.  I reckon inference with GPT-3.5-like models will be about as cheap as search queries are today in about 3-6 years. I think ChatGPT and many other generative models will be profitable within 1-2 years if they aren’t already. There’s substantial demand for them (ChatGPT reached 100M monthly active users after two months, quite impressive next to Twitter’s ~450M) and people are only beginning to explore their uses.  If an AI winter does happen, I’d guess some of the more likely reasons would be (1) scaling hitting a wall, (2) deep-learning-based models being chronically unable to generalise out-of-distribution and/or (3) AI companies running out of good-enough data. I don’t think this is very likely, but I would be relieved if it were the case, given that we as a species currently seem completely unprepared for TAI.  The Prospect of a New AI Winter #  What does a speculative bubble look like from the inside? Trick question – you don’t see it.  Or, I suppose some people do see it. One or two may even be right, and some of the others are still worth listening to. William Eden tweeting out a long thread explaining why he’s not worried about risks from advanced AI is one example, I don’t know of which. He argues in support of his thesis that another AI winter is looming, making the following points:  AI systems aren’t that good. In particular (argues Eden), they are too unreliable and too inscrutable. It’s far harder to achieve three or four nines reliability than merely one or two nines; as an example, autonomous vehicles have been arriving for over a decade. The kinds of things you can do with low reliability don’t capture most of the value. AI systems won’t get that much better. Some people think we can scale up current architectures to AGI. But, Eden says, we may not have enough compute to get there. Moore’s law is “looking weaker and weaker”, and price-performance is no longer falling exponentially. We’ll most likely not get “more than another 2 orders of magnitude” of compute available globally, and 2 orders of magnitude probably won’t get us to TAI. “Without some major changes (new architecture/paradigm?) this looks played out.” Besides, the semiconductor supply chain is centralised and fragile and could get disrupted, for example by a US-China war over Taiwan. AI products won’t be that profitable. AI systems (says Eden) seem good for “automating low cost/risk/importance work”, but that’s not enough to meet expectations. (See point (1) on reliability and inscrutability.) Some applications, like web search, have such low margins that the inference costs of large ML models are prohibitive.  I’ve left out some detail and recommend reading the entire thread before proceeding. Also before proceeding, a disclosure: my day job is doing research on the governance of AI, and so if we’re about to see another AI winter, I’d pretty much be out of a job, as there wouldn’t be much to govern anymore. That said, I think an AI winter, while not the best that can happen, is vastly better than some of the alternatives, axiologically speaking. I also think I’d be of the same opinion even if I had still worked as a programmer today (assuming I had known as much or little about AI as I actually do).  Past Winters #  There is something of a precedent.  The first AI winter – traditionally, from 1974 to 1980 – was precipitated by the unsympathetic Lighthill report. More fundamentally it was caused by AI researchers’ failure to achieve their grandiose objectives. In 1965, Herbert Simon famously predicted that AI systems would be capable of any work a human can do in 20 years, and Marvin Minsky wrote in 1967 that “within a generation […] the problem of creating ‘artificial intelligence’ will be substantially solved”. Of Frank Rosenblatt’s Perceptron Project the New York Times reported (claims of Rosenblatt which aroused ire among other AI researchers due to their extravagance), “[It] revealed an embryo of an electronic computer that it expects will be able to walk, talk, see, write, reproduce itself and be conscious of its existence. Later perceptrons will be able to recognize people and call out their names and instantly translate speech in one language to speech and writing in another language, it was predicted” (Olazaran 1996). Far from human intelligence, not even adequate machine translation materialised (it took until the mid-2010s when DeepL and Google Translate’s deep learning upgrade were released for that to happen).  The second AI winter – traditionally, from 1987 to 1993 – again followed unrealised expectations. This was the era of expert systems and connectionism (in AI, the application of artificial neural networks). But expert systems failed to scale, and neural networks learned slowly, had low accuracy and didn’t generalise. It was not the era of 1e9 FLOP/s per dollar; I reckon the LISP machines of the day were ~6-7 orders of magnitude less price-performant than that.  Wikipedia lists a number of factors behind these winters, but to me it is the failure to actually produce formidable results that seems most important. Even in an economic downturn, and even with academic funding dried up, you still would’ve seen substantial investments in AI had it shown good results. Expert systems did have some success, but nowhere near what we see AI systems do today, and with none of the momentum but all of the brittleness. This seems like an important crux to me: will AI systems fulfil the expectations investors have for them?  Moore’s Law and the Future of Compute #  Improving these days means scaling up. One reason why scaling might fail is if the hardware that is used to train AI models stops improving.  Moore’s Law is the dictum that the number of transistors on a chip will double every ~2 years, and as a consequence hardware performance is able to double every ~2 years (Hobbhahn and Besiroglu 2022). (Coincidentally, Gordon Moore died last week at the age of 94, survived by his Law.) It’s often claimed that Moore’s Law will slow as the size of transistors (and this fact never ceases to amaze me) approaches the silicon atom limit. In Eden’s words, Moore’s Law looks played out.  I’m no expert at semiconductors or GPUs, but as I understand things it’s (1) not a given that Moore’s Law will fail in the next decade and (2) quite possible that, even if it does, hardware performance will keep running on improvements other than increased transistor density. It wouldn’t be the first time something like this happened: single-thread performance went off-trend as Dennard scaling failed around 2005, but transistor counts kept rising thanks to increasing numbers of cores:  Some of the technologies that could keep GPU performance going as the atom limit approaches include vertical scaling, advanced packaging, new transistor designs and 2D materials as well as improved architectures and connectivity. (To be clear, I don’t have a detailed picture of what these things are, I’m mostly just deferring to the linked source.) TSMC, Samsung and Intel all have plans for <2 nm process nodes (the current SOTA is 3 nm). Some companies are exploring more out-there solutions, like analog computing for speeding up low-precision matrix multiplication. Technologies on exponential trajectories are always out of far-frontier ideas, until they aren’t (at least so long as there is immense pressure to innovate, as for semiconductors there is). Peter Lee said in 2016, “The number of people predicting the death of Moore’s law doubles every two years.” By the end of 2019, the Metaculus community gave “Moore’s Law will end by 2025” 58%, whereas now one oughtn’t give it more than a few measly per cent.  Is Transformative AI on the Horizon? #  But the main thing we care about here is not FLOP/s, and not even FLOP/s per dollar, but how much compute AI labs can afford to pour into a model. That’s affected by a number of things beyond theoretical peak performance, including hardware costs, energy efficiency, line/die yields, utilisation and the amount of money that a lab is willing to spend. So will we get enough compute to train a TAI in the next few decades?  There are many sophisticated attempts to answer that question – here’s one that isn’t, but that is hopefully easier to understand.  Daniel Kokotajlo imagines what you could do with 1e35 FLOP of compute on current GPU architectures. That’s a lot of compute – about 11 orders of magnitude more than what today’s largest models were trained with (Sevilla et al. 2022). The post gives a dizzying picture of just how much you can do with such an abundance of computing power. Now it’s true that we don’t know for sure whether scaling will keep working, and it’s also true that there can be other important bottlenecks besides compute, like data. But anyway something like 1e34 to 1e36 of 2022-compute seems like it could be enough to create TAI.  Entertain that notion and make the following assumptions:  The price-performance of AI chips seems to double every 1.5 to 3.1 years (Hobbhahn and Besiroglu 2022); assume that that’ll keep going until 2030, after which the doubling time will double as Moore’s Law fails.  Algorithmic progress on ImageNet seems to effectively halve compute requirements every 4 to 25 months (Erdil and Besiroglu 2022); assume that the doubling time is 50% longer for transformers.  Spending on training runs for ML systems seems to roughly double every 6 to 10 months; assume that that’ll continue until we reach a maximum of $10B.  What all that gives you is 50% probability of TAI by 2040, and 80% by 2045:  That is a simple model of course. There’s a far more sophisticated and rigorous version, namely Cotra (2020) which gives a median of ~2050 (though she’s since changed her best guess to a median of ~2040). There are many reasons why my simple model might be wrong:  Scaling laws may fail and/or, as models get larger, scaling may get increasingly harder at a rate that exceeds ML researchers’ efforts to make scaling less hard.  Scaling laws may continue to hold but a model trained with 1e35 2022-FLOP does not prove transformative. Either more compute is needed, or new architectures are needed.  1e35 FLOP may be orders of magnitude more than what is needed to create TAI. For example, this Metaculus question has a community prediction of 1e28 to 1e33 FLOP for the largest training run prior to the first year in which GWP growth exceeds 30%; plugging that range into the model as a 90% CI gives a terrifying median estimate of 2029.  Hardware price-performance progress slows more and/or earlier than assumed, or slows less and/or later than assumed.  The pace of algorithmic advancements may slow down or increase, or the doubling time of algorithmic progress for prospective-transformative models may be lower or greater than estimated.  ML researchers may run out of data, or may run out of high-quality (like books, Wikipedia) or even low-quality (like Reddit) data; see e.g. Villalobos et al. (2022) which forecasts high-quality text data being exhausted in 2024 or thereabouts, or Chinchilla’s wild implications and the discussion there.  A severe extreme geopolitical tail event, such as a great power conflict between the US and China, may occur.  Increasingly powerful AI systems may help automate or otherwise speed up AI progress.  Social resistance and/or stringent regulations may diminish investment and/or hinder progress.  Unknown unknowns arise.  Still, I really do think a 1e35 2022-FLOP training run could be enough (>50% likely, say) for TAI, and I really do think, on roughly this model, we could get such a training run by 2040 (also >50% likely). One of the main reasons why I think so is that as AI systems get increasingly more powerful and useful (and dangerous), incentives will keep pointing in the direction of AI capabilities increases, and funding will keep flowing into efforts to keep scaling laws going. And if TAI is on the horizon, that suggests capabilities (and as a consequence, business opportunities) will keep improving.  You Won’t Find Reliability on the Frontier #  One way that AI systems can disappoint is if it turn out they are, and for the forseeable future remain, chronically unreliable. Eden writes, “[Which] areas of the economy can deal with 99% correct solutions? My answer is: ones that don’t create/capture most of the value.” And people often point out that modern AI systems, and large language models (henceforth, LLMs) in particular, are unreliable. (I take reliable to mean something like “consistently does what you expect, i.e. doesn’t fail”.) This view is both true and false:  AI systems are highly unreliable if you only look at frontier capabilities. At any given time, an AI system will tend to succeed only some of the time at the <10% most impressive tasks it is capable of. These tasks are the ones that will get the most attention, and so the system will seem unreliable.  AI systems are pretty reliable if you only look at long-familiar capabilities. For any given task, successive generations of AI systems will generally (not always) get better and better at it. These tasks are old news: we take it for granted that AIs will do them correctly.  John McCarthy lamented: “As soon as it works, no one calls it AI anymore.” Larry Tesler declared: “AI is whatever hasn’t been done yet.”  Take for example the sorting of randomly generated single-digit integer lists. Two years ago janus tested this on GPT-3 and found that, even with a 32-shot (!) prompt, GPT-3 managed to sort lists of 5 integers only 10/50 times, and lists of 10 integers 0/50 times. (A 0-shot, Python-esque prompt did better at 38/50 and 2/50 respectively). I tested the same thing with ChatGPT using GPT-3 and it got it right 5/5 times for 10-integer lists. I then asked it to sort five 10-integer lists in one go, and it got 4/5 right! (NB: I’m pretty confident that this improvement didn’t come with ChatGPT exactly, but rather with the newer versions of GPT-3 that ChatGPT is built on top of.)  (Eden also brings up the problem of accountability. I agree that this is an issue. Modern AI systems are basically inscrutable. That is one reason why it is so hard to make them safe. But I don’t expect this flaw to stop AI systems from being put to use in any except the most safety-critical domains, so long as companies expect those systems to win them market dominance and/or make a profit.)  Autonomous Driving #  But then why are autonomous vehicles (henceforth, AVs) still not reliable enough to be widely used? I suspect because driving a car is not a single task, but a task complex, a bundle of many different subtasks with varying inputs. The overall reliability of driving is highly dependent on the performance of those subtasks, and failure in any one of them could lead to overall failure. Cars are relatively safety-critical: to be widely adopted, autonomous cars need to be able to reliably perform ~all subtasks you need to master to drive a car. As the distribution of the difficulties of these subtasks likely follows a power law (or something like it), the last 10% will always be harder to get right than the first 90%, and progress will look like it’s “almost there” for years before the overall system is truly ready, as has also transparently been the case for AVs. I think this is what Eden is getting at when he writes that it’s “hard to overstate the difference between solving toy problems like keeping a car between some cones on an open desert, and having a car deal with unspecified situations involving many other agents and uncertain info navigating a busy city street”.  This seems like a serious obstacle for more complex AI applications like driving. And what we want AI for is complicated tasks – simple tasks are easy to automate with traditional software. I think this is some reason to think an AI winter is more likely, but only a minor one.  One, I don’t think what has happened to AVs amounts to an AV winter. Despite expectations clearly having been unmet, and public interest clearly having declined, my impression (though I couldn’t find great data on this) is that investment in AVs hasn’t declined much, and maybe not at all (apparently 2021 saw >$12B of funding for AV companies, above the yearly average of the past decade ), and also that AV patents are steadily rising (both in absolute numbers and as a share of driving technology patents). Autonomous driving exists on a spectrum anyway; we do have “conditionally autonomous” L3 features like cruise control and auto lane change in cars on the road today, with adoption apparently increasing every year. The way I see it, AVs have undergone the typical hype cycle, and are now by steady, incremental change climbing the so-called slope of enlightenment. Meaning: plausibly, even if expectations for LLMs and other AI systems are mostly unmet, there still won’t be an AI winter comparable to previous winters as investment plateaus rather than declines.  Two, modern AI systems, and LLMs specifically, are quite unlike AVs. Again, cars are safety-critical machines. There’s regulation, of course. But people also just don’t want to get in a car that isn’t highly reliable (where highly reliable means something like “far more reliable than an off-brand charger”). For LLMs, there’s no regulation, and people are incredibly motivated to use them even in the absence of safeguards (in fact, especially in the absence of safeguards). I think there are lots of complex tasks that (1) aren’t safety-critical (i.e., where accidents aren’t that costly) but (2) can be automated and/or supported by AI systems.  Costs and Profitability #  Part of why I’m discussing TAI is that it’s probably correlated with other AI advancements, and part is that, despite years of AI researchers’ trying to avoid such expectations, people are now starting to suspect that AI labs will create TAI in this century. Investors mostly aren’t betting on TAI – as I understand it, they generally want a return on their investment in <10 years, and had they expected AGI in the next 10-20 years they would have been pouring far more than some measly hundreds of millions (per investment) into AI companies today. Instead, they expect – I’m guessing – tools that will broadly speed up labour, automate common tasks and make possible new types of services and products.  Ignoring TAI, will systems similar to ChatGPT, Bing/Sydney and/or modern image generators become profitable within the next 5 or so years? I think they will within 1-2 years if they aren’t already. Surely the demand is there. I have been using ChatGPT, Bing/Sydney and DALL-E 2 extensively since they were released, would be willing to pay non-trivial sums for all these services and think it’s perfectly reasonable and natural to do so (and I’m not alone in this, ChatGPT reportedly having reached 100M monthly active users two months after launch, though this was before the introduction of a paid tier; by way of comparison, Twitter reportedly has ~450M).  Eden writes: “The All-In podcast folks estimated a ChatGPT query as being about 10x more expensive than a Google search. I’ve talked to analysts who carefully estimated more like 3-5x. In a business like search, something like a 10% improvement is a killer app. 3-5x is not in the running!”  An estimate by SemiAnalysis suggests that ChatGPT (prior to the release of GPT-4) costs $700K/day in hardware operating costs, meaning (if we assume 13M active users) ~$0.054/user/day or ~$1.6/user/month (the subscription fee for ChatGPT Plus is $20/user/month). That’s $700K × 365 = $255M/year in hardware operating costs alone, quite a sum, though to be fair these costs likely exceed operational costs, employee salaries, marketing and so on by an order of magnitude or so. OpenAI apparently expects $200M revenue in 2023 and a staggering $1B by 2024.  At the same time, as mentioned in a previous section, the hardware costs of inference are decreasing rapidly: the price-performance of AI accelerators doubles every ~2.1 years (Hobbhahn and Besiroglu 2022). So even if Eden is right that GPT-like models are 3-5x too expensive to beat old-school search engines right now, based on hardware price-performance trends alone that difference will be ~gone in 3-6 years (though I’m assuming there’s no algorithmic progress for inference, and that traditional search queries won’t get much cheaper). True, there will be better models available in future that are more expensive to run, but it seems that this year’s models are already capable of capturing substantial market share from traditional search engines, and old-school search engines seem to be declining in quality rather than improving.  It does seem fairly likely (>30%?) to me that AI companies building products on top of foundation models like GPT-3 or GPT-4 are overhyped. For example, Character.AI recently raised >$200M at a $1B valuation for a service that doesn’t really seem to add much value on top of the standard ChatGPT API, especially now that OpenAI has added the system prompt feature. But as I think these companies may disappoint precisely because they are obsoleted by other, more general AI systems, I don’t think their failure would lead to an AI winter.  Reasons Why There Could Be a Winter After All #  Everything I’ve written so far is premised on something like “any AI winter would be caused by AI systems’ ceasing to get more practically useful and therefore profitable”. AIs being unreliable, hardware price-performance progress slowing, compute for inference being too expensive – these all matter only insofar as they affect the practical usefulness/profitability of AI. I think this is by far the most likely way that an AI winter happens, but it’s not the only plausible way; others possibilities include restrictive legislation/regulation, spectacular failures and/or accidents, great power conflicts and extreme economic downturns.  But if we do see a AI winter within a decade, I think the most likely reason will turn out to be one of:  Scaling hits a wall; the blessings of scale cease past a certain amount of compute/data/parameters. For example, OpenAI trains GPT-5 with substantially more compute, data and parameters than GPT-4, but it just turns out not to be that impressive. There’s no sign of this happening so far, as far as I can see.  For example, OpenAI trains GPT-5 with substantially more compute, data and parameters than GPT-4, but it just turns out not to be that impressive. True out-of-distribution generalisation is far off, even though AIs keep getting better and more reliable at performing in-distribution tasks. This would partly vindicate some of the LLM reductionists. I find it pretty hard to say whether this is the case currently, maybe because the line between in-distribution and out-of-distribution inputs is often blurry. I also think that plausibly there’d be no AI winter in the next decade even if AIs won’t fully generalise out-of-distribution, because in-distribution data covers a lot of economically useful ground.  This would partly vindicate some of the LLM reductionists. We run out of high-quality data (cf. Villalobos et al. (2022)). I’m more unsure about this one, but I reckon ML engineers will find ways around it. OpenAI is already paying workers in LMIC countries to label data; they could pay them to generate data, too. Or you could generate text data from video and audio data. But more likely is perhaps the use of synthetic data. For example, you could generate training data with AIs (cf. Alpaca which was fine tuned on GPT-3-generated texts). ML researchers have surely already thought of these things, there just hasn’t been much of a need to try them yet, because cheap text data has been abundant.    I still think an AI winter looks really unlikely. At this point I would put only 5% on an AI winter happening by 2030, where AI winter is operationalised as a drawdown in annual global AI investment of ≥50%. This is unfortunate if you think, as I do, that we as a species are completely unprepared for TAI.","likely, winter, models, think, prospect, law, compute, systems, data, tai, ai",
2,Self-driving cars are headed toward an AI roadblock,Russell Brandom,https://www.theverge.com/2018/7/3/17530232/self-driving-ai-winter-full-autonomy-waymo-tesla-uber,"There’s growing concern among AI experts that it may be years, if not decades, before self-driving systems can reliably avoid accidents. That leaves Tesla and other autonomy companies with a scary question: Will self-driving cars keep getting better, like image search, voice recognition, and the other AI success stories? But nearly every car accident involves some sort of unforeseen circumstance, and without the power to generalize, self-driving cars will have to confront each of these scenarios as if for the first time. “I see all these micro-improvements as extraordinary features on the journey towards full autonomy.”Still, it’s not clear how long self-driving cars can stay in their current limbo. One study by the Rand Corporation estimated that self-driving cars would have to drive 275 million miles without a fatality to prove they were as safe as human drivers.","Part of / The Real-World AI Issue  If you believe the CEOs, a fully autonomous car could be only months away. In 2015, Elon Musk predicted a fully autonomous Tesla by 2018; so did Google. Delphi and MobileEye’s Level 4 system is currently slated for 2019, the same year Nutonomy plans to deploy thousands of driverless taxis on the streets of Singapore. GM will put a fully autonomous car into production in 2019, with no steering wheel or ability for drivers to intervene. There’s real money behind these predictions, bets made on the assumption that the software will be able to catch up to the hype.  On its face, full autonomy seems closer than ever. Waymo is already testing cars on limited-but-public roads in Arizona. Tesla and a host of other imitators already sell a limited form of Autopilot, counting on drivers to intervene if anything unexpected happens. There have been a few crashes, some deadly, but as long as the systems keep improving, the logic goes, we can’t be that far from not having to intervene at all.  But the dream of a fully autonomous car may be further than we realize. There’s growing concern among AI experts that it may be years, if not decades, before self-driving systems can reliably avoid accidents. As self-trained systems grapple with the chaos of the real world, experts like NYU’s Gary Marcus are bracing for a painful recalibration in expectations, a correction sometimes called “AI winter.” That delay could have disastrous consequences for companies banking on self-driving technology, putting full autonomy out of reach for an entire generation.  “Driverless cars are like a scientific experiment where we don’t know the answer”  It’s easy to see why car companies are optimistic about autonomy. Over the past ten years, deep learning — a method that uses layered machine-learning algorithms to extract structured information from massive data sets — has driven almost unthinkable progress in AI and the tech industry. It powers Google Search, the Facebook News Feed, conversational speech-to-text algorithms, and champion Go-playing systems. Outside the internet, we use deep learning to detect earthquakes, predict heart disease, and flag suspicious behavior on a camera feed, along with countless other innovations that would have been impossible otherwise.  But deep learning requires massive amounts of training data to work properly, incorporating nearly every scenario the algorithm will encounter. Systems like Google Images, for instance, are great at recognizing animals as long as they have training data to show them what each animal looks like. Marcus describes this kind of task as “interpolation,” taking a survey of all the images labeled “ocelot” and deciding whether the new picture belongs in the group.  Engineers can get creative in where the data comes from and how it’s structured, but it places a hard limit on how far a given algorithm can reach. The same algorithm can’t recognize an ocelot unless it’s seen thousands of pictures of an ocelot — even if it’s seen pictures of housecats and jaguars, and knows ocelots are somewhere in between. That process, called “generalization,” requires a different set of skills.  For a long time, researchers thought they could improve generalization skills with the right algorithms, but recent research has shown that conventional deep learning is even worse at generalizing than we thought. One study found that conventional deep learning systems have a hard time even generalizing across different frames of a video, labeling the same polar bear as a baboon, mongoose, or weasel depending on minor shifts in the background. With each classification based on hundreds of factors in aggregate, even small changes to pictures can completely change the system’s judgment, something other researchers have taken advantage of in adversarial data sets.  Marcus points to the chat bot craze as the most recent example of hype running up against the generalization problem. “We were promised chat bots in 2015,” he says, “but they’re not any good because it’s not just a matter of collecting data.” When you’re talking to a person online, you don’t just want them to rehash earlier conversations. You want them to respond to what you’re saying, drawing on broader conversational skills to produce a response that’s unique to you. Deep learning just couldn’t make that kind of chat bot. Once the initial hype faded, companies lost faith in their chat bot projects, and there are very few still in active development.  That leaves Tesla and other autonomy companies with a scary question: Will self-driving cars keep getting better, like image search, voice recognition, and the other AI success stories? Or will they run into the generalization problem like chat bots? Is autonomy an interpolation problem or a generalization problem? How unpredictable is driving, really?  It may be too early to know. “Driverless cars are like a scientific experiment where we don’t know the answer,” Marcus says. We’ve never been able to automate driving at this level before, so we don’t know what kind of task it is. To the extent that it’s about identifying familiar objects and following rules, existing technologies should be up to the task. But Marcus worries that driving well in accident-prone scenarios may be more complicated than the industry wants to admit. “To the extent that surprising new things happen, it’s not a good thing for deep learning.”  “Safety isn’t just about the quality of the AI technology”  The experimental data we have comes from public accident reports, each of which offers some unusual wrinkle. A fatal 2016 crash saw a Model S drive full speed into the rear portion of a white tractor trailer, confused by the high ride height of the trailer and bright reflection of the sun. In March, a self-driving Uber crash killed a woman pushing a bicycle, after she emerged from an unauthorized crosswalk. According to the NTSB report, Uber’s software misidentified the woman as an unknown object, then a vehicle, then finally as a bicycle, updating its projections each time. In a California crash, a Model X steered toward a barrier and sped up in the moments before impact, for reasons that remain unclear.  Each accident seems like an edge case, the kind of thing engineers couldn’t be expected to predict in advance. But nearly every car accident involves some sort of unforeseen circumstance, and without the power to generalize, self-driving cars will have to confront each of these scenarios as if for the first time. The result would be a string of fluke-y accidents that don’t get less common or less dangerous as time goes on. For skeptics, a turn through the manual disengagement reports shows that scenario already well under way, with progress already reaching a plateau.  Andrew Ng — a former Baidu executive, Drive.AI board member, and one of the industry’s most prominent boosters — argues the problem is less about building a perfect driving system than training bystanders to anticipate self-driving behavior. In other words, we can make roads safe for the cars instead of the other way around. As an example of an unpredictable case, I asked him whether he thought modern systems could handle a pedestrian on a pogo stick, even if they had never seen one before. “I think many AV teams could handle a pogo stick user in pedestrian crosswalk,” Ng told me. “Having said that, bouncing on a pogo stick in the middle of a highway would be really dangerous.”  “Rather than building AI to solve the pogo stick problem, we should partner with the government to ask people to be lawful and considerate,” he said. “Safety isn’t just about the quality of the AI technology.”  “This is not an easily isolated problem”  Deep learning isn’t the only AI technique, and companies are already exploring alternatives. Though techniques are closely guarded within the industry (just look at Waymo’s recent lawsuit against Uber), many companies have shifted to rule-based AI, an older technique that lets engineers hard-code specific behaviors or logic into an otherwise self-directed system. It doesn’t have the same capacity to write its own behaviors just by studying data, which is what makes deep learning so exciting, but it would let companies avoid some of the deep learning’s limitations. But with the basic tasks of perception still profoundly shaped by deep learning techniques, it’s hard to say how successfully engineers can quarantine potential errors.  Ann Miura-Ko, a venture capitalist who sits on the board of Lyft, says she thinks part of the problem is high expectations for autonomous cars themselves, classifying anything less than full autonomy as a failure. “To expect them to go from zero to level five is a mismatch in expectations more than a failure of technology,” Miura-Ko says. “I see all these micro-improvements as extraordinary features on the journey towards full autonomy.”  Still, it’s not clear how long self-driving cars can stay in their current limbo. Semi-autonomous products like Tesla’s Autopilot are smart enough to handle most situations, but require human intervention if anything too unpredictable happens. When something does go wrong, it’s hard to know whether the car or the driver is to blame. For some critics, that hybrid is arguably less safe than a human driver, even if the errors are hard to blame entirely on the machine. One study by the Rand Corporation estimated that self-driving cars would have to drive 275 million miles without a fatality to prove they were as safe as human drivers. The first death linked to Tesla’s Autopilot came roughly 130 million miles into the project, well short of the mark.  But with deep learning sitting at the heart of how cars perceive objects and decide to respond, improving the accident rate may be harder than it looks. “This is not an easily isolated problem,” says Duke professor Mary Cummings, pointing to an Uber crash that killed a pedestrian earlier this year. “The perception-decision cycle is often linked, as in the case of the pedestrian death. A decision was made to do nothing based on ambiguity in perception, and the emergency braking was turned off because it got too many false alarms from the sensor”  That crash ended with Uber pausing its self-driving efforts for the summer, an ominous sign for other companies planning rollouts. Across the industry, companies are racing for more data to solve the problem, assuming the company with the most miles will build the strongest system. But where companies see a data problem, Marcus sees something much harder to solve. “They’re just using the techniques that they have in the hopes that it will work,” Marcus says. “They’re leaning on the big data because that’s the crutch that they have, but there’s no proof that ever gets you to the level of precision that we need.”","headed, companies, marcus, problem, cars, roadblock, systems, data, selfdriving, learning, ai, deep",2018-07-03 00:00:00
3,AI winter,,https://en.wikipedia.org/wiki/AI_winter,"Period of reduced funding and interest in AI researchIn the history of artificial intelligence, an AI winter is a period of reduced funding and interest in artificial intelligence research. Roger Schank and Marvin Minsky—two leading AI researchers who experienced the ""winter"" of the 1970s—warned the business community that enthusiasm for AI had spiraled out of control in the 1980s and that disappointment would certainly follow. [6]Just like all AI booms that have been followed by desperate AI winters, the media tended to exaggerate the significance of these developments. ""[25] The result, Moravec claims, is that some of the staff at DARPA had lost patience with AI research. In 2006, Nick Bostrom explained that ""a lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.","Period of reduced funding and interest in AI research  In the history of artificial intelligence, an AI winter is a period of reduced funding and interest in artificial intelligence research.[1] The field has experienced several hype cycles, followed by disappointment and criticism, followed by funding cuts, followed by renewed interest years or even decades later.  The term first appeared in 1984 as the topic of a public debate at the annual meeting of AAAI (then called the ""American Association of Artificial Intelligence""). Roger Schank and Marvin Minsky—two leading AI researchers who experienced the ""winter"" of the 1970s—warned the business community that enthusiasm for AI had spiraled out of control in the 1980s and that disappointment would certainly follow. They described a chain reaction, similar to a ""nuclear winter"", that would begin with pessimism in the AI community, followed by pessimism in the press, followed by a severe cutback in funding, followed by the end of serious research. Three years later the billion-dollar AI industry began to collapse.  There were two major winters approximately 1974–1980 and 1987–2000,[3] and several smaller episodes, including the following:  Enthusiasm and optimism about AI has generally increased since its low point in the early 1990s. Beginning about 2012, interest in artificial intelligence (and especially the sub-field of machine learning) from the research and corporate communities led to a dramatic increase in funding and investment, leading to the current (as of 2024 ) AI boom.  Early episodes [ edit ]  Machine translation and the ALPAC report of 1966 [ edit ]  Natural language processing (NLP) research has its roots in the early 1930s and began its existence with the work on machine translation (MT).[4] However, significant advancements and applications began to emerge after the publication of Warren Weaver's influential memorandum in 1949.[5] The memorandum generated great excitement within the research community. In the following years, notable events unfolded: IBM embarked on the development of the first machine, MIT appointed its first full-time professor in machine translation, and several conferences dedicated to MT took place. The culmination came with the public demonstration of the IBM-Georgetown machine, which garnered widespread attention in respected newspapers in 1954.[6]  Just like all AI booms that have been followed by desperate AI winters, the media tended to exaggerate the significance of these developments. Headlines about the IBM-Georgetown experiment proclaimed phrases like ""The bilingual machine,"" ""Robot brain translates Russian into King's English,""[7] and ""Polyglot brainchild.""[8] However, the actual demonstration involved the translation of a curated set of only 49 Russian sentences into English, with the machine's vocabulary limited to just 250 words.[6] To put things into perspective, a 2006 study made by Paul Nation found that humans need a vocabulary of around 8,000 to 9,000-word families to comprehend written texts with 98% accuracy.[9]  During the Cold War, the US government was particularly interested in the automatic, instant translation of Russian documents and scientific reports. The government aggressively supported efforts at machine translation starting in 1954. Another factor that propelled the field of mechanical translation was the interest shown by the Central Intelligence Agency (CIA). During that period, the CIA firmly believed in the importance of developing machine translation capabilities and supported such initiatives. They also recognized that this program had implications that extended beyond the interests of the CIA and the intelligence community.[6]  At the outset, the researchers were optimistic. Noam Chomsky's new work in grammar was streamlining the translation process and there were ""many predictions of imminent 'breakthroughs'"".[10]  Briefing for US Vice President Gerald Ford in 1973 on the junction-grammar-based computer translation model  However, researchers had underestimated the profound difficulty of word-sense disambiguation. In order to translate a sentence, a machine needed to have some idea what the sentence was about, otherwise it made mistakes. An apocryphal[11] example is ""the spirit is willing but the flesh is weak."" Translated back and forth with Russian, it became ""the vodka is good but the meat is rotten."" Later researchers would call this the commonsense knowledge problem.  By 1964, the National Research Council had become concerned about the lack of progress and formed the Automatic Language Processing Advisory Committee (ALPAC) to look into the problem. They concluded, in a famous 1966 report, that machine translation was more expensive, less accurate and slower than human translation. After spending some 20 million dollars, the NRC ended all support. Careers were destroyed and research ended.[10]  Machine translation shared the same path with NLP from the rule-based approaches through the statistical approaches up to the neural network approaches, which have in 2023 culminated in large language models.  The failure of single-layer neural networks in 1969 [ edit ]  Simple networks or circuits of connected units, including Walter Pitts and Warren McCulloch's neural network for logic and Marvin Minsky's SNARC system, have failed to deliver the promised results and were abandoned in the late 1950s. Following the success of programs such as the Logic Theorist and the General Problem Solver,[13] algorithms for manipulating symbols seemed more promising at the time as means to achieve logical reasoning viewed at the time as the essence of intelligence, either natural or artificial.  Interest in perceptrons, invented by Frank Rosenblatt, was kept alive only by the sheer force of his personality.[14] He optimistically predicted that the perceptron ""may eventually be able to learn, make decisions, and translate languages"".[15] Mainstream research into perceptrons ended partially because the 1969 book Perceptrons by Marvin Minsky and Seymour Papert emphasized the limits of what perceptrons could do. While it was already known that multilayered perceptrons are not subject to the criticism, nobody in the 1960s knew how to train a multilayered perceptron. Backpropagation was still years away.[17]  Major funding for projects neural network approaches was difficult to find in the 1970s and early 1980s.[18] Important theoretical work continued despite the lack of funding. The ""winter"" of neural network approach came to an end in the middle 1980s, when the work of John Hopfield, David Rumelhart and others revived large scale interest.[19] Rosenblatt did not live to see this, however, as he died in a boating accident shortly after Perceptrons was published.[15]  The setbacks of 1974 [ edit ]  The Lighthill report [ edit ]  In 1973, professor Sir James Lighthill was asked by the UK Parliament to evaluate the state of AI research in the United Kingdom. His report, now called the Lighthill report, criticized the utter failure of AI to achieve its ""grandiose objectives"". He concluded that nothing being done in AI could not be done in other sciences. He specifically mentioned the problem of ""combinatorial explosion"" or ""intractability"", which implied that many of AI's most successful algorithms would grind to a halt on real world problems and were only suitable for solving ""toy"" versions.[20]  The report was contested in a debate broadcast in the BBC ""Controversy"" series in 1973. The debate ""The general purpose robot is a mirage"" from the Royal Institution was Lighthill versus the team of Donald Michie, John McCarthy and Richard Gregory.[21] McCarthy later wrote that ""the combinatorial explosion problem has been recognized in AI from the beginning"".[22]  The report led to the complete dismantling of AI research in the UK.[20] AI research continued in only a few universities (Edinburgh, Essex and Sussex). Research would not revive on a large scale until 1983, when Alvey (a research project of the British Government) began to fund AI again from a war chest of £350 million in response to the Japanese Fifth Generation Project (see below). Alvey had a number of UK-only requirements which did not sit well internationally, especially with US partners, and lost Phase 2 funding.  DARPA's early 1970s funding cuts [ edit ]  During the 1960s, the Defense Advanced Research Projects Agency (then known as ""ARPA"", now known as ""DARPA"") provided millions of dollars for AI research with few strings attached. J. C. R. Licklider, the founding director of DARPA's computing division, believed in ""funding people, not projects""[23] and he and several successors allowed AI's leaders (such as Marvin Minsky, John McCarthy, Herbert A. Simon or Allen Newell) to spend it almost any way they liked.  This attitude changed after the passage of Mansfield Amendment in 1969, which required DARPA to fund ""mission-oriented direct research, rather than basic undirected research"".[24] Pure undirected research of the kind that had gone on in the 1960s would no longer be funded by DARPA. Researchers now had to show that their work would soon produce some useful military technology. AI research proposals were held to a very high standard. The situation was not helped when the Lighthill report and DARPA's own study (the American Study Group) suggested that most AI research was unlikely to produce anything truly useful in the foreseeable future. DARPA's money was directed at specific projects with identifiable goals, such as autonomous tanks and battle management systems. By 1974, funding for AI projects was hard to find.[24]  AI researcher Hans Moravec blamed the crisis on the unrealistic predictions of his colleagues: ""Many researchers were caught up in a web of increasing exaggeration. Their initial promises to DARPA had been much too optimistic. Of course, what they delivered stopped considerably short of that. But they felt they couldn't in their next proposal promise less than in the first one, so they promised more.""[25] The result, Moravec claims, is that some of the staff at DARPA had lost patience with AI research. ""It was literally phrased at DARPA that 'some of these people were going to be taught a lesson [by] having their two-million-dollar-a-year contracts cut to almost nothing!'"" Moravec told Daniel Crevier.[26]  While the autonomous tank project was a failure, the battle management system (the Dynamic Analysis and Replanning Tool) proved to be enormously successful, saving billions in the first Gulf War, repaying all of DARPAs investment in AI[27] and justifying DARPA's pragmatic policy.[28]  The SUR debacle [ edit ]  As described in:[29]  In 1971, the Defense Advanced Research Projects Agency (DARPA) began an ambitious five-year experiment in speech understanding. The goals of the project were to provide recognition of utterances from a limited vocabulary in near-real time. Three organizations finally demonstrated systems at the conclusion of the project in 1976. These were Carnegie-Mellon University (CMU), who actually demonstrated two system [HEARSAY-II and HARPY]; Bolt, Beranek and Newman (BBN); and System Development Corporation with Stanford Research Institute (SDC/SRI)  The system that came closest to satisfying the original project goals was the CMU HARPY system. The relatively high performance of the HARPY system was largely achieved through 'hard-wiring' information about possible utterances into the system's knowledge base. Although HARPY made some interesting contributions, its dependence on extensive pre-knowledge limited the applicability of the approach to other signal-understanding tasks.  DARPA was deeply disappointed with researchers working on the Speech Understanding Research program at Carnegie Mellon University. DARPA had hoped for, and felt it had been promised, a system that could respond to voice commands from a pilot. The SUR team had developed a system which could recognize spoken English, but only if the words were spoken in a particular order. DARPA felt it had been duped and, in 1974, they cancelled a three million dollar a year contract.[30]  Many years later, several successful commercial speech recognition systems would use the technology developed by the Carnegie Mellon team (such as hidden Markov models) and the market for speech recognition systems would reach $4 billion by 2001.[31]  For a description of Hearsay-II see Hearsay-II, The Hearsay-II Speech Understanding System: Integrating Knowledge to Resolve Uncertainty and A Retrospective View of the Hearsay-II Architecture which appear in Blackboard Systems[32]  Reddy gives a review of progress in speech understanding at the end of the DARPA project in a 1976 article in Proceedings of the IEEE.[33]  Contrary view [ edit ]  Thomas Haigh argues that activity in the domain of AI did not slow down, even as funding from DoD was being redirected, mostly in the wake of congressional legislation meant to separate military and academic activities.[34] That indeed professional interest was growing throughout the 70s. Using the membership count of ACM's SIGART, the Special Interest Group on Artificial Intelligence, as a proxy for interest in the subject, the author writes:[34]  (...) I located two data sources, neither of which supports the idea of a broadly based AI winter during the 1970s. One is membership of ACM's SIGART, the major venue for sharing news and research abstracts during the 1970s. When the Lighthill report was published in 1973 the fast-growing group had 1,241 members, approximately twice the level in 1969. The next five years are conventionally thought of as the darkest part of the first AI winter. Was the AI community shrinking? No! By mid-1978 SIGART membership had almost tripled, to 3,500. Not only was the group growing faster than ever, it was increasing proportionally faster than ACM as a whole which had begun to plateau (expanding by less than 50% over the entire period from 1969 to 1978). One in every 11 ACM members was in SIGART.  The setbacks of the late 1980s and early 1990s [ edit ]  The collapse of the LISP machine market [ edit ]  In the 1980s, a form of AI program called an ""expert system"" was adopted by corporations around the world. The first commercial expert system was XCON, developed at Carnegie Mellon for Digital Equipment Corporation, and it was an enormous success: it was estimated to have saved the company 40 million dollars over just six years of operation. Corporations around the world began to develop and deploy expert systems and by 1985 they were spending over a billion dollars on AI, most of it to in-house AI departments. An industry grew up to support them, including software companies like Teknowledge and Intellicorp (KEE), and hardware companies like Symbolics and LISP Machines Inc. who built specialized computers, called LISP machines, that were optimized to process the programming language LISP, the preferred language for AI research in the USA.[35][36]  In 1987, three years after Minsky and Schank's prediction, the market for specialized LISP-based AI hardware collapsed. Workstations by companies like Sun Microsystems offered a powerful alternative to LISP machines and companies like Lucid offered a LISP environment for this new class of workstations. The performance of these general workstations became an increasingly difficult challenge for LISP Machines. Companies like Lucid and Franz LISP offered increasingly powerful versions of LISP that were portable to all UNIX systems. For example, benchmarks were published showing workstations maintaining a performance advantage over LISP machines.[37] Later desktop computers built by Apple and IBM would also offer a simpler and more popular architecture to run LISP applications on. By 1987, some of them had become as powerful as the more expensive LISP machines. The desktop computers had rule-based engines such as CLIPS available.[38] These alternatives left consumers with no reason to buy an expensive machine specialized for running LISP. An entire industry worth half a billion dollars was replaced in a single year.[39]  By the early 1990s, most commercial LISP companies had failed, including Symbolics, LISP Machines Inc., Lucid Inc., etc. Other companies, like Texas Instruments and Xerox, abandoned the field. A small number of customer companies (that is, companies using systems written in LISP and developed on LISP machine platforms) continued to maintain systems. In some cases, this maintenance involved the assumption of the resulting support work.[40]  Slowdown in deployment of expert systems [ edit ]  By the early 1990s, the earliest successful expert systems, such as XCON, proved too expensive to maintain. They were difficult to update, they could not learn, they were ""brittle"" (i.e., they could make grotesque mistakes when given unusual inputs), and they fell prey to problems (such as the qualification problem) that had been identified years earlier in research in nonmonotonic logic. Expert systems proved useful, but only in a few special contexts.[41][42] Another problem dealt with the computational hardness of truth maintenance efforts for general knowledge. KEE used an assumption-based approach supporting multiple-world scenarios that was difficult to understand and apply.  The few remaining expert system shell companies were eventually forced to downsize and search for new markets and software paradigms, like case-based reasoning or universal database access. The maturation of Common Lisp saved many systems such as ICAD which found application in knowledge-based engineering. Other systems, such as Intellicorp's KEE, moved from LISP to a C++ (variant) on the PC and helped establish object-oriented technology (including providing major support for the development of UML (see UML Partners).  The end of the Fifth Generation project [ edit ]  In 1981, the Japanese Ministry of International Trade and Industry set aside $850 million for the Fifth Generation computer project. Their objectives were to write programs and build machines that could carry on conversations, translate languages, interpret pictures, and reason like human beings. By 1991, the impressive list of goals penned in 1981 had not been met. According to HP Newquist in The Brain Makers, ""On June 1, 1992, The Fifth Generation Project ended not with a successful roar, but with a whimper.""[40] As with other AI projects, expectations had run much higher than what was actually possible.[43][44]  Strategic Computing Initiative cutbacks [ edit ]  In 1983, in response to the fifth generation project, DARPA again began to fund AI research through the Strategic Computing Initiative. As originally proposed the project would begin with practical, achievable goals, which even included artificial general intelligence as long-term objective. The program was under the direction of the Information Processing Technology Office (IPTO) and was also directed at supercomputing and microelectronics. By 1985 it had spent $100 million and 92 projects were underway at 60 institutions, half in industry, half in universities and government labs. AI research was well-funded by the SCI.[45]  Jack Schwarz, who ascended to the leadership of IPTO in 1987, dismissed expert systems as ""clever programming"" and cut funding to AI ""deeply and brutally"", ""eviscerating"" SCI. Schwarz felt that DARPA should focus its funding only on those technologies which showed the most promise, in his words, DARPA should ""surf"", rather than ""dog paddle"", and he felt strongly AI was not ""the next wave"". Insiders in the program cited problems in communication, organization and integration. A few projects survived the funding cuts, including pilot's assistant and an autonomous land vehicle (which were never delivered) and the DART battle management system, which (as noted above) was successful.[46]  AI winter of the 1990's and early 2000's [ edit ]  A survey of reports from the early 2000's suggests that AI's reputation was still poor:  Alex Castro, quoted in The Economist , 7 June 2007: ""[Investors] were put off by the term 'voice recognition' which, like 'artificial intelligence', is associated with systems that have all too often failed to live up to their promises."" [ 47 ]  , 7 June 2007: ""[Investors] were put off by the term 'voice recognition' which, like 'artificial intelligence', is associated with systems that have all too often failed to live up to their promises."" Patty Tascarella in Pittsburgh Business Times , 2006: ""Some believe the word 'robotics' actually carries a stigma that hurts a company's chances at funding."" [ 48 ]  , 2006: ""Some believe the word 'robotics' actually carries a stigma that hurts a company's chances at funding."" John Markoff in the New York Times, 2005: ""At its low point, some computer scientists and software engineers avoided the term artificial intelligence for fear of being viewed as wild-eyed dreamers."" [ 49 ]  Many researchers in AI in the mid 2000's deliberately called their work by other names, such as informatics, machine learning, analytics, knowledge-based systems, business rules management, cognitive systems, intelligent systems, intelligent agents or computational intelligence, to indicate that their work emphasizes particular tools or is directed at a particular sub-problem. Although this may be partly because they consider their field to be fundamentally different from AI, it is also true that the new names help to procure funding by avoiding the stigma of false promises attached to the name ""artificial intelligence"".[49][50]  In the late 1990's and early 21st century, AI technology became widely used as elements of larger systems,[51] but the field is rarely credited for these successes. In 2006, Nick Bostrom explained that ""a lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.""[53] Rodney Brooks stated around the same time that ""there's this stupid myth out there that AI has failed, but AI is around you every second of the day.""  Current AI spring (2022–present) [ edit ]  AI has reached the highest levels of interest and funding in its history in the past few years by every possible measure, including: publications, patent applications,[56] total investment ($50 billion in 2022), and job openings (800,000 U.S. job openings in 2022). The successes of the current ""AI spring"" or ""AI boom"" are advances in language translation (in particular, Google Translate), image recognition (spurred by the ImageNet training database) as commercialized by Google Image Search, and in game-playing systems such as AlphaZero (chess champion) and AlphaGo (go champion), and Watson (Jeopardy champion). A turning point was in 2012 when AlexNet (a deep learning network) won the ImageNet Large Scale Visual Recognition Challenge with half as many errors as the second place winner.  The 2022 release of OpenAI's AI chatbot ChatGPT which as of January 2023 has over 100 million users,[60] has reinvigorated the discussion about artificial intelligence and its effects on the world.[61][62]  See also [ edit ]  Notes [ edit ]  References [ edit ]","research, translation, lisp, winter, funding, system, systems, intelligence, machine, ai, edit",
4,AI winter - update,,https://blog.piekniewski.info/2018/10/29/ai-winter-update/,"IntroductionAlmost six months ago (May 28th 2018) I posted the ""AI winter is well on its way"" post that went viral. First of all a bit of clarification: some readers have misinterpreted my claims, in that I predicted that the AI hype is declining. Andrew Ng is a rare example of a person who jumped from an academic bubble into an even bigger AI hype bubble. I'm pretty certain that following Hotz's lead, many of today's AI hype blowers will be screaming how they've been warning about AI winter all along, once the bubble bursts. Musk reiterated that he believes in the self driving Tesla fleet however full self driving remains ""off menu"" as it was too confusing (two years after introduction of the item?!","Introduction  Almost six months ago (May 28th 2018) I posted the ""AI winter is well on its way"" post that went viral. The post amassed nearly a quarter million views and got picked up in Bloomberg, Forbes, Politico, Venturebeat, BBC, Datascience Podcast and numerous other smaller media outlets and blogs [1, 2, 3, 4, ...], triggered violent debate on Hacker news and Reddit. I could not have anticipated this post to be so successful and hence I realized I touched on a very sensitive subject. One can agree with my claims or not, but the sheer popularity of the post almost itself serves as a proof that something is going on behind the scenes and people are actually curious and doubtful if there is anything solid behind the AI hype.  Since the post made a prediction, that the AI hype is cracking (particularly in the space of autonomous vehicles) and as a result we will have another ""AI winter"" episode, I decided to periodically go over those claims, see what has changed and bring some new evidence.  First of all a bit of clarification: some readers have misinterpreted my claims, in that I predicted that the AI hype is declining. In fact to the contrary, I started my original post by stating that on the surface still everything looks great. And that is still the case. The NIPS conference sold out in a matter of minutes, surprising even the biggest enthusiasts. In fact some known researchers have apparently missed out and will not be going (probably the most interesting part of this conference this year will be the whole drama with its name anyway). This does not contradict anything, hype is a lagging indicator of what is actually going on. I discussed that, as well as other immediate feedback in ""AI winter addendum"". Let's review what happened over that last several months.  Updates  The twitter activity of many of the prominent researchers I follow was rather sparse over the period in question. Dr Fei Fei Li gave testimony in front of congress, with a highlight in the tweet below (and a hilarious reply):  I'm not sure how long people can go for spreading such utter nonsense without any harm to their reputation. I suppose for quite a while, but hopefully not forever.  Andrew Ng, one of my favorite ""enthusiasts"" is busy building his swarm of startup companies, but luckily found the time to give a short interview to Fortune, in which he gloats about how he singlehandedly transformed Google and Baidu into AI companies. Andrew Ng is a rare example of a person who jumped from an academic bubble into an even bigger AI hype bubble. He combines certain amount of contagious enthusiasm and a fair amount of arrogance which apparently appeals to many young people today. His recent gig is about industrial inspection. In one video he presented an example in which a deep learning system recognizes a faulty solder pad. I actually worked for a company specializing in visual inspection of electronics, so I know rather well what is the state of the art. The machines routinely scan large PCB's and within fractions of a second create a full 3d reconstruction of the inspected part, analyze each solder joint for imperfections and a set of well defined defects, create interpretable scores which could be used for binning and so on. In face of that, the little video from Andrew seemed rather comical and if anything indicated Dunning-Kruger effect. Now I'm not saying that deep learning could not improve this industry in some way, but the entry level is set pretty high. And same is true for many other industries that rely on very well optimized and often elegant algorithmic solutions.  Now that we are on the subject of colorful characters in the AI scene, another one of my favorites, George Hotz from Comma.ai announced he is resigning from the role of CEO at comma. Hotz, known among other things for hacking playstation, made some ripples in the industry in 2015 when he teased Elon Musk, that he could build a self-driving-car system for Tesla, and he could do it much better and cheaper than Mobileye, which was the company that Tesla used at that time. Not too long after that, a first fatal autopilot accident happened (Joshua Brown) and Mobileye dropped the relationship with Tesla, apparently noting that their use of technology was irresponsible. Anyway, back then Hotz was confident he could build a self driving car in his garage with some old phone parts and deep learning magic. Since then his narrative has changed by some 180 degrees, and now George is on ""crusade against the 'scam' of self-driving cars""... I guess I could leave this one without a comment, though Picard's face palm meme would be in order here. I'm pretty certain that following Hotz's lead, many of today's AI hype blowers will be screaming how they've been warning about AI winter all along, once the bubble bursts.  Speaking of Tesla, as of yet the long promised autonomous coast-to-coast drive has not happened. Earlier this year Elon Musk - CEO of Tesla, stated that the drive would certainly happen by the summer 2018, but on August 1 2018 conference call he indicated that the coast-to-coast drive is actually not a priority anymore and will get delayed (no new timeframe was given). This is quite a change, particularly in the light of the conference call on which the so called full self driving feature was announced almost exactly two years ago. By August, first ""autonomous features"" were supposed to be unrolled in Tesla's via over the air update, but no such thing happened (noteworthy, in the beginning of 2018 Musk was rather confident that full self driving will noticeably depart from advanced autopilot by summer). Instead the new autopilot v9 can sometimes switch lanes but by many commentators, the new system actually requires even more attention than the previous one and still is buggy [1] and dangerous. In fact it is not uncommon to hear opinions that even the current Tesla autopilot with respect to stability and reliability barely approaches the Mobileye solution from several years ago. On October 18'th the full-self-driving (FSD) option disappeared from all Tesla models configurations, without much explanation other than that it was ""confusing"". At the time of writing this article it is not clear if the option will be coming back or if those who already payed for it will get a refund. It is still not clear to me if Elon Musk fell a victim of the hype himself or did he deliberately blew the AI bubble, but he certainly contributed substantially to the AI mania between 2015 and now, especially with that egregious stunt with Stephen Hawking.  I should add here perhaps, that I actually think Tesla autopilot is a pretty impressive piece of engineering, just not anywhere near being safely deployable beyond as fancy cruise control. The August Tesla quarterly conference call was also interesting in that a lot of time on the call was spent on the new hardware that Tesla was building that is supposed to offer an order of magnitude improvement over the current nVidia drive PX 2 system, and that this new system will finally allow for full autonomy. I remain skeptical, since having an order of magnitude more compute capabilities than drive PX is nothing extraordinary, a rig of two GTX 1080Ti's will accomplish it without any problem, and this is what many companies are putting on their test vehicles along with expensive lidars etc, and yet the full autonomy remains elusive. Secondly, it seems that cars which Tesla had sold as ""full self-driving compatible"" will require a computer swap, which will not be cheap. One can get at least some feel of what stage is Tesla at with respect to autonomy by taking a look at the slides (from May 10, 2018) of Andrej Karpathy, director of AI there. Half way through the presentation he realizes that the driving reality is full of corner cases, which are not only hard for neural nets, but are even not obvious to label by a human. The other half of his presentation is spent on what he calls software 2.0. It is a concept in which computers are no longer programmed but are trained. This reminds me a lot of the hype in the 80's, the fifth generation computers which were supposed to program themselves based on high level logical specification written in Prolog or similar functional language. This hype cycle was closely related to the expert system mania, which collapsed by the end of 80's causing an AI winter episode. I'm pretty certain this software 2.0 nonsense will share the same fate.  To conclude the Tesla case, the most recent Q3 conference call for Tesla which took place on October 24th did not bring much resolution to the above uncertainties. Musk reiterated that he believes in the self driving Tesla fleet however full self driving remains ""off menu"" as it was too confusing (two years after introduction of the item?!), Karpathy quickly mentioned that new hardware will support bigger neural networks which work ""very good"" and the new version of Autopilot will allow to navigate on the freeway, with the restriction that lane changes will require confirmation from the driver (read, any incidents will be blamed on the driver). No word about the coast to coast drive. No timeline on full self driving.  While on the self-driving car subject, one of the main criticisms of my original AI winter post was that I omitted Waymo from my discussion, them being the unquestionable leader in autonomy. This criticism was a bit unjustified in that I did include and discussed Waymo extensively in my other posts [1], but in these circumstances it appears prudent to mention what is going on there. Luckily a recent very good piece of investigative journalism shines some light on the matter. Apparently Waymo cars tested in Phoenix area had trouble with the most basic driving situations such as merging onto a freeway or making a left turn, [1]. The piece worth citing from the article:  There are times when it seems “autonomy is around the corner,” and the vehicle can go for a day without a human driver intervening, said a person familiar with Waymo. Other days reality sets in because “the edge cases are endless.”  Some independent observations appear to confirm this assessment. As much as I agree that Waymo is probably the most advanced in this game, this does not mean they are anywhere near to actually deploying anything seriously, and even further away from making such deployment economically feasible (contrary to what is suggested in occasional puff pieces such as this one). Aside from a periodic PR nonsense, Waymo does not seem to be revealing much, though recently some baffling reports of past shenanigans in google chauffeur (which later became Waymo) surfaced, involving Anthony Levandowski who is responsible for the whole Uber-Waymo fiasco. To add some comical aspect to the Waymo-Uber story, apparently an unrelated engineer managed to invalidate the patent that Uber got sued over, spending altogether 6000 dollars in fees. This is probably how much Uber payed their patent attorneys for a minute of their work...  Speaking of Uber they substantially slowed their self-driving program, practically killed their self driving truck program (same one that delivered a few crates of beer in Colorado in 2016 with great fanfares, a demo that later on turned out to be completely staged), and recent rumors indicate they might be even looking to sell the unit.  Generally the other self driving car projects are facing increasing headwinds, with some projects already getting shut down by the government agencies, and others going more low-key with respect to public announcements. Particularly interesting news came recently out of Cruise, the second in the race right after Waymo (at least according to California disengagement data). Some noteworthy bits from the Reuters article:  Those expectations are now hitting speed bumps, according to interviews with eight current and former GM and Cruise employees and executives, along with nine autonomous vehicle technology experts familiar with Cruise. These sources say that some unexpected technical challenges - including the difficulty that Cruise cars have identifying whether objects are in motion - mean putting GM’s driverless cars on the road in a large scale way in 2019 is looking highly unlikely. “Nothing is on schedule,” said one GM source, referring to certain mileage targets and other milestones the company has already missed.  And a few paragraphs further:  “Everyone in the industry is becoming more and more nervous that they will waste billions of dollars,” said Klaus Froehlich, a board member at BMW and its head of research and development.  The future of self driving cars is getting more and more uncertain, in perfect agreement with my original thesis expressed in 2016 [see also here].  Briefly on other players on the AI scene: DeepMind was rather quiet (last time I checked Montezuma's revenge remained unsolved for AI in the general see update below), but OpenAI had a small PR offensive with their DotA 2 playing agent. After the initial tournament in which the system won, it quickly became apparent that the game was in many ways restricted in favor of the computer. Hence another tournament was organized, in which most restrictions were lifted, and the tournament was ... spectacularly lost to humans... Bummer after OpenAI spent obscene amounts of money training their agents. Now I could not care less about results in game domains, since as I stated multiple times on this blog [1, 2], the only problem really worth solving in AI is the Moravec's paradox, which is exactly the opposite of what DeepMind or OpenAI are doing, but I nevertheless found this media misfire hilarious.  While touching on Moravec's paradox, one of the handful of companies that actually tried to move robotics forward, Rethink Robotics, shut down its operation. This shows that making robots do anything beyond what they already do very well in controlled factory production lines is not only difficult technically but also poses a challenging business case, even with the experience of Rodney Brooks. Unlike most other startups in this field, whose main asset is the ego of their founder fueled by some cheap VC money, Rethink actually accomplished many impressive technical achievements and the news saddened me quite a bit. Robotics will need to be rethought again, likely many times over.  Finally, as an additional indication of the changing sentiment, one can cite this tweet from François Chollet, author of Keras Deep learning framework, certainly a person in the know:  Today more people are working on deep learning than ever before -- around two orders of magnitude more than in 2014. And the rate of progress as I see it is the slowest in 5 years. Time for something new  This tweet at the time of writing of this post has been retweeted nearly 350 times and liked >1300 times.  Conclusion  So there you go: the state of AI towards the end of 2018 - it is borderline comical (hence Krusty the clown in the title image). Certain things have changed however, some of the smoke dissipated and some of the mirrors cracked. Since my original post, a lot more mainstream media articles have shown up, in which the reporters are at least willing to exercise a possibility that we are on top of a giant AI bubble that is already letting the air out [e.g. this one and many others cited in the text above]. This spike of skepticism is a natural next step in the inevitable disillusionment, but I think it will take a while before this bubble finally deflates. The next 6 months are likely to be particularly interesting in this AI circus.  Update [Nov 1, 2018]: Apparently openAI actually beat Deepmind to solving Montezuma's revenge by instead of rewarding for winning the game, rewarding for avoiding predictable states. This is very interesting since it highlights how much predicting the state of the world is important, a core topic of this blog and Predictive Vision Model [1,2,3,4].  Update 2 [Jan 9, 2019]: Melanie Mitchell pointed out that Uber AI lab also solved Montezuma's revenge, it seems however they were a few weeks late after OpenAI. Nevertheless that settles it for good, we solved Montezuma's revenge, Yay!  If you found an error, highlight it and press Shift + Enter or click here to inform us.  Related  Comments  comments","driving, winter, hype, 2018, update, post, actually, self, waymo, system, tesla, ai",2018-10-29 00:00:00
5,AI winter,,http://en.wikipedia.org/wiki/AI_winter,"Period of reduced funding and interest in AI researchIn the history of artificial intelligence, an AI winter is a period of reduced funding and interest in artificial intelligence research. Roger Schank and Marvin Minsky—two leading AI researchers who experienced the ""winter"" of the 1970s—warned the business community that enthusiasm for AI had spiraled out of control in the 1980s and that disappointment would certainly follow. [6]Just like all AI booms that have been followed by desperate AI winters, the media tended to exaggerate the significance of these developments. ""[25] The result, Moravec claims, is that some of the staff at DARPA had lost patience with AI research. In 2006, Nick Bostrom explained that ""a lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.","Period of reduced funding and interest in AI research  In the history of artificial intelligence, an AI winter is a period of reduced funding and interest in artificial intelligence research.[1] The field has experienced several hype cycles, followed by disappointment and criticism, followed by funding cuts, followed by renewed interest years or even decades later.  The term first appeared in 1984 as the topic of a public debate at the annual meeting of AAAI (then called the ""American Association of Artificial Intelligence""). Roger Schank and Marvin Minsky—two leading AI researchers who experienced the ""winter"" of the 1970s—warned the business community that enthusiasm for AI had spiraled out of control in the 1980s and that disappointment would certainly follow. They described a chain reaction, similar to a ""nuclear winter"", that would begin with pessimism in the AI community, followed by pessimism in the press, followed by a severe cutback in funding, followed by the end of serious research. Three years later the billion-dollar AI industry began to collapse.  There were two major winters approximately 1974–1980 and 1987–2000,[3] and several smaller episodes, including the following:  Enthusiasm and optimism about AI has generally increased since its low point in the early 1990s. Beginning about 2012, interest in artificial intelligence (and especially the sub-field of machine learning) from the research and corporate communities led to a dramatic increase in funding and investment, leading to the current (as of 2024 ) AI boom.  Early episodes [ edit ]  Machine translation and the ALPAC report of 1966 [ edit ]  Natural language processing (NLP) research has its roots in the early 1930s and began its existence with the work on machine translation (MT).[4] However, significant advancements and applications began to emerge after the publication of Warren Weaver's influential memorandum in 1949.[5] The memorandum generated great excitement within the research community. In the following years, notable events unfolded: IBM embarked on the development of the first machine, MIT appointed its first full-time professor in machine translation, and several conferences dedicated to MT took place. The culmination came with the public demonstration of the IBM-Georgetown machine, which garnered widespread attention in respected newspapers in 1954.[6]  Just like all AI booms that have been followed by desperate AI winters, the media tended to exaggerate the significance of these developments. Headlines about the IBM-Georgetown experiment proclaimed phrases like ""The bilingual machine,"" ""Robot brain translates Russian into King's English,""[7] and ""Polyglot brainchild.""[8] However, the actual demonstration involved the translation of a curated set of only 49 Russian sentences into English, with the machine's vocabulary limited to just 250 words.[6] To put things into perspective, a 2006 study made by Paul Nation found that humans need a vocabulary of around 8,000 to 9,000-word families to comprehend written texts with 98% accuracy.[9]  During the Cold War, the US government was particularly interested in the automatic, instant translation of Russian documents and scientific reports. The government aggressively supported efforts at machine translation starting in 1954. Another factor that propelled the field of mechanical translation was the interest shown by the Central Intelligence Agency (CIA). During that period, the CIA firmly believed in the importance of developing machine translation capabilities and supported such initiatives. They also recognized that this program had implications that extended beyond the interests of the CIA and the intelligence community.[6]  At the outset, the researchers were optimistic. Noam Chomsky's new work in grammar was streamlining the translation process and there were ""many predictions of imminent 'breakthroughs'"".[10]  Briefing for US Vice President Gerald Ford in 1973 on the junction-grammar-based computer translation model  However, researchers had underestimated the profound difficulty of word-sense disambiguation. In order to translate a sentence, a machine needed to have some idea what the sentence was about, otherwise it made mistakes. An apocryphal[11] example is ""the spirit is willing but the flesh is weak."" Translated back and forth with Russian, it became ""the vodka is good but the meat is rotten."" Later researchers would call this the commonsense knowledge problem.  By 1964, the National Research Council had become concerned about the lack of progress and formed the Automatic Language Processing Advisory Committee (ALPAC) to look into the problem. They concluded, in a famous 1966 report, that machine translation was more expensive, less accurate and slower than human translation. After spending some 20 million dollars, the NRC ended all support. Careers were destroyed and research ended.[10]  Machine translation shared the same path with NLP from the rule-based approaches through the statistical approaches up to the neural network approaches, which have in 2023 culminated in large language models.  The failure of single-layer neural networks in 1969 [ edit ]  Simple networks or circuits of connected units, including Walter Pitts and Warren McCulloch's neural network for logic and Marvin Minsky's SNARC system, have failed to deliver the promised results and were abandoned in the late 1950s. Following the success of programs such as the Logic Theorist and the General Problem Solver,[13] algorithms for manipulating symbols seemed more promising at the time as means to achieve logical reasoning viewed at the time as the essence of intelligence, either natural or artificial.  Interest in perceptrons, invented by Frank Rosenblatt, was kept alive only by the sheer force of his personality.[14] He optimistically predicted that the perceptron ""may eventually be able to learn, make decisions, and translate languages"".[15] Mainstream research into perceptrons ended partially because the 1969 book Perceptrons by Marvin Minsky and Seymour Papert emphasized the limits of what perceptrons could do. While it was already known that multilayered perceptrons are not subject to the criticism, nobody in the 1960s knew how to train a multilayered perceptron. Backpropagation was still years away.[17]  Major funding for projects neural network approaches was difficult to find in the 1970s and early 1980s.[18] Important theoretical work continued despite the lack of funding. The ""winter"" of neural network approach came to an end in the middle 1980s, when the work of John Hopfield, David Rumelhart and others revived large scale interest.[19] Rosenblatt did not live to see this, however, as he died in a boating accident shortly after Perceptrons was published.[15]  The setbacks of 1974 [ edit ]  The Lighthill report [ edit ]  In 1973, professor Sir James Lighthill was asked by the UK Parliament to evaluate the state of AI research in the United Kingdom. His report, now called the Lighthill report, criticized the utter failure of AI to achieve its ""grandiose objectives"". He concluded that nothing being done in AI could not be done in other sciences. He specifically mentioned the problem of ""combinatorial explosion"" or ""intractability"", which implied that many of AI's most successful algorithms would grind to a halt on real world problems and were only suitable for solving ""toy"" versions.[20]  The report was contested in a debate broadcast in the BBC ""Controversy"" series in 1973. The debate ""The general purpose robot is a mirage"" from the Royal Institution was Lighthill versus the team of Donald Michie, John McCarthy and Richard Gregory.[21] McCarthy later wrote that ""the combinatorial explosion problem has been recognized in AI from the beginning"".[22]  The report led to the complete dismantling of AI research in the UK.[20] AI research continued in only a few universities (Edinburgh, Essex and Sussex). Research would not revive on a large scale until 1983, when Alvey (a research project of the British Government) began to fund AI again from a war chest of £350 million in response to the Japanese Fifth Generation Project (see below). Alvey had a number of UK-only requirements which did not sit well internationally, especially with US partners, and lost Phase 2 funding.  DARPA's early 1970s funding cuts [ edit ]  During the 1960s, the Defense Advanced Research Projects Agency (then known as ""ARPA"", now known as ""DARPA"") provided millions of dollars for AI research with few strings attached. J. C. R. Licklider, the founding director of DARPA's computing division, believed in ""funding people, not projects""[23] and he and several successors allowed AI's leaders (such as Marvin Minsky, John McCarthy, Herbert A. Simon or Allen Newell) to spend it almost any way they liked.  This attitude changed after the passage of Mansfield Amendment in 1969, which required DARPA to fund ""mission-oriented direct research, rather than basic undirected research"".[24] Pure undirected research of the kind that had gone on in the 1960s would no longer be funded by DARPA. Researchers now had to show that their work would soon produce some useful military technology. AI research proposals were held to a very high standard. The situation was not helped when the Lighthill report and DARPA's own study (the American Study Group) suggested that most AI research was unlikely to produce anything truly useful in the foreseeable future. DARPA's money was directed at specific projects with identifiable goals, such as autonomous tanks and battle management systems. By 1974, funding for AI projects was hard to find.[24]  AI researcher Hans Moravec blamed the crisis on the unrealistic predictions of his colleagues: ""Many researchers were caught up in a web of increasing exaggeration. Their initial promises to DARPA had been much too optimistic. Of course, what they delivered stopped considerably short of that. But they felt they couldn't in their next proposal promise less than in the first one, so they promised more.""[25] The result, Moravec claims, is that some of the staff at DARPA had lost patience with AI research. ""It was literally phrased at DARPA that 'some of these people were going to be taught a lesson [by] having their two-million-dollar-a-year contracts cut to almost nothing!'"" Moravec told Daniel Crevier.[26]  While the autonomous tank project was a failure, the battle management system (the Dynamic Analysis and Replanning Tool) proved to be enormously successful, saving billions in the first Gulf War, repaying all of DARPAs investment in AI[27] and justifying DARPA's pragmatic policy.[28]  The SUR debacle [ edit ]  As described in:[29]  In 1971, the Defense Advanced Research Projects Agency (DARPA) began an ambitious five-year experiment in speech understanding. The goals of the project were to provide recognition of utterances from a limited vocabulary in near-real time. Three organizations finally demonstrated systems at the conclusion of the project in 1976. These were Carnegie-Mellon University (CMU), who actually demonstrated two system [HEARSAY-II and HARPY]; Bolt, Beranek and Newman (BBN); and System Development Corporation with Stanford Research Institute (SDC/SRI)  The system that came closest to satisfying the original project goals was the CMU HARPY system. The relatively high performance of the HARPY system was largely achieved through 'hard-wiring' information about possible utterances into the system's knowledge base. Although HARPY made some interesting contributions, its dependence on extensive pre-knowledge limited the applicability of the approach to other signal-understanding tasks.  DARPA was deeply disappointed with researchers working on the Speech Understanding Research program at Carnegie Mellon University. DARPA had hoped for, and felt it had been promised, a system that could respond to voice commands from a pilot. The SUR team had developed a system which could recognize spoken English, but only if the words were spoken in a particular order. DARPA felt it had been duped and, in 1974, they cancelled a three million dollar a year contract.[30]  Many years later, several successful commercial speech recognition systems would use the technology developed by the Carnegie Mellon team (such as hidden Markov models) and the market for speech recognition systems would reach $4 billion by 2001.[31]  For a description of Hearsay-II see Hearsay-II, The Hearsay-II Speech Understanding System: Integrating Knowledge to Resolve Uncertainty and A Retrospective View of the Hearsay-II Architecture which appear in Blackboard Systems[32]  Reddy gives a review of progress in speech understanding at the end of the DARPA project in a 1976 article in Proceedings of the IEEE.[33]  Contrary view [ edit ]  Thomas Haigh argues that activity in the domain of AI did not slow down, even as funding from DoD was being redirected, mostly in the wake of congressional legislation meant to separate military and academic activities.[34] That indeed professional interest was growing throughout the 70s. Using the membership count of ACM's SIGART, the Special Interest Group on Artificial Intelligence, as a proxy for interest in the subject, the author writes:[34]  (...) I located two data sources, neither of which supports the idea of a broadly based AI winter during the 1970s. One is membership of ACM's SIGART, the major venue for sharing news and research abstracts during the 1970s. When the Lighthill report was published in 1973 the fast-growing group had 1,241 members, approximately twice the level in 1969. The next five years are conventionally thought of as the darkest part of the first AI winter. Was the AI community shrinking? No! By mid-1978 SIGART membership had almost tripled, to 3,500. Not only was the group growing faster than ever, it was increasing proportionally faster than ACM as a whole which had begun to plateau (expanding by less than 50% over the entire period from 1969 to 1978). One in every 11 ACM members was in SIGART.  The setbacks of the late 1980s and early 1990s [ edit ]  The collapse of the LISP machine market [ edit ]  In the 1980s, a form of AI program called an ""expert system"" was adopted by corporations around the world. The first commercial expert system was XCON, developed at Carnegie Mellon for Digital Equipment Corporation, and it was an enormous success: it was estimated to have saved the company 40 million dollars over just six years of operation. Corporations around the world began to develop and deploy expert systems and by 1985 they were spending over a billion dollars on AI, most of it to in-house AI departments. An industry grew up to support them, including software companies like Teknowledge and Intellicorp (KEE), and hardware companies like Symbolics and LISP Machines Inc. who built specialized computers, called LISP machines, that were optimized to process the programming language LISP, the preferred language for AI research in the USA.[35][36]  In 1987, three years after Minsky and Schank's prediction, the market for specialized LISP-based AI hardware collapsed. Workstations by companies like Sun Microsystems offered a powerful alternative to LISP machines and companies like Lucid offered a LISP environment for this new class of workstations. The performance of these general workstations became an increasingly difficult challenge for LISP Machines. Companies like Lucid and Franz LISP offered increasingly powerful versions of LISP that were portable to all UNIX systems. For example, benchmarks were published showing workstations maintaining a performance advantage over LISP machines.[37] Later desktop computers built by Apple and IBM would also offer a simpler and more popular architecture to run LISP applications on. By 1987, some of them had become as powerful as the more expensive LISP machines. The desktop computers had rule-based engines such as CLIPS available.[38] These alternatives left consumers with no reason to buy an expensive machine specialized for running LISP. An entire industry worth half a billion dollars was replaced in a single year.[39]  By the early 1990s, most commercial LISP companies had failed, including Symbolics, LISP Machines Inc., Lucid Inc., etc. Other companies, like Texas Instruments and Xerox, abandoned the field. A small number of customer companies (that is, companies using systems written in LISP and developed on LISP machine platforms) continued to maintain systems. In some cases, this maintenance involved the assumption of the resulting support work.[40]  Slowdown in deployment of expert systems [ edit ]  By the early 1990s, the earliest successful expert systems, such as XCON, proved too expensive to maintain. They were difficult to update, they could not learn, they were ""brittle"" (i.e., they could make grotesque mistakes when given unusual inputs), and they fell prey to problems (such as the qualification problem) that had been identified years earlier in research in nonmonotonic logic. Expert systems proved useful, but only in a few special contexts.[41][42] Another problem dealt with the computational hardness of truth maintenance efforts for general knowledge. KEE used an assumption-based approach supporting multiple-world scenarios that was difficult to understand and apply.  The few remaining expert system shell companies were eventually forced to downsize and search for new markets and software paradigms, like case-based reasoning or universal database access. The maturation of Common Lisp saved many systems such as ICAD which found application in knowledge-based engineering. Other systems, such as Intellicorp's KEE, moved from LISP to a C++ (variant) on the PC and helped establish object-oriented technology (including providing major support for the development of UML (see UML Partners).  The end of the Fifth Generation project [ edit ]  In 1981, the Japanese Ministry of International Trade and Industry set aside $850 million for the Fifth Generation computer project. Their objectives were to write programs and build machines that could carry on conversations, translate languages, interpret pictures, and reason like human beings. By 1991, the impressive list of goals penned in 1981 had not been met. According to HP Newquist in The Brain Makers, ""On June 1, 1992, The Fifth Generation Project ended not with a successful roar, but with a whimper.""[40] As with other AI projects, expectations had run much higher than what was actually possible.[43][44]  Strategic Computing Initiative cutbacks [ edit ]  In 1983, in response to the fifth generation project, DARPA again began to fund AI research through the Strategic Computing Initiative. As originally proposed the project would begin with practical, achievable goals, which even included artificial general intelligence as long-term objective. The program was under the direction of the Information Processing Technology Office (IPTO) and was also directed at supercomputing and microelectronics. By 1985 it had spent $100 million and 92 projects were underway at 60 institutions, half in industry, half in universities and government labs. AI research was well-funded by the SCI.[45]  Jack Schwarz, who ascended to the leadership of IPTO in 1987, dismissed expert systems as ""clever programming"" and cut funding to AI ""deeply and brutally"", ""eviscerating"" SCI. Schwarz felt that DARPA should focus its funding only on those technologies which showed the most promise, in his words, DARPA should ""surf"", rather than ""dog paddle"", and he felt strongly AI was not ""the next wave"". Insiders in the program cited problems in communication, organization and integration. A few projects survived the funding cuts, including pilot's assistant and an autonomous land vehicle (which were never delivered) and the DART battle management system, which (as noted above) was successful.[46]  AI winter of the 1990's and early 2000's [ edit ]  A survey of reports from the early 2000's suggests that AI's reputation was still poor:  Alex Castro, quoted in The Economist , 7 June 2007: ""[Investors] were put off by the term 'voice recognition' which, like 'artificial intelligence', is associated with systems that have all too often failed to live up to their promises."" [ 47 ]  , 7 June 2007: ""[Investors] were put off by the term 'voice recognition' which, like 'artificial intelligence', is associated with systems that have all too often failed to live up to their promises."" Patty Tascarella in Pittsburgh Business Times , 2006: ""Some believe the word 'robotics' actually carries a stigma that hurts a company's chances at funding."" [ 48 ]  , 2006: ""Some believe the word 'robotics' actually carries a stigma that hurts a company's chances at funding."" John Markoff in the New York Times, 2005: ""At its low point, some computer scientists and software engineers avoided the term artificial intelligence for fear of being viewed as wild-eyed dreamers."" [ 49 ]  Many researchers in AI in the mid 2000's deliberately called their work by other names, such as informatics, machine learning, analytics, knowledge-based systems, business rules management, cognitive systems, intelligent systems, intelligent agents or computational intelligence, to indicate that their work emphasizes particular tools or is directed at a particular sub-problem. Although this may be partly because they consider their field to be fundamentally different from AI, it is also true that the new names help to procure funding by avoiding the stigma of false promises attached to the name ""artificial intelligence"".[49][50]  In the late 1990's and early 21st century, AI technology became widely used as elements of larger systems,[51] but the field is rarely credited for these successes. In 2006, Nick Bostrom explained that ""a lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.""[53] Rodney Brooks stated around the same time that ""there's this stupid myth out there that AI has failed, but AI is around you every second of the day.""  Current AI spring (2022–present) [ edit ]  AI has reached the highest levels of interest and funding in its history in the past few years by every possible measure, including: publications, patent applications,[56] total investment ($50 billion in 2022), and job openings (800,000 U.S. job openings in 2022). The successes of the current ""AI spring"" or ""AI boom"" are advances in language translation (in particular, Google Translate), image recognition (spurred by the ImageNet training database) as commercialized by Google Image Search, and in game-playing systems such as AlphaZero (chess champion) and AlphaGo (go champion), and Watson (Jeopardy champion). A turning point was in 2012 when AlexNet (a deep learning network) won the ImageNet Large Scale Visual Recognition Challenge with half as many errors as the second place winner.  The 2022 release of OpenAI's AI chatbot ChatGPT which as of January 2023 has over 100 million users,[60] has reinvigorated the discussion about artificial intelligence and its effects on the world.[61][62]  See also [ edit ]  Notes [ edit ]  References [ edit ]","research, translation, lisp, winter, funding, system, systems, intelligence, machine, ai, edit",
6,The AI Winter: some possible scenarios – Grey Enlightenment,,https://greyenlightenment.com/2023/04/08/the-ai-winter-some-possible-scenarios/,"Admittedly, Chat GPT is a much bigger, more comprehensive product than Dall-E, but the pattern seems the same. Similar to Wolfram Alpha, students are using Chat GPT to circumvent school work, but for writing assignments instead of math assignments. Like Wolfram Alpha, Open AI offers two tiers: a free service and a subscription, which costs $20/month. VCs are trying to create media hype surrounding Open AI in anticipation of an IPO , so they can sell inflated shares on retail suckers/investors. In Jan. 2021, Open AI raised capital from four VC firms, including Andreessen Horowitz and Sequoia Capital, which are among the biggest players in VC.","Here are some possible scenarios for the ‘AI winter’:  1. GPUs are regulated. Or an AI mortarium. This is the solution Eliezer Yudkowsky advocates. The likelihood of this happening is really slim. Good luck getting Congress to ever agree to such a thing, given that four years later TikTok has still not been banned despite a stronger case for TikTok being a threat to national security compared to AI. Same for cryptocurrencies despite being used for fraud and cybercrime. And good luck getting China to ever go along with an AI mortarium unless it voluntarily agrees to it.  2. Major economic crisis/recession, like 2007-2009.  3. The third and imho the most likely outcome is that users and the media simply lose interest. Subsequent iterations of GPT fail to live up to hype.  There was huge surge of interest in Chat GPT after it was released to the pubic in Nov. 2022:  Remember all the hype in May/June 2022 about Dall-E? So much for that:  I remember the dire pronouncements at the time about how Dall-E would render artists unemployable and cause a crisis in the arts by flooding the market with AI-generated knockoffs.  What happened? I am guessing the media and users lost interest. There was a 6 month gap between Dall-E and the public unveiling of Chat GPT, so it’s not like the latter stole the spotlight former. Admittedly, Chat GPT is a much bigger, more comprehensive product than Dall-E, but the pattern seems the same.  4. The overuse of paywalls and the degradation of free functionality of Open AI products can cause users to lose interest, hence less virality on social media. Wolfram Alpha debuted in 2009 to similar media fanfare for its AI capabilities. However, it stopped being as well maintained, put most of its functionality behind a paywall, it’s very buggy , and hardly anyone talks about it or cares about it anymore despite all the attention it got earlier.  Similar to Wolfram Alpha, students are using Chat GPT to circumvent school work, but for writing assignments instead of math assignments. Like Wolfram Alpha, Open AI offers two tiers: a free service and a subscription, which costs $20/month. The free service however puts stress on the servers, and given that students are either incapable or unwilling to spend money on subscription plans, imposes a significant cost on Open AI. This is why the free version of Wolfram Alpha became much less useful and heavily metered, with much of the original free functionality put behind various subscription plans.  5. VCs find a new fad. AI is the latest bubble for 2022-2023 after the crypto/DEFI and ‘WFH’ bubbles imploded in 2021-2022. VCs are trying to create media hype surrounding Open AI in anticipation of an IPO , so they can sell inflated shares on retail suckers/investors. It’s the same pattern as always. In Jan. 2021, Open AI raised capital from four VC firms, including Andreessen Horowitz and Sequoia Capital, which are among the biggest players in VC. Now they are looking to ‘exit’ , which means hopefully an IPO.  Overall, based on my 15+ years of writing about and following and investing in consumer tech, pundits either vastly overestimate or underestimate the influence of technologies. Same for market valuations. Facebook/Meta has been underestimated since its founding: “It’s just another Myspace…it will be a fad…teens/kids will grow up and lose interest in it,” yet over a decade since its IPO it’s worth over half a trillion dollars and earns tens of billions of dollars in annual profits. As for teens losing interest, the experts failed to anticipate that Facebook would capture the highly lucrative ‘boomer’ market, and that teens would move on to Instagram, which is also owned by Meta and has very lucrative, high-paying mobile ads. Or the iPhone: “Blackberry is better…saturated market,” yet 15 years since the debut of the iPhone, Apple is worth over $2 trillion. Or Tesla: “Too expensive, charging stations unfeasible, does not work well in cold weather, limited distance, etc.”  In 2017-2018 ‘experts’ were giving 6-7 figure price forecasts for Bitcoin for 2020+, and here we are in 2023 and Bitcoin is still at $28,000, well below even the most conservative of bullish forecasts and only up 50% since late 2017. In terms of adoption, Bitcoin is still mostly a niche and is nowhere close to replacing or even disrupting the financial system, but as shown by the collapse of Silicon Valley Bank and Signature Bank, seems highly dependent on it and just another highly correlated risky asset. My own take/guess is that AI falls into the same category as Bitcoin, as something in which pundits are expecting too much change, either good or bad.","interest, winter, possible, market, gpt, free, open, wolfram, dalle, scenarios, grey, enlightenment, chat, media, ai",2023-04-08 00:00:00
7,AI winter,,https://en.wikipedia.org/wiki/AI_winter,"Period of reduced funding and interest in AI researchIn the history of artificial intelligence, an AI winter is a period of reduced funding and interest in artificial intelligence research. Roger Schank and Marvin Minsky—two leading AI researchers who experienced the ""winter"" of the 1970s—warned the business community that enthusiasm for AI had spiraled out of control in the 1980s and that disappointment would certainly follow. [6]Just like all AI booms that have been followed by desperate AI winters, the media tended to exaggerate the significance of these developments. ""[25] The result, Moravec claims, is that some of the staff at DARPA had lost patience with AI research. In 2006, Nick Bostrom explained that ""a lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.","Period of reduced funding and interest in AI research  In the history of artificial intelligence, an AI winter is a period of reduced funding and interest in artificial intelligence research.[1] The field has experienced several hype cycles, followed by disappointment and criticism, followed by funding cuts, followed by renewed interest years or even decades later.  The term first appeared in 1984 as the topic of a public debate at the annual meeting of AAAI (then called the ""American Association of Artificial Intelligence""). Roger Schank and Marvin Minsky—two leading AI researchers who experienced the ""winter"" of the 1970s—warned the business community that enthusiasm for AI had spiraled out of control in the 1980s and that disappointment would certainly follow. They described a chain reaction, similar to a ""nuclear winter"", that would begin with pessimism in the AI community, followed by pessimism in the press, followed by a severe cutback in funding, followed by the end of serious research. Three years later the billion-dollar AI industry began to collapse.  There were two major winters approximately 1974–1980 and 1987–2000,[3] and several smaller episodes, including the following:  Enthusiasm and optimism about AI has generally increased since its low point in the early 1990s. Beginning about 2012, interest in artificial intelligence (and especially the sub-field of machine learning) from the research and corporate communities led to a dramatic increase in funding and investment, leading to the current (as of 2024 ) AI boom.  Early episodes [ edit ]  Machine translation and the ALPAC report of 1966 [ edit ]  Natural language processing (NLP) research has its roots in the early 1930s and began its existence with the work on machine translation (MT).[4] However, significant advancements and applications began to emerge after the publication of Warren Weaver's influential memorandum in 1949.[5] The memorandum generated great excitement within the research community. In the following years, notable events unfolded: IBM embarked on the development of the first machine, MIT appointed its first full-time professor in machine translation, and several conferences dedicated to MT took place. The culmination came with the public demonstration of the IBM-Georgetown machine, which garnered widespread attention in respected newspapers in 1954.[6]  Just like all AI booms that have been followed by desperate AI winters, the media tended to exaggerate the significance of these developments. Headlines about the IBM-Georgetown experiment proclaimed phrases like ""The bilingual machine,"" ""Robot brain translates Russian into King's English,""[7] and ""Polyglot brainchild.""[8] However, the actual demonstration involved the translation of a curated set of only 49 Russian sentences into English, with the machine's vocabulary limited to just 250 words.[6] To put things into perspective, a 2006 study made by Paul Nation found that humans need a vocabulary of around 8,000 to 9,000-word families to comprehend written texts with 98% accuracy.[9]  During the Cold War, the US government was particularly interested in the automatic, instant translation of Russian documents and scientific reports. The government aggressively supported efforts at machine translation starting in 1954. Another factor that propelled the field of mechanical translation was the interest shown by the Central Intelligence Agency (CIA). During that period, the CIA firmly believed in the importance of developing machine translation capabilities and supported such initiatives. They also recognized that this program had implications that extended beyond the interests of the CIA and the intelligence community.[6]  At the outset, the researchers were optimistic. Noam Chomsky's new work in grammar was streamlining the translation process and there were ""many predictions of imminent 'breakthroughs'"".[10]  Briefing for US Vice President Gerald Ford in 1973 on the junction-grammar-based computer translation model  However, researchers had underestimated the profound difficulty of word-sense disambiguation. In order to translate a sentence, a machine needed to have some idea what the sentence was about, otherwise it made mistakes. An apocryphal[11] example is ""the spirit is willing but the flesh is weak."" Translated back and forth with Russian, it became ""the vodka is good but the meat is rotten."" Later researchers would call this the commonsense knowledge problem.  By 1964, the National Research Council had become concerned about the lack of progress and formed the Automatic Language Processing Advisory Committee (ALPAC) to look into the problem. They concluded, in a famous 1966 report, that machine translation was more expensive, less accurate and slower than human translation. After spending some 20 million dollars, the NRC ended all support. Careers were destroyed and research ended.[10]  Machine translation shared the same path with NLP from the rule-based approaches through the statistical approaches up to the neural network approaches, which have in 2023 culminated in large language models.  The failure of single-layer neural networks in 1969 [ edit ]  Simple networks or circuits of connected units, including Walter Pitts and Warren McCulloch's neural network for logic and Marvin Minsky's SNARC system, have failed to deliver the promised results and were abandoned in the late 1950s. Following the success of programs such as the Logic Theorist and the General Problem Solver,[13] algorithms for manipulating symbols seemed more promising at the time as means to achieve logical reasoning viewed at the time as the essence of intelligence, either natural or artificial.  Interest in perceptrons, invented by Frank Rosenblatt, was kept alive only by the sheer force of his personality.[14] He optimistically predicted that the perceptron ""may eventually be able to learn, make decisions, and translate languages"".[15] Mainstream research into perceptrons ended partially because the 1969 book Perceptrons by Marvin Minsky and Seymour Papert emphasized the limits of what perceptrons could do. While it was already known that multilayered perceptrons are not subject to the criticism, nobody in the 1960s knew how to train a multilayered perceptron. Backpropagation was still years away.[17]  Major funding for projects neural network approaches was difficult to find in the 1970s and early 1980s.[18] Important theoretical work continued despite the lack of funding. The ""winter"" of neural network approach came to an end in the middle 1980s, when the work of John Hopfield, David Rumelhart and others revived large scale interest.[19] Rosenblatt did not live to see this, however, as he died in a boating accident shortly after Perceptrons was published.[15]  The setbacks of 1974 [ edit ]  The Lighthill report [ edit ]  In 1973, professor Sir James Lighthill was asked by the UK Parliament to evaluate the state of AI research in the United Kingdom. His report, now called the Lighthill report, criticized the utter failure of AI to achieve its ""grandiose objectives"". He concluded that nothing being done in AI could not be done in other sciences. He specifically mentioned the problem of ""combinatorial explosion"" or ""intractability"", which implied that many of AI's most successful algorithms would grind to a halt on real world problems and were only suitable for solving ""toy"" versions.[20]  The report was contested in a debate broadcast in the BBC ""Controversy"" series in 1973. The debate ""The general purpose robot is a mirage"" from the Royal Institution was Lighthill versus the team of Donald Michie, John McCarthy and Richard Gregory.[21] McCarthy later wrote that ""the combinatorial explosion problem has been recognized in AI from the beginning"".[22]  The report led to the complete dismantling of AI research in the UK.[20] AI research continued in only a few universities (Edinburgh, Essex and Sussex). Research would not revive on a large scale until 1983, when Alvey (a research project of the British Government) began to fund AI again from a war chest of £350 million in response to the Japanese Fifth Generation Project (see below). Alvey had a number of UK-only requirements which did not sit well internationally, especially with US partners, and lost Phase 2 funding.  DARPA's early 1970s funding cuts [ edit ]  During the 1960s, the Defense Advanced Research Projects Agency (then known as ""ARPA"", now known as ""DARPA"") provided millions of dollars for AI research with few strings attached. J. C. R. Licklider, the founding director of DARPA's computing division, believed in ""funding people, not projects""[23] and he and several successors allowed AI's leaders (such as Marvin Minsky, John McCarthy, Herbert A. Simon or Allen Newell) to spend it almost any way they liked.  This attitude changed after the passage of Mansfield Amendment in 1969, which required DARPA to fund ""mission-oriented direct research, rather than basic undirected research"".[24] Pure undirected research of the kind that had gone on in the 1960s would no longer be funded by DARPA. Researchers now had to show that their work would soon produce some useful military technology. AI research proposals were held to a very high standard. The situation was not helped when the Lighthill report and DARPA's own study (the American Study Group) suggested that most AI research was unlikely to produce anything truly useful in the foreseeable future. DARPA's money was directed at specific projects with identifiable goals, such as autonomous tanks and battle management systems. By 1974, funding for AI projects was hard to find.[24]  AI researcher Hans Moravec blamed the crisis on the unrealistic predictions of his colleagues: ""Many researchers were caught up in a web of increasing exaggeration. Their initial promises to DARPA had been much too optimistic. Of course, what they delivered stopped considerably short of that. But they felt they couldn't in their next proposal promise less than in the first one, so they promised more.""[25] The result, Moravec claims, is that some of the staff at DARPA had lost patience with AI research. ""It was literally phrased at DARPA that 'some of these people were going to be taught a lesson [by] having their two-million-dollar-a-year contracts cut to almost nothing!'"" Moravec told Daniel Crevier.[26]  While the autonomous tank project was a failure, the battle management system (the Dynamic Analysis and Replanning Tool) proved to be enormously successful, saving billions in the first Gulf War, repaying all of DARPAs investment in AI[27] and justifying DARPA's pragmatic policy.[28]  The SUR debacle [ edit ]  As described in:[29]  In 1971, the Defense Advanced Research Projects Agency (DARPA) began an ambitious five-year experiment in speech understanding. The goals of the project were to provide recognition of utterances from a limited vocabulary in near-real time. Three organizations finally demonstrated systems at the conclusion of the project in 1976. These were Carnegie-Mellon University (CMU), who actually demonstrated two system [HEARSAY-II and HARPY]; Bolt, Beranek and Newman (BBN); and System Development Corporation with Stanford Research Institute (SDC/SRI)  The system that came closest to satisfying the original project goals was the CMU HARPY system. The relatively high performance of the HARPY system was largely achieved through 'hard-wiring' information about possible utterances into the system's knowledge base. Although HARPY made some interesting contributions, its dependence on extensive pre-knowledge limited the applicability of the approach to other signal-understanding tasks.  DARPA was deeply disappointed with researchers working on the Speech Understanding Research program at Carnegie Mellon University. DARPA had hoped for, and felt it had been promised, a system that could respond to voice commands from a pilot. The SUR team had developed a system which could recognize spoken English, but only if the words were spoken in a particular order. DARPA felt it had been duped and, in 1974, they cancelled a three million dollar a year contract.[30]  Many years later, several successful commercial speech recognition systems would use the technology developed by the Carnegie Mellon team (such as hidden Markov models) and the market for speech recognition systems would reach $4 billion by 2001.[31]  For a description of Hearsay-II see Hearsay-II, The Hearsay-II Speech Understanding System: Integrating Knowledge to Resolve Uncertainty and A Retrospective View of the Hearsay-II Architecture which appear in Blackboard Systems[32]  Reddy gives a review of progress in speech understanding at the end of the DARPA project in a 1976 article in Proceedings of the IEEE.[33]  Contrary view [ edit ]  Thomas Haigh argues that activity in the domain of AI did not slow down, even as funding from DoD was being redirected, mostly in the wake of congressional legislation meant to separate military and academic activities.[34] That indeed professional interest was growing throughout the 70s. Using the membership count of ACM's SIGART, the Special Interest Group on Artificial Intelligence, as a proxy for interest in the subject, the author writes:[34]  (...) I located two data sources, neither of which supports the idea of a broadly based AI winter during the 1970s. One is membership of ACM's SIGART, the major venue for sharing news and research abstracts during the 1970s. When the Lighthill report was published in 1973 the fast-growing group had 1,241 members, approximately twice the level in 1969. The next five years are conventionally thought of as the darkest part of the first AI winter. Was the AI community shrinking? No! By mid-1978 SIGART membership had almost tripled, to 3,500. Not only was the group growing faster than ever, it was increasing proportionally faster than ACM as a whole which had begun to plateau (expanding by less than 50% over the entire period from 1969 to 1978). One in every 11 ACM members was in SIGART.  The setbacks of the late 1980s and early 1990s [ edit ]  The collapse of the LISP machine market [ edit ]  In the 1980s, a form of AI program called an ""expert system"" was adopted by corporations around the world. The first commercial expert system was XCON, developed at Carnegie Mellon for Digital Equipment Corporation, and it was an enormous success: it was estimated to have saved the company 40 million dollars over just six years of operation. Corporations around the world began to develop and deploy expert systems and by 1985 they were spending over a billion dollars on AI, most of it to in-house AI departments. An industry grew up to support them, including software companies like Teknowledge and Intellicorp (KEE), and hardware companies like Symbolics and LISP Machines Inc. who built specialized computers, called LISP machines, that were optimized to process the programming language LISP, the preferred language for AI research in the USA.[35][36]  In 1987, three years after Minsky and Schank's prediction, the market for specialized LISP-based AI hardware collapsed. Workstations by companies like Sun Microsystems offered a powerful alternative to LISP machines and companies like Lucid offered a LISP environment for this new class of workstations. The performance of these general workstations became an increasingly difficult challenge for LISP Machines. Companies like Lucid and Franz LISP offered increasingly powerful versions of LISP that were portable to all UNIX systems. For example, benchmarks were published showing workstations maintaining a performance advantage over LISP machines.[37] Later desktop computers built by Apple and IBM would also offer a simpler and more popular architecture to run LISP applications on. By 1987, some of them had become as powerful as the more expensive LISP machines. The desktop computers had rule-based engines such as CLIPS available.[38] These alternatives left consumers with no reason to buy an expensive machine specialized for running LISP. An entire industry worth half a billion dollars was replaced in a single year.[39]  By the early 1990s, most commercial LISP companies had failed, including Symbolics, LISP Machines Inc., Lucid Inc., etc. Other companies, like Texas Instruments and Xerox, abandoned the field. A small number of customer companies (that is, companies using systems written in LISP and developed on LISP machine platforms) continued to maintain systems. In some cases, this maintenance involved the assumption of the resulting support work.[40]  Slowdown in deployment of expert systems [ edit ]  By the early 1990s, the earliest successful expert systems, such as XCON, proved too expensive to maintain. They were difficult to update, they could not learn, they were ""brittle"" (i.e., they could make grotesque mistakes when given unusual inputs), and they fell prey to problems (such as the qualification problem) that had been identified years earlier in research in nonmonotonic logic. Expert systems proved useful, but only in a few special contexts.[41][42] Another problem dealt with the computational hardness of truth maintenance efforts for general knowledge. KEE used an assumption-based approach supporting multiple-world scenarios that was difficult to understand and apply.  The few remaining expert system shell companies were eventually forced to downsize and search for new markets and software paradigms, like case-based reasoning or universal database access. The maturation of Common Lisp saved many systems such as ICAD which found application in knowledge-based engineering. Other systems, such as Intellicorp's KEE, moved from LISP to a C++ (variant) on the PC and helped establish object-oriented technology (including providing major support for the development of UML (see UML Partners).  The end of the Fifth Generation project [ edit ]  In 1981, the Japanese Ministry of International Trade and Industry set aside $850 million for the Fifth Generation computer project. Their objectives were to write programs and build machines that could carry on conversations, translate languages, interpret pictures, and reason like human beings. By 1991, the impressive list of goals penned in 1981 had not been met. According to HP Newquist in The Brain Makers, ""On June 1, 1992, The Fifth Generation Project ended not with a successful roar, but with a whimper.""[40] As with other AI projects, expectations had run much higher than what was actually possible.[43][44]  Strategic Computing Initiative cutbacks [ edit ]  In 1983, in response to the fifth generation project, DARPA again began to fund AI research through the Strategic Computing Initiative. As originally proposed the project would begin with practical, achievable goals, which even included artificial general intelligence as long-term objective. The program was under the direction of the Information Processing Technology Office (IPTO) and was also directed at supercomputing and microelectronics. By 1985 it had spent $100 million and 92 projects were underway at 60 institutions, half in industry, half in universities and government labs. AI research was well-funded by the SCI.[45]  Jack Schwarz, who ascended to the leadership of IPTO in 1987, dismissed expert systems as ""clever programming"" and cut funding to AI ""deeply and brutally"", ""eviscerating"" SCI. Schwarz felt that DARPA should focus its funding only on those technologies which showed the most promise, in his words, DARPA should ""surf"", rather than ""dog paddle"", and he felt strongly AI was not ""the next wave"". Insiders in the program cited problems in communication, organization and integration. A few projects survived the funding cuts, including pilot's assistant and an autonomous land vehicle (which were never delivered) and the DART battle management system, which (as noted above) was successful.[46]  AI winter of the 1990's and early 2000's [ edit ]  A survey of reports from the early 2000's suggests that AI's reputation was still poor:  Alex Castro, quoted in The Economist , 7 June 2007: ""[Investors] were put off by the term 'voice recognition' which, like 'artificial intelligence', is associated with systems that have all too often failed to live up to their promises."" [ 47 ]  , 7 June 2007: ""[Investors] were put off by the term 'voice recognition' which, like 'artificial intelligence', is associated with systems that have all too often failed to live up to their promises."" Patty Tascarella in Pittsburgh Business Times , 2006: ""Some believe the word 'robotics' actually carries a stigma that hurts a company's chances at funding."" [ 48 ]  , 2006: ""Some believe the word 'robotics' actually carries a stigma that hurts a company's chances at funding."" John Markoff in the New York Times, 2005: ""At its low point, some computer scientists and software engineers avoided the term artificial intelligence for fear of being viewed as wild-eyed dreamers."" [ 49 ]  Many researchers in AI in the mid 2000's deliberately called their work by other names, such as informatics, machine learning, analytics, knowledge-based systems, business rules management, cognitive systems, intelligent systems, intelligent agents or computational intelligence, to indicate that their work emphasizes particular tools or is directed at a particular sub-problem. Although this may be partly because they consider their field to be fundamentally different from AI, it is also true that the new names help to procure funding by avoiding the stigma of false promises attached to the name ""artificial intelligence"".[49][50]  In the late 1990's and early 21st century, AI technology became widely used as elements of larger systems,[51] but the field is rarely credited for these successes. In 2006, Nick Bostrom explained that ""a lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.""[53] Rodney Brooks stated around the same time that ""there's this stupid myth out there that AI has failed, but AI is around you every second of the day.""  Current AI spring (2022–present) [ edit ]  AI has reached the highest levels of interest and funding in its history in the past few years by every possible measure, including: publications, patent applications,[56] total investment ($50 billion in 2022), and job openings (800,000 U.S. job openings in 2022). The successes of the current ""AI spring"" or ""AI boom"" are advances in language translation (in particular, Google Translate), image recognition (spurred by the ImageNet training database) as commercialized by Google Image Search, and in game-playing systems such as AlphaZero (chess champion) and AlphaGo (go champion), and Watson (Jeopardy champion). A turning point was in 2012 when AlexNet (a deep learning network) won the ImageNet Large Scale Visual Recognition Challenge with half as many errors as the second place winner.  The 2022 release of OpenAI's AI chatbot ChatGPT which as of January 2023 has over 100 million users,[60] has reinvigorated the discussion about artificial intelligence and its effects on the world.[61][62]  See also [ edit ]  Notes [ edit ]  References [ edit ]","research, translation, lisp, winter, funding, system, systems, intelligence, machine, ai, edit",
8,Self-driving cars are headed toward an AI roadblock,Russell Brandom,https://www.theverge.com/2018/7/3/17530232/self-driving-ai-winter-full-autonomy-waymo-tesla-uber,"There’s growing concern among AI experts that it may be years, if not decades, before self-driving systems can reliably avoid accidents. That leaves Tesla and other autonomy companies with a scary question: Will self-driving cars keep getting better, like image search, voice recognition, and the other AI success stories? But nearly every car accident involves some sort of unforeseen circumstance, and without the power to generalize, self-driving cars will have to confront each of these scenarios as if for the first time. “I see all these micro-improvements as extraordinary features on the journey towards full autonomy.”Still, it’s not clear how long self-driving cars can stay in their current limbo. One study by the Rand Corporation estimated that self-driving cars would have to drive 275 million miles without a fatality to prove they were as safe as human drivers.","Part of / The Real-World AI Issue  If you believe the CEOs, a fully autonomous car could be only months away. In 2015, Elon Musk predicted a fully autonomous Tesla by 2018; so did Google. Delphi and MobileEye’s Level 4 system is currently slated for 2019, the same year Nutonomy plans to deploy thousands of driverless taxis on the streets of Singapore. GM will put a fully autonomous car into production in 2019, with no steering wheel or ability for drivers to intervene. There’s real money behind these predictions, bets made on the assumption that the software will be able to catch up to the hype.  On its face, full autonomy seems closer than ever. Waymo is already testing cars on limited-but-public roads in Arizona. Tesla and a host of other imitators already sell a limited form of Autopilot, counting on drivers to intervene if anything unexpected happens. There have been a few crashes, some deadly, but as long as the systems keep improving, the logic goes, we can’t be that far from not having to intervene at all.  But the dream of a fully autonomous car may be further than we realize. There’s growing concern among AI experts that it may be years, if not decades, before self-driving systems can reliably avoid accidents. As self-trained systems grapple with the chaos of the real world, experts like NYU’s Gary Marcus are bracing for a painful recalibration in expectations, a correction sometimes called “AI winter.” That delay could have disastrous consequences for companies banking on self-driving technology, putting full autonomy out of reach for an entire generation.  “Driverless cars are like a scientific experiment where we don’t know the answer”  It’s easy to see why car companies are optimistic about autonomy. Over the past ten years, deep learning — a method that uses layered machine-learning algorithms to extract structured information from massive data sets — has driven almost unthinkable progress in AI and the tech industry. It powers Google Search, the Facebook News Feed, conversational speech-to-text algorithms, and champion Go-playing systems. Outside the internet, we use deep learning to detect earthquakes, predict heart disease, and flag suspicious behavior on a camera feed, along with countless other innovations that would have been impossible otherwise.  But deep learning requires massive amounts of training data to work properly, incorporating nearly every scenario the algorithm will encounter. Systems like Google Images, for instance, are great at recognizing animals as long as they have training data to show them what each animal looks like. Marcus describes this kind of task as “interpolation,” taking a survey of all the images labeled “ocelot” and deciding whether the new picture belongs in the group.  Engineers can get creative in where the data comes from and how it’s structured, but it places a hard limit on how far a given algorithm can reach. The same algorithm can’t recognize an ocelot unless it’s seen thousands of pictures of an ocelot — even if it’s seen pictures of housecats and jaguars, and knows ocelots are somewhere in between. That process, called “generalization,” requires a different set of skills.  For a long time, researchers thought they could improve generalization skills with the right algorithms, but recent research has shown that conventional deep learning is even worse at generalizing than we thought. One study found that conventional deep learning systems have a hard time even generalizing across different frames of a video, labeling the same polar bear as a baboon, mongoose, or weasel depending on minor shifts in the background. With each classification based on hundreds of factors in aggregate, even small changes to pictures can completely change the system’s judgment, something other researchers have taken advantage of in adversarial data sets.  Marcus points to the chat bot craze as the most recent example of hype running up against the generalization problem. “We were promised chat bots in 2015,” he says, “but they’re not any good because it’s not just a matter of collecting data.” When you’re talking to a person online, you don’t just want them to rehash earlier conversations. You want them to respond to what you’re saying, drawing on broader conversational skills to produce a response that’s unique to you. Deep learning just couldn’t make that kind of chat bot. Once the initial hype faded, companies lost faith in their chat bot projects, and there are very few still in active development.  That leaves Tesla and other autonomy companies with a scary question: Will self-driving cars keep getting better, like image search, voice recognition, and the other AI success stories? Or will they run into the generalization problem like chat bots? Is autonomy an interpolation problem or a generalization problem? How unpredictable is driving, really?  It may be too early to know. “Driverless cars are like a scientific experiment where we don’t know the answer,” Marcus says. We’ve never been able to automate driving at this level before, so we don’t know what kind of task it is. To the extent that it’s about identifying familiar objects and following rules, existing technologies should be up to the task. But Marcus worries that driving well in accident-prone scenarios may be more complicated than the industry wants to admit. “To the extent that surprising new things happen, it’s not a good thing for deep learning.”  “Safety isn’t just about the quality of the AI technology”  The experimental data we have comes from public accident reports, each of which offers some unusual wrinkle. A fatal 2016 crash saw a Model S drive full speed into the rear portion of a white tractor trailer, confused by the high ride height of the trailer and bright reflection of the sun. In March, a self-driving Uber crash killed a woman pushing a bicycle, after she emerged from an unauthorized crosswalk. According to the NTSB report, Uber’s software misidentified the woman as an unknown object, then a vehicle, then finally as a bicycle, updating its projections each time. In a California crash, a Model X steered toward a barrier and sped up in the moments before impact, for reasons that remain unclear.  Each accident seems like an edge case, the kind of thing engineers couldn’t be expected to predict in advance. But nearly every car accident involves some sort of unforeseen circumstance, and without the power to generalize, self-driving cars will have to confront each of these scenarios as if for the first time. The result would be a string of fluke-y accidents that don’t get less common or less dangerous as time goes on. For skeptics, a turn through the manual disengagement reports shows that scenario already well under way, with progress already reaching a plateau.  Andrew Ng — a former Baidu executive, Drive.AI board member, and one of the industry’s most prominent boosters — argues the problem is less about building a perfect driving system than training bystanders to anticipate self-driving behavior. In other words, we can make roads safe for the cars instead of the other way around. As an example of an unpredictable case, I asked him whether he thought modern systems could handle a pedestrian on a pogo stick, even if they had never seen one before. “I think many AV teams could handle a pogo stick user in pedestrian crosswalk,” Ng told me. “Having said that, bouncing on a pogo stick in the middle of a highway would be really dangerous.”  “Rather than building AI to solve the pogo stick problem, we should partner with the government to ask people to be lawful and considerate,” he said. “Safety isn’t just about the quality of the AI technology.”  “This is not an easily isolated problem”  Deep learning isn’t the only AI technique, and companies are already exploring alternatives. Though techniques are closely guarded within the industry (just look at Waymo’s recent lawsuit against Uber), many companies have shifted to rule-based AI, an older technique that lets engineers hard-code specific behaviors or logic into an otherwise self-directed system. It doesn’t have the same capacity to write its own behaviors just by studying data, which is what makes deep learning so exciting, but it would let companies avoid some of the deep learning’s limitations. But with the basic tasks of perception still profoundly shaped by deep learning techniques, it’s hard to say how successfully engineers can quarantine potential errors.  Ann Miura-Ko, a venture capitalist who sits on the board of Lyft, says she thinks part of the problem is high expectations for autonomous cars themselves, classifying anything less than full autonomy as a failure. “To expect them to go from zero to level five is a mismatch in expectations more than a failure of technology,” Miura-Ko says. “I see all these micro-improvements as extraordinary features on the journey towards full autonomy.”  Still, it’s not clear how long self-driving cars can stay in their current limbo. Semi-autonomous products like Tesla’s Autopilot are smart enough to handle most situations, but require human intervention if anything too unpredictable happens. When something does go wrong, it’s hard to know whether the car or the driver is to blame. For some critics, that hybrid is arguably less safe than a human driver, even if the errors are hard to blame entirely on the machine. One study by the Rand Corporation estimated that self-driving cars would have to drive 275 million miles without a fatality to prove they were as safe as human drivers. The first death linked to Tesla’s Autopilot came roughly 130 million miles into the project, well short of the mark.  But with deep learning sitting at the heart of how cars perceive objects and decide to respond, improving the accident rate may be harder than it looks. “This is not an easily isolated problem,” says Duke professor Mary Cummings, pointing to an Uber crash that killed a pedestrian earlier this year. “The perception-decision cycle is often linked, as in the case of the pedestrian death. A decision was made to do nothing based on ambiguity in perception, and the emergency braking was turned off because it got too many false alarms from the sensor”  That crash ended with Uber pausing its self-driving efforts for the summer, an ominous sign for other companies planning rollouts. Across the industry, companies are racing for more data to solve the problem, assuming the company with the most miles will build the strongest system. But where companies see a data problem, Marcus sees something much harder to solve. “They’re just using the techniques that they have in the hopes that it will work,” Marcus says. “They’re leaning on the big data because that’s the crutch that they have, but there’s no proof that ever gets you to the level of precision that we need.”","headed, companies, marcus, problem, cars, roadblock, systems, data, selfdriving, learning, ai, deep",2018-07-03 00:00:00
9,AI winter,,https://en.wikipedia.org/wiki/AI_winter#The_collapse_of_the_Lisp_machine_market_in_1987,"Period of reduced funding and interest in AI researchIn the history of artificial intelligence, an AI winter is a period of reduced funding and interest in artificial intelligence research. Roger Schank and Marvin Minsky—two leading AI researchers who experienced the ""winter"" of the 1970s—warned the business community that enthusiasm for AI had spiraled out of control in the 1980s and that disappointment would certainly follow. [6]Just like all AI booms that have been followed by desperate AI winters, the media tended to exaggerate the significance of these developments. ""[25] The result, Moravec claims, is that some of the staff at DARPA had lost patience with AI research. In 2006, Nick Bostrom explained that ""a lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.","Period of reduced funding and interest in AI research  In the history of artificial intelligence, an AI winter is a period of reduced funding and interest in artificial intelligence research.[1] The field has experienced several hype cycles, followed by disappointment and criticism, followed by funding cuts, followed by renewed interest years or even decades later.  The term first appeared in 1984 as the topic of a public debate at the annual meeting of AAAI (then called the ""American Association of Artificial Intelligence""). Roger Schank and Marvin Minsky—two leading AI researchers who experienced the ""winter"" of the 1970s—warned the business community that enthusiasm for AI had spiraled out of control in the 1980s and that disappointment would certainly follow. They described a chain reaction, similar to a ""nuclear winter"", that would begin with pessimism in the AI community, followed by pessimism in the press, followed by a severe cutback in funding, followed by the end of serious research. Three years later the billion-dollar AI industry began to collapse.  There were two major winters approximately 1974–1980 and 1987–2000,[3] and several smaller episodes, including the following:  Enthusiasm and optimism about AI has generally increased since its low point in the early 1990s. Beginning about 2012, interest in artificial intelligence (and especially the sub-field of machine learning) from the research and corporate communities led to a dramatic increase in funding and investment, leading to the current (as of 2024 ) AI boom.  Early episodes [ edit ]  Machine translation and the ALPAC report of 1966 [ edit ]  Natural language processing (NLP) research has its roots in the early 1930s and began its existence with the work on machine translation (MT).[4] However, significant advancements and applications began to emerge after the publication of Warren Weaver's influential memorandum in 1949.[5] The memorandum generated great excitement within the research community. In the following years, notable events unfolded: IBM embarked on the development of the first machine, MIT appointed its first full-time professor in machine translation, and several conferences dedicated to MT took place. The culmination came with the public demonstration of the IBM-Georgetown machine, which garnered widespread attention in respected newspapers in 1954.[6]  Just like all AI booms that have been followed by desperate AI winters, the media tended to exaggerate the significance of these developments. Headlines about the IBM-Georgetown experiment proclaimed phrases like ""The bilingual machine,"" ""Robot brain translates Russian into King's English,""[7] and ""Polyglot brainchild.""[8] However, the actual demonstration involved the translation of a curated set of only 49 Russian sentences into English, with the machine's vocabulary limited to just 250 words.[6] To put things into perspective, a 2006 study made by Paul Nation found that humans need a vocabulary of around 8,000 to 9,000-word families to comprehend written texts with 98% accuracy.[9]  During the Cold War, the US government was particularly interested in the automatic, instant translation of Russian documents and scientific reports. The government aggressively supported efforts at machine translation starting in 1954. Another factor that propelled the field of mechanical translation was the interest shown by the Central Intelligence Agency (CIA). During that period, the CIA firmly believed in the importance of developing machine translation capabilities and supported such initiatives. They also recognized that this program had implications that extended beyond the interests of the CIA and the intelligence community.[6]  At the outset, the researchers were optimistic. Noam Chomsky's new work in grammar was streamlining the translation process and there were ""many predictions of imminent 'breakthroughs'"".[10]  Briefing for US Vice President Gerald Ford in 1973 on the junction-grammar-based computer translation model  However, researchers had underestimated the profound difficulty of word-sense disambiguation. In order to translate a sentence, a machine needed to have some idea what the sentence was about, otherwise it made mistakes. An apocryphal[11] example is ""the spirit is willing but the flesh is weak."" Translated back and forth with Russian, it became ""the vodka is good but the meat is rotten."" Later researchers would call this the commonsense knowledge problem.  By 1964, the National Research Council had become concerned about the lack of progress and formed the Automatic Language Processing Advisory Committee (ALPAC) to look into the problem. They concluded, in a famous 1966 report, that machine translation was more expensive, less accurate and slower than human translation. After spending some 20 million dollars, the NRC ended all support. Careers were destroyed and research ended.[10]  Machine translation shared the same path with NLP from the rule-based approaches through the statistical approaches up to the neural network approaches, which have in 2023 culminated in large language models.  The failure of single-layer neural networks in 1969 [ edit ]  Simple networks or circuits of connected units, including Walter Pitts and Warren McCulloch's neural network for logic and Marvin Minsky's SNARC system, have failed to deliver the promised results and were abandoned in the late 1950s. Following the success of programs such as the Logic Theorist and the General Problem Solver,[13] algorithms for manipulating symbols seemed more promising at the time as means to achieve logical reasoning viewed at the time as the essence of intelligence, either natural or artificial.  Interest in perceptrons, invented by Frank Rosenblatt, was kept alive only by the sheer force of his personality.[14] He optimistically predicted that the perceptron ""may eventually be able to learn, make decisions, and translate languages"".[15] Mainstream research into perceptrons ended partially because the 1969 book Perceptrons by Marvin Minsky and Seymour Papert emphasized the limits of what perceptrons could do. While it was already known that multilayered perceptrons are not subject to the criticism, nobody in the 1960s knew how to train a multilayered perceptron. Backpropagation was still years away.[17]  Major funding for projects neural network approaches was difficult to find in the 1970s and early 1980s.[18] Important theoretical work continued despite the lack of funding. The ""winter"" of neural network approach came to an end in the middle 1980s, when the work of John Hopfield, David Rumelhart and others revived large scale interest.[19] Rosenblatt did not live to see this, however, as he died in a boating accident shortly after Perceptrons was published.[15]  The setbacks of 1974 [ edit ]  The Lighthill report [ edit ]  In 1973, professor Sir James Lighthill was asked by the UK Parliament to evaluate the state of AI research in the United Kingdom. His report, now called the Lighthill report, criticized the utter failure of AI to achieve its ""grandiose objectives"". He concluded that nothing being done in AI could not be done in other sciences. He specifically mentioned the problem of ""combinatorial explosion"" or ""intractability"", which implied that many of AI's most successful algorithms would grind to a halt on real world problems and were only suitable for solving ""toy"" versions.[20]  The report was contested in a debate broadcast in the BBC ""Controversy"" series in 1973. The debate ""The general purpose robot is a mirage"" from the Royal Institution was Lighthill versus the team of Donald Michie, John McCarthy and Richard Gregory.[21] McCarthy later wrote that ""the combinatorial explosion problem has been recognized in AI from the beginning"".[22]  The report led to the complete dismantling of AI research in the UK.[20] AI research continued in only a few universities (Edinburgh, Essex and Sussex). Research would not revive on a large scale until 1983, when Alvey (a research project of the British Government) began to fund AI again from a war chest of £350 million in response to the Japanese Fifth Generation Project (see below). Alvey had a number of UK-only requirements which did not sit well internationally, especially with US partners, and lost Phase 2 funding.  DARPA's early 1970s funding cuts [ edit ]  During the 1960s, the Defense Advanced Research Projects Agency (then known as ""ARPA"", now known as ""DARPA"") provided millions of dollars for AI research with few strings attached. J. C. R. Licklider, the founding director of DARPA's computing division, believed in ""funding people, not projects""[23] and he and several successors allowed AI's leaders (such as Marvin Minsky, John McCarthy, Herbert A. Simon or Allen Newell) to spend it almost any way they liked.  This attitude changed after the passage of Mansfield Amendment in 1969, which required DARPA to fund ""mission-oriented direct research, rather than basic undirected research"".[24] Pure undirected research of the kind that had gone on in the 1960s would no longer be funded by DARPA. Researchers now had to show that their work would soon produce some useful military technology. AI research proposals were held to a very high standard. The situation was not helped when the Lighthill report and DARPA's own study (the American Study Group) suggested that most AI research was unlikely to produce anything truly useful in the foreseeable future. DARPA's money was directed at specific projects with identifiable goals, such as autonomous tanks and battle management systems. By 1974, funding for AI projects was hard to find.[24]  AI researcher Hans Moravec blamed the crisis on the unrealistic predictions of his colleagues: ""Many researchers were caught up in a web of increasing exaggeration. Their initial promises to DARPA had been much too optimistic. Of course, what they delivered stopped considerably short of that. But they felt they couldn't in their next proposal promise less than in the first one, so they promised more.""[25] The result, Moravec claims, is that some of the staff at DARPA had lost patience with AI research. ""It was literally phrased at DARPA that 'some of these people were going to be taught a lesson [by] having their two-million-dollar-a-year contracts cut to almost nothing!'"" Moravec told Daniel Crevier.[26]  While the autonomous tank project was a failure, the battle management system (the Dynamic Analysis and Replanning Tool) proved to be enormously successful, saving billions in the first Gulf War, repaying all of DARPAs investment in AI[27] and justifying DARPA's pragmatic policy.[28]  The SUR debacle [ edit ]  As described in:[29]  In 1971, the Defense Advanced Research Projects Agency (DARPA) began an ambitious five-year experiment in speech understanding. The goals of the project were to provide recognition of utterances from a limited vocabulary in near-real time. Three organizations finally demonstrated systems at the conclusion of the project in 1976. These were Carnegie-Mellon University (CMU), who actually demonstrated two system [HEARSAY-II and HARPY]; Bolt, Beranek and Newman (BBN); and System Development Corporation with Stanford Research Institute (SDC/SRI)  The system that came closest to satisfying the original project goals was the CMU HARPY system. The relatively high performance of the HARPY system was largely achieved through 'hard-wiring' information about possible utterances into the system's knowledge base. Although HARPY made some interesting contributions, its dependence on extensive pre-knowledge limited the applicability of the approach to other signal-understanding tasks.  DARPA was deeply disappointed with researchers working on the Speech Understanding Research program at Carnegie Mellon University. DARPA had hoped for, and felt it had been promised, a system that could respond to voice commands from a pilot. The SUR team had developed a system which could recognize spoken English, but only if the words were spoken in a particular order. DARPA felt it had been duped and, in 1974, they cancelled a three million dollar a year contract.[30]  Many years later, several successful commercial speech recognition systems would use the technology developed by the Carnegie Mellon team (such as hidden Markov models) and the market for speech recognition systems would reach $4 billion by 2001.[31]  For a description of Hearsay-II see Hearsay-II, The Hearsay-II Speech Understanding System: Integrating Knowledge to Resolve Uncertainty and A Retrospective View of the Hearsay-II Architecture which appear in Blackboard Systems[32]  Reddy gives a review of progress in speech understanding at the end of the DARPA project in a 1976 article in Proceedings of the IEEE.[33]  Contrary view [ edit ]  Thomas Haigh argues that activity in the domain of AI did not slow down, even as funding from DoD was being redirected, mostly in the wake of congressional legislation meant to separate military and academic activities.[34] That indeed professional interest was growing throughout the 70s. Using the membership count of ACM's SIGART, the Special Interest Group on Artificial Intelligence, as a proxy for interest in the subject, the author writes:[34]  (...) I located two data sources, neither of which supports the idea of a broadly based AI winter during the 1970s. One is membership of ACM's SIGART, the major venue for sharing news and research abstracts during the 1970s. When the Lighthill report was published in 1973 the fast-growing group had 1,241 members, approximately twice the level in 1969. The next five years are conventionally thought of as the darkest part of the first AI winter. Was the AI community shrinking? No! By mid-1978 SIGART membership had almost tripled, to 3,500. Not only was the group growing faster than ever, it was increasing proportionally faster than ACM as a whole which had begun to plateau (expanding by less than 50% over the entire period from 1969 to 1978). One in every 11 ACM members was in SIGART.  The setbacks of the late 1980s and early 1990s [ edit ]  The collapse of the LISP machine market [ edit ]  In the 1980s, a form of AI program called an ""expert system"" was adopted by corporations around the world. The first commercial expert system was XCON, developed at Carnegie Mellon for Digital Equipment Corporation, and it was an enormous success: it was estimated to have saved the company 40 million dollars over just six years of operation. Corporations around the world began to develop and deploy expert systems and by 1985 they were spending over a billion dollars on AI, most of it to in-house AI departments. An industry grew up to support them, including software companies like Teknowledge and Intellicorp (KEE), and hardware companies like Symbolics and LISP Machines Inc. who built specialized computers, called LISP machines, that were optimized to process the programming language LISP, the preferred language for AI research in the USA.[35][36]  In 1987, three years after Minsky and Schank's prediction, the market for specialized LISP-based AI hardware collapsed. Workstations by companies like Sun Microsystems offered a powerful alternative to LISP machines and companies like Lucid offered a LISP environment for this new class of workstations. The performance of these general workstations became an increasingly difficult challenge for LISP Machines. Companies like Lucid and Franz LISP offered increasingly powerful versions of LISP that were portable to all UNIX systems. For example, benchmarks were published showing workstations maintaining a performance advantage over LISP machines.[37] Later desktop computers built by Apple and IBM would also offer a simpler and more popular architecture to run LISP applications on. By 1987, some of them had become as powerful as the more expensive LISP machines. The desktop computers had rule-based engines such as CLIPS available.[38] These alternatives left consumers with no reason to buy an expensive machine specialized for running LISP. An entire industry worth half a billion dollars was replaced in a single year.[39]  By the early 1990s, most commercial LISP companies had failed, including Symbolics, LISP Machines Inc., Lucid Inc., etc. Other companies, like Texas Instruments and Xerox, abandoned the field. A small number of customer companies (that is, companies using systems written in LISP and developed on LISP machine platforms) continued to maintain systems. In some cases, this maintenance involved the assumption of the resulting support work.[40]  Slowdown in deployment of expert systems [ edit ]  By the early 1990s, the earliest successful expert systems, such as XCON, proved too expensive to maintain. They were difficult to update, they could not learn, they were ""brittle"" (i.e., they could make grotesque mistakes when given unusual inputs), and they fell prey to problems (such as the qualification problem) that had been identified years earlier in research in nonmonotonic logic. Expert systems proved useful, but only in a few special contexts.[41][42] Another problem dealt with the computational hardness of truth maintenance efforts for general knowledge. KEE used an assumption-based approach supporting multiple-world scenarios that was difficult to understand and apply.  The few remaining expert system shell companies were eventually forced to downsize and search for new markets and software paradigms, like case-based reasoning or universal database access. The maturation of Common Lisp saved many systems such as ICAD which found application in knowledge-based engineering. Other systems, such as Intellicorp's KEE, moved from LISP to a C++ (variant) on the PC and helped establish object-oriented technology (including providing major support for the development of UML (see UML Partners).  The end of the Fifth Generation project [ edit ]  In 1981, the Japanese Ministry of International Trade and Industry set aside $850 million for the Fifth Generation computer project. Their objectives were to write programs and build machines that could carry on conversations, translate languages, interpret pictures, and reason like human beings. By 1991, the impressive list of goals penned in 1981 had not been met. According to HP Newquist in The Brain Makers, ""On June 1, 1992, The Fifth Generation Project ended not with a successful roar, but with a whimper.""[40] As with other AI projects, expectations had run much higher than what was actually possible.[43][44]  Strategic Computing Initiative cutbacks [ edit ]  In 1983, in response to the fifth generation project, DARPA again began to fund AI research through the Strategic Computing Initiative. As originally proposed the project would begin with practical, achievable goals, which even included artificial general intelligence as long-term objective. The program was under the direction of the Information Processing Technology Office (IPTO) and was also directed at supercomputing and microelectronics. By 1985 it had spent $100 million and 92 projects were underway at 60 institutions, half in industry, half in universities and government labs. AI research was well-funded by the SCI.[45]  Jack Schwarz, who ascended to the leadership of IPTO in 1987, dismissed expert systems as ""clever programming"" and cut funding to AI ""deeply and brutally"", ""eviscerating"" SCI. Schwarz felt that DARPA should focus its funding only on those technologies which showed the most promise, in his words, DARPA should ""surf"", rather than ""dog paddle"", and he felt strongly AI was not ""the next wave"". Insiders in the program cited problems in communication, organization and integration. A few projects survived the funding cuts, including pilot's assistant and an autonomous land vehicle (which were never delivered) and the DART battle management system, which (as noted above) was successful.[46]  AI winter of the 1990's and early 2000's [ edit ]  A survey of reports from the early 2000's suggests that AI's reputation was still poor:  Alex Castro, quoted in The Economist , 7 June 2007: ""[Investors] were put off by the term 'voice recognition' which, like 'artificial intelligence', is associated with systems that have all too often failed to live up to their promises."" [ 47 ]  , 7 June 2007: ""[Investors] were put off by the term 'voice recognition' which, like 'artificial intelligence', is associated with systems that have all too often failed to live up to their promises."" Patty Tascarella in Pittsburgh Business Times , 2006: ""Some believe the word 'robotics' actually carries a stigma that hurts a company's chances at funding."" [ 48 ]  , 2006: ""Some believe the word 'robotics' actually carries a stigma that hurts a company's chances at funding."" John Markoff in the New York Times, 2005: ""At its low point, some computer scientists and software engineers avoided the term artificial intelligence for fear of being viewed as wild-eyed dreamers."" [ 49 ]  Many researchers in AI in the mid 2000's deliberately called their work by other names, such as informatics, machine learning, analytics, knowledge-based systems, business rules management, cognitive systems, intelligent systems, intelligent agents or computational intelligence, to indicate that their work emphasizes particular tools or is directed at a particular sub-problem. Although this may be partly because they consider their field to be fundamentally different from AI, it is also true that the new names help to procure funding by avoiding the stigma of false promises attached to the name ""artificial intelligence"".[49][50]  In the late 1990's and early 21st century, AI technology became widely used as elements of larger systems,[51] but the field is rarely credited for these successes. In 2006, Nick Bostrom explained that ""a lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.""[53] Rodney Brooks stated around the same time that ""there's this stupid myth out there that AI has failed, but AI is around you every second of the day.""  Current AI spring (2022–present) [ edit ]  AI has reached the highest levels of interest and funding in its history in the past few years by every possible measure, including: publications, patent applications,[56] total investment ($50 billion in 2022), and job openings (800,000 U.S. job openings in 2022). The successes of the current ""AI spring"" or ""AI boom"" are advances in language translation (in particular, Google Translate), image recognition (spurred by the ImageNet training database) as commercialized by Google Image Search, and in game-playing systems such as AlphaZero (chess champion) and AlphaGo (go champion), and Watson (Jeopardy champion). A turning point was in 2012 when AlexNet (a deep learning network) won the ImageNet Large Scale Visual Recognition Challenge with half as many errors as the second place winner.  The 2022 release of OpenAI's AI chatbot ChatGPT which as of January 2023 has over 100 million users,[60] has reinvigorated the discussion about artificial intelligence and its effects on the world.[61][62]  See also [ edit ]  Notes [ edit ]  References [ edit ]","research, translation, lisp, winter, funding, system, systems, intelligence, machine, ai, edit",
10,Smells a little bit like AI winter?,Gary Marcus,https://garymarcus.substack.com/p/smells-a-little-bit-like-ai-winter,"A lot is going wrong all at once. perhaps the product might get paused? TodayCould a simultaneous implosion lead to an AI winter? Someone said to me on Twitter, these are three separate issues. No, they are not:Will be interesting to see what happens next.","A lot is going wrong all at once. Not sure it is coincidence; my flight is about to take off but a quick note, on three major bits of AI news:  Tesla FSD recall, today  Google stock drop after Bard fiasco, last week  Microsoft must be having a code red over the Bing meltdowns, especially with the bizarre conversation between Bing and Kevin Roose at NYT. perhaps the product might get paused? Today  Could a simultaneous implosion lead to an AI winter?  Someone said to me on Twitter, these are three separate issues. No, they are not:  Will be interesting to see what happens next.","little, sure, stock, winter, wrong, wintersomeone, todaycould, weekmicrosoft, twitter, todaygoogle, bit, smells, bing, ai",
11,AI Winter Isn’t Coming,Will Knight,https://www.technologyreview.com/s/603062/ai-winter-isnt-coming/,"Andrew Ng, chief scientist at Baidu Research, and a major figure in the field of machine learning and AI, says improvements in computer processor design will keep performance advances and breakthroughs coming for the foreseeable future. The advances seen in recent years have come thanks to the development of powerful “deep learning” systems (see “10 Breakthrough Technologies 2013: Deep Learning”). This might not only increase the accuracy of existing deep learning tools, but also allow the technique to be leveraged in new areas, such as parsing and generating language. What’s more, Ng says, hardware advances will provide the fuel required to make emerging AI techniques feasible. The world’s leading AI experts convened in Barcelona this week for a prominent event called the Neural Information Processing Systems conference.","Andrew Ng, chief scientist at Baidu Research, and a major figure in the field of machine learning and AI, says improvements in computer processor design will keep performance advances and breakthroughs coming for the foreseeable future. “Multiple [hardware vendors] have been kind enough to share their roadmaps,” Ng says. “I feel very confident that they are credible and we will get more computational power and faster networks in the next several years.”  The field of AI has gone through phases of rapid progress and hype in the past, quickly followed by a cooling in investment and interest, often referred to as “AI winters.” The first chill occurred in the 1970s, as progress slowed and government funding dried up; another struck in the 1980s as the latest trends failed to have the expected commercial impact.  Then again, there’s perhaps been no boom to match the current one, propelled by rapid progress in training machines to do useful tasks. Artificial intelligence researchers are now offered huge wages to perform fundamental research, as companies build research teams on the assumption that commercially important breakthroughs will follow.  Andrew Ng, chief scientist at Baidu Research.  The advances seen in recent years have come thanks to the development of powerful “deep learning” systems (see “10 Breakthrough Technologies 2013: Deep Learning”). Starting a few years ago, researchers found that very large, or deep, neural networks could be trained, using labeled examples, to recognize all sorts of things with human-like accuracy. This has led to stunning advances in image and voice recognition and elsewhere.  Ng says these systems will only become more powerful. This might not only increase the accuracy of existing deep learning tools, but also allow the technique to be leveraged in new areas, such as parsing and generating language.  What’s more, Ng says, hardware advances will provide the fuel required to make emerging AI techniques feasible.  “There are multiple experiments I’d love to run if only we had a 10-x increase in performance,” Ng adds. For instance, he says, instead of having various different image-processing algorithms, greater computer power might make it possible to build a single algorithm capable of doing all sorts of image-related tasks.  The world’s leading AI experts convened in Barcelona this week for a prominent event called the Neural Information Processing Systems conference. The scale of the gathering, which has grown from several hundred people a few years ago to more than 6,000 this year, offers some sense of the huge interest there is in artificial intelligence.","research, coming, winter, scientist, progress, ai, advances, systems, researchers, learning, sorts, isnt, deep",
12,AI winter - update,,https://blog.piekniewski.info/2018/10/29/ai-winter-update/,"IntroductionAlmost six months ago (May 28th 2018) I posted the ""AI winter is well on its way"" post that went viral. First of all a bit of clarification: some readers have misinterpreted my claims, in that I predicted that the AI hype is declining. Andrew Ng is a rare example of a person who jumped from an academic bubble into an even bigger AI hype bubble. I'm pretty certain that following Hotz's lead, many of today's AI hype blowers will be screaming how they've been warning about AI winter all along, once the bubble bursts. Musk reiterated that he believes in the self driving Tesla fleet however full self driving remains ""off menu"" as it was too confusing (two years after introduction of the item?!","Introduction  Almost six months ago (May 28th 2018) I posted the ""AI winter is well on its way"" post that went viral. The post amassed nearly a quarter million views and got picked up in Bloomberg, Forbes, Politico, Venturebeat, BBC, Datascience Podcast and numerous other smaller media outlets and blogs [1, 2, 3, 4, ...], triggered violent debate on Hacker news and Reddit. I could not have anticipated this post to be so successful and hence I realized I touched on a very sensitive subject. One can agree with my claims or not, but the sheer popularity of the post almost itself serves as a proof that something is going on behind the scenes and people are actually curious and doubtful if there is anything solid behind the AI hype.  Since the post made a prediction, that the AI hype is cracking (particularly in the space of autonomous vehicles) and as a result we will have another ""AI winter"" episode, I decided to periodically go over those claims, see what has changed and bring some new evidence.  First of all a bit of clarification: some readers have misinterpreted my claims, in that I predicted that the AI hype is declining. In fact to the contrary, I started my original post by stating that on the surface still everything looks great. And that is still the case. The NIPS conference sold out in a matter of minutes, surprising even the biggest enthusiasts. In fact some known researchers have apparently missed out and will not be going (probably the most interesting part of this conference this year will be the whole drama with its name anyway). This does not contradict anything, hype is a lagging indicator of what is actually going on. I discussed that, as well as other immediate feedback in ""AI winter addendum"". Let's review what happened over that last several months.  Updates  The twitter activity of many of the prominent researchers I follow was rather sparse over the period in question. Dr Fei Fei Li gave testimony in front of congress, with a highlight in the tweet below (and a hilarious reply):  I'm not sure how long people can go for spreading such utter nonsense without any harm to their reputation. I suppose for quite a while, but hopefully not forever.  Andrew Ng, one of my favorite ""enthusiasts"" is busy building his swarm of startup companies, but luckily found the time to give a short interview to Fortune, in which he gloats about how he singlehandedly transformed Google and Baidu into AI companies. Andrew Ng is a rare example of a person who jumped from an academic bubble into an even bigger AI hype bubble. He combines certain amount of contagious enthusiasm and a fair amount of arrogance which apparently appeals to many young people today. His recent gig is about industrial inspection. In one video he presented an example in which a deep learning system recognizes a faulty solder pad. I actually worked for a company specializing in visual inspection of electronics, so I know rather well what is the state of the art. The machines routinely scan large PCB's and within fractions of a second create a full 3d reconstruction of the inspected part, analyze each solder joint for imperfections and a set of well defined defects, create interpretable scores which could be used for binning and so on. In face of that, the little video from Andrew seemed rather comical and if anything indicated Dunning-Kruger effect. Now I'm not saying that deep learning could not improve this industry in some way, but the entry level is set pretty high. And same is true for many other industries that rely on very well optimized and often elegant algorithmic solutions.  Now that we are on the subject of colorful characters in the AI scene, another one of my favorites, George Hotz from Comma.ai announced he is resigning from the role of CEO at comma. Hotz, known among other things for hacking playstation, made some ripples in the industry in 2015 when he teased Elon Musk, that he could build a self-driving-car system for Tesla, and he could do it much better and cheaper than Mobileye, which was the company that Tesla used at that time. Not too long after that, a first fatal autopilot accident happened (Joshua Brown) and Mobileye dropped the relationship with Tesla, apparently noting that their use of technology was irresponsible. Anyway, back then Hotz was confident he could build a self driving car in his garage with some old phone parts and deep learning magic. Since then his narrative has changed by some 180 degrees, and now George is on ""crusade against the 'scam' of self-driving cars""... I guess I could leave this one without a comment, though Picard's face palm meme would be in order here. I'm pretty certain that following Hotz's lead, many of today's AI hype blowers will be screaming how they've been warning about AI winter all along, once the bubble bursts.  Speaking of Tesla, as of yet the long promised autonomous coast-to-coast drive has not happened. Earlier this year Elon Musk - CEO of Tesla, stated that the drive would certainly happen by the summer 2018, but on August 1 2018 conference call he indicated that the coast-to-coast drive is actually not a priority anymore and will get delayed (no new timeframe was given). This is quite a change, particularly in the light of the conference call on which the so called full self driving feature was announced almost exactly two years ago. By August, first ""autonomous features"" were supposed to be unrolled in Tesla's via over the air update, but no such thing happened (noteworthy, in the beginning of 2018 Musk was rather confident that full self driving will noticeably depart from advanced autopilot by summer). Instead the new autopilot v9 can sometimes switch lanes but by many commentators, the new system actually requires even more attention than the previous one and still is buggy [1] and dangerous. In fact it is not uncommon to hear opinions that even the current Tesla autopilot with respect to stability and reliability barely approaches the Mobileye solution from several years ago. On October 18'th the full-self-driving (FSD) option disappeared from all Tesla models configurations, without much explanation other than that it was ""confusing"". At the time of writing this article it is not clear if the option will be coming back or if those who already payed for it will get a refund. It is still not clear to me if Elon Musk fell a victim of the hype himself or did he deliberately blew the AI bubble, but he certainly contributed substantially to the AI mania between 2015 and now, especially with that egregious stunt with Stephen Hawking.  I should add here perhaps, that I actually think Tesla autopilot is a pretty impressive piece of engineering, just not anywhere near being safely deployable beyond as fancy cruise control. The August Tesla quarterly conference call was also interesting in that a lot of time on the call was spent on the new hardware that Tesla was building that is supposed to offer an order of magnitude improvement over the current nVidia drive PX 2 system, and that this new system will finally allow for full autonomy. I remain skeptical, since having an order of magnitude more compute capabilities than drive PX is nothing extraordinary, a rig of two GTX 1080Ti's will accomplish it without any problem, and this is what many companies are putting on their test vehicles along with expensive lidars etc, and yet the full autonomy remains elusive. Secondly, it seems that cars which Tesla had sold as ""full self-driving compatible"" will require a computer swap, which will not be cheap. One can get at least some feel of what stage is Tesla at with respect to autonomy by taking a look at the slides (from May 10, 2018) of Andrej Karpathy, director of AI there. Half way through the presentation he realizes that the driving reality is full of corner cases, which are not only hard for neural nets, but are even not obvious to label by a human. The other half of his presentation is spent on what he calls software 2.0. It is a concept in which computers are no longer programmed but are trained. This reminds me a lot of the hype in the 80's, the fifth generation computers which were supposed to program themselves based on high level logical specification written in Prolog or similar functional language. This hype cycle was closely related to the expert system mania, which collapsed by the end of 80's causing an AI winter episode. I'm pretty certain this software 2.0 nonsense will share the same fate.  To conclude the Tesla case, the most recent Q3 conference call for Tesla which took place on October 24th did not bring much resolution to the above uncertainties. Musk reiterated that he believes in the self driving Tesla fleet however full self driving remains ""off menu"" as it was too confusing (two years after introduction of the item?!), Karpathy quickly mentioned that new hardware will support bigger neural networks which work ""very good"" and the new version of Autopilot will allow to navigate on the freeway, with the restriction that lane changes will require confirmation from the driver (read, any incidents will be blamed on the driver). No word about the coast to coast drive. No timeline on full self driving.  While on the self-driving car subject, one of the main criticisms of my original AI winter post was that I omitted Waymo from my discussion, them being the unquestionable leader in autonomy. This criticism was a bit unjustified in that I did include and discussed Waymo extensively in my other posts [1], but in these circumstances it appears prudent to mention what is going on there. Luckily a recent very good piece of investigative journalism shines some light on the matter. Apparently Waymo cars tested in Phoenix area had trouble with the most basic driving situations such as merging onto a freeway or making a left turn, [1]. The piece worth citing from the article:  There are times when it seems “autonomy is around the corner,” and the vehicle can go for a day without a human driver intervening, said a person familiar with Waymo. Other days reality sets in because “the edge cases are endless.”  Some independent observations appear to confirm this assessment. As much as I agree that Waymo is probably the most advanced in this game, this does not mean they are anywhere near to actually deploying anything seriously, and even further away from making such deployment economically feasible (contrary to what is suggested in occasional puff pieces such as this one). Aside from a periodic PR nonsense, Waymo does not seem to be revealing much, though recently some baffling reports of past shenanigans in google chauffeur (which later became Waymo) surfaced, involving Anthony Levandowski who is responsible for the whole Uber-Waymo fiasco. To add some comical aspect to the Waymo-Uber story, apparently an unrelated engineer managed to invalidate the patent that Uber got sued over, spending altogether 6000 dollars in fees. This is probably how much Uber payed their patent attorneys for a minute of their work...  Speaking of Uber they substantially slowed their self-driving program, practically killed their self driving truck program (same one that delivered a few crates of beer in Colorado in 2016 with great fanfares, a demo that later on turned out to be completely staged), and recent rumors indicate they might be even looking to sell the unit.  Generally the other self driving car projects are facing increasing headwinds, with some projects already getting shut down by the government agencies, and others going more low-key with respect to public announcements. Particularly interesting news came recently out of Cruise, the second in the race right after Waymo (at least according to California disengagement data). Some noteworthy bits from the Reuters article:  Those expectations are now hitting speed bumps, according to interviews with eight current and former GM and Cruise employees and executives, along with nine autonomous vehicle technology experts familiar with Cruise. These sources say that some unexpected technical challenges - including the difficulty that Cruise cars have identifying whether objects are in motion - mean putting GM’s driverless cars on the road in a large scale way in 2019 is looking highly unlikely. “Nothing is on schedule,” said one GM source, referring to certain mileage targets and other milestones the company has already missed.  And a few paragraphs further:  “Everyone in the industry is becoming more and more nervous that they will waste billions of dollars,” said Klaus Froehlich, a board member at BMW and its head of research and development.  The future of self driving cars is getting more and more uncertain, in perfect agreement with my original thesis expressed in 2016 [see also here].  Briefly on other players on the AI scene: DeepMind was rather quiet (last time I checked Montezuma's revenge remained unsolved for AI in the general see update below), but OpenAI had a small PR offensive with their DotA 2 playing agent. After the initial tournament in which the system won, it quickly became apparent that the game was in many ways restricted in favor of the computer. Hence another tournament was organized, in which most restrictions were lifted, and the tournament was ... spectacularly lost to humans... Bummer after OpenAI spent obscene amounts of money training their agents. Now I could not care less about results in game domains, since as I stated multiple times on this blog [1, 2], the only problem really worth solving in AI is the Moravec's paradox, which is exactly the opposite of what DeepMind or OpenAI are doing, but I nevertheless found this media misfire hilarious.  While touching on Moravec's paradox, one of the handful of companies that actually tried to move robotics forward, Rethink Robotics, shut down its operation. This shows that making robots do anything beyond what they already do very well in controlled factory production lines is not only difficult technically but also poses a challenging business case, even with the experience of Rodney Brooks. Unlike most other startups in this field, whose main asset is the ego of their founder fueled by some cheap VC money, Rethink actually accomplished many impressive technical achievements and the news saddened me quite a bit. Robotics will need to be rethought again, likely many times over.  Finally, as an additional indication of the changing sentiment, one can cite this tweet from François Chollet, author of Keras Deep learning framework, certainly a person in the know:  Today more people are working on deep learning than ever before -- around two orders of magnitude more than in 2014. And the rate of progress as I see it is the slowest in 5 years. Time for something new  This tweet at the time of writing of this post has been retweeted nearly 350 times and liked >1300 times.  Conclusion  So there you go: the state of AI towards the end of 2018 - it is borderline comical (hence Krusty the clown in the title image). Certain things have changed however, some of the smoke dissipated and some of the mirrors cracked. Since my original post, a lot more mainstream media articles have shown up, in which the reporters are at least willing to exercise a possibility that we are on top of a giant AI bubble that is already letting the air out [e.g. this one and many others cited in the text above]. This spike of skepticism is a natural next step in the inevitable disillusionment, but I think it will take a while before this bubble finally deflates. The next 6 months are likely to be particularly interesting in this AI circus.  Update [Nov 1, 2018]: Apparently openAI actually beat Deepmind to solving Montezuma's revenge by instead of rewarding for winning the game, rewarding for avoiding predictable states. This is very interesting since it highlights how much predicting the state of the world is important, a core topic of this blog and Predictive Vision Model [1,2,3,4].  Update 2 [Jan 9, 2019]: Melanie Mitchell pointed out that Uber AI lab also solved Montezuma's revenge, it seems however they were a few weeks late after OpenAI. Nevertheless that settles it for good, we solved Montezuma's revenge, Yay!  If you found an error, highlight it and press Shift + Enter or click here to inform us.  Related  Comments  comments","driving, winter, hype, 2018, update, post, actually, self, waymo, system, tesla, ai",2018-10-29 00:00:00
13,The AGI hype train is running out of steam,"Thomas Macaulay, Story By",https://thenextweb.com/news/agi-hype-fading-artificial-general-intelligence-analysisi-ai-winter,"The AGI hype train has hit some heavy traffic. Peter Thiel — the tech billionaire and rumored vampire — says Silicon Valley big brains have lost enthusiasm for AGI. Thiel described Musk as “a weathervane for the zeitgeist,” who’s stopped talking about AGI because interest has declined. Critics fear that another AI winter is coming. Hyping AGI helped spurred enormous investment in artificial intelligence, but overconfident predictions could be detrimental to progress in the field — if the real-world advances prove disappointing.","The AGI hype train has hit some heavy traffic.  While futurists and fundraisers used to make bullish predictions about artificial general intelligence, they’ve become quieter lately. Peter Thiel — the tech billionaire and rumored vampire — says Silicon Valley big brains have lost enthusiasm for AGI.  “Elon’s not talking about it anymore and Larry [Page] is off to Fiji and doesn’t seem to be working on it quite as hard,” Thiel said at a recent event.  Thiel described Musk as “a weathervane for the zeitgeist,” who’s stopped talking about AGI because interest has declined.  Scientists are also increasingly skeptical. A recent study paper posited that AGI is “in principle impossible,” while other researchers have mocked the term’s proponents.  “I have yet to come across work on AGI that I can take seriously,” tweeted Abeba Birhane, a cognitive scientist based at University College Dublin.  I have yet to come across work on AGI that I can take seriously pic.twitter.com/Gjs1QM8BW2 — Abeba Birhane (@Abebab) November 14, 2021  The path towards AGI increasingly appears — at best — a long one.  Imbuing machines with human-like intelligence remains immensely challenging. As Melanie Mitchell, a computer science professor at Portland State University noted in a preprint paper published last year on arXiv:  Since its beginning in the 1950s, the field of artificial intelligence has cycled several times between periods of optimistic predictions and massive investment (‘AI spring’) and periods of disappointment, loss of confidence, and reduced funding (‘AI winter’). Even with today’s seemingly fast pace of AI breakthroughs, the development of long-promised technologies such as self-driving cars, housekeeping robots, and conversational companions has turned out to be much harder than many people expected. One reason for these repeating cycles is our limited understanding of the nature and complexity of intelligence itself.  Critics fear that another AI winter is coming. Hyping AGI helped spurred enormous investment in artificial intelligence, but overconfident predictions could be detrimental to progress in the field — if the real-world advances prove disappointing.","talking, work, running, artificial, steam, winter, train, hype, university, predictions, thiel, intelligence, ai, agi",2021-11-15 23:18:13+00:00
14,AI winter,,https://en.wikipedia.org/wiki/AI_winter,"Period of reduced funding and interest in AI researchIn the history of artificial intelligence, an AI winter is a period of reduced funding and interest in artificial intelligence research. Roger Schank and Marvin Minsky—two leading AI researchers who experienced the ""winter"" of the 1970s—warned the business community that enthusiasm for AI had spiraled out of control in the 1980s and that disappointment would certainly follow. [6]Just like all AI booms that have been followed by desperate AI winters, the media tended to exaggerate the significance of these developments. ""[25] The result, Moravec claims, is that some of the staff at DARPA had lost patience with AI research. In 2006, Nick Bostrom explained that ""a lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.","Period of reduced funding and interest in AI research  In the history of artificial intelligence, an AI winter is a period of reduced funding and interest in artificial intelligence research.[1] The field has experienced several hype cycles, followed by disappointment and criticism, followed by funding cuts, followed by renewed interest years or even decades later.  The term first appeared in 1984 as the topic of a public debate at the annual meeting of AAAI (then called the ""American Association of Artificial Intelligence""). Roger Schank and Marvin Minsky—two leading AI researchers who experienced the ""winter"" of the 1970s—warned the business community that enthusiasm for AI had spiraled out of control in the 1980s and that disappointment would certainly follow. They described a chain reaction, similar to a ""nuclear winter"", that would begin with pessimism in the AI community, followed by pessimism in the press, followed by a severe cutback in funding, followed by the end of serious research. Three years later the billion-dollar AI industry began to collapse.  There were two major winters approximately 1974–1980 and 1987–2000,[3] and several smaller episodes, including the following:  Enthusiasm and optimism about AI has generally increased since its low point in the early 1990s. Beginning about 2012, interest in artificial intelligence (and especially the sub-field of machine learning) from the research and corporate communities led to a dramatic increase in funding and investment, leading to the current (as of 2024 ) AI boom.  Early episodes [ edit ]  Machine translation and the ALPAC report of 1966 [ edit ]  Natural language processing (NLP) research has its roots in the early 1930s and began its existence with the work on machine translation (MT).[4] However, significant advancements and applications began to emerge after the publication of Warren Weaver's influential memorandum in 1949.[5] The memorandum generated great excitement within the research community. In the following years, notable events unfolded: IBM embarked on the development of the first machine, MIT appointed its first full-time professor in machine translation, and several conferences dedicated to MT took place. The culmination came with the public demonstration of the IBM-Georgetown machine, which garnered widespread attention in respected newspapers in 1954.[6]  Just like all AI booms that have been followed by desperate AI winters, the media tended to exaggerate the significance of these developments. Headlines about the IBM-Georgetown experiment proclaimed phrases like ""The bilingual machine,"" ""Robot brain translates Russian into King's English,""[7] and ""Polyglot brainchild.""[8] However, the actual demonstration involved the translation of a curated set of only 49 Russian sentences into English, with the machine's vocabulary limited to just 250 words.[6] To put things into perspective, a 2006 study made by Paul Nation found that humans need a vocabulary of around 8,000 to 9,000-word families to comprehend written texts with 98% accuracy.[9]  During the Cold War, the US government was particularly interested in the automatic, instant translation of Russian documents and scientific reports. The government aggressively supported efforts at machine translation starting in 1954. Another factor that propelled the field of mechanical translation was the interest shown by the Central Intelligence Agency (CIA). During that period, the CIA firmly believed in the importance of developing machine translation capabilities and supported such initiatives. They also recognized that this program had implications that extended beyond the interests of the CIA and the intelligence community.[6]  At the outset, the researchers were optimistic. Noam Chomsky's new work in grammar was streamlining the translation process and there were ""many predictions of imminent 'breakthroughs'"".[10]  Briefing for US Vice President Gerald Ford in 1973 on the junction-grammar-based computer translation model  However, researchers had underestimated the profound difficulty of word-sense disambiguation. In order to translate a sentence, a machine needed to have some idea what the sentence was about, otherwise it made mistakes. An apocryphal[11] example is ""the spirit is willing but the flesh is weak."" Translated back and forth with Russian, it became ""the vodka is good but the meat is rotten."" Later researchers would call this the commonsense knowledge problem.  By 1964, the National Research Council had become concerned about the lack of progress and formed the Automatic Language Processing Advisory Committee (ALPAC) to look into the problem. They concluded, in a famous 1966 report, that machine translation was more expensive, less accurate and slower than human translation. After spending some 20 million dollars, the NRC ended all support. Careers were destroyed and research ended.[10]  Machine translation shared the same path with NLP from the rule-based approaches through the statistical approaches up to the neural network approaches, which have in 2023 culminated in large language models.  The failure of single-layer neural networks in 1969 [ edit ]  Simple networks or circuits of connected units, including Walter Pitts and Warren McCulloch's neural network for logic and Marvin Minsky's SNARC system, have failed to deliver the promised results and were abandoned in the late 1950s. Following the success of programs such as the Logic Theorist and the General Problem Solver,[13] algorithms for manipulating symbols seemed more promising at the time as means to achieve logical reasoning viewed at the time as the essence of intelligence, either natural or artificial.  Interest in perceptrons, invented by Frank Rosenblatt, was kept alive only by the sheer force of his personality.[14] He optimistically predicted that the perceptron ""may eventually be able to learn, make decisions, and translate languages"".[15] Mainstream research into perceptrons ended partially because the 1969 book Perceptrons by Marvin Minsky and Seymour Papert emphasized the limits of what perceptrons could do. While it was already known that multilayered perceptrons are not subject to the criticism, nobody in the 1960s knew how to train a multilayered perceptron. Backpropagation was still years away.[17]  Major funding for projects neural network approaches was difficult to find in the 1970s and early 1980s.[18] Important theoretical work continued despite the lack of funding. The ""winter"" of neural network approach came to an end in the middle 1980s, when the work of John Hopfield, David Rumelhart and others revived large scale interest.[19] Rosenblatt did not live to see this, however, as he died in a boating accident shortly after Perceptrons was published.[15]  The setbacks of 1974 [ edit ]  The Lighthill report [ edit ]  In 1973, professor Sir James Lighthill was asked by the UK Parliament to evaluate the state of AI research in the United Kingdom. His report, now called the Lighthill report, criticized the utter failure of AI to achieve its ""grandiose objectives"". He concluded that nothing being done in AI could not be done in other sciences. He specifically mentioned the problem of ""combinatorial explosion"" or ""intractability"", which implied that many of AI's most successful algorithms would grind to a halt on real world problems and were only suitable for solving ""toy"" versions.[20]  The report was contested in a debate broadcast in the BBC ""Controversy"" series in 1973. The debate ""The general purpose robot is a mirage"" from the Royal Institution was Lighthill versus the team of Donald Michie, John McCarthy and Richard Gregory.[21] McCarthy later wrote that ""the combinatorial explosion problem has been recognized in AI from the beginning"".[22]  The report led to the complete dismantling of AI research in the UK.[20] AI research continued in only a few universities (Edinburgh, Essex and Sussex). Research would not revive on a large scale until 1983, when Alvey (a research project of the British Government) began to fund AI again from a war chest of £350 million in response to the Japanese Fifth Generation Project (see below). Alvey had a number of UK-only requirements which did not sit well internationally, especially with US partners, and lost Phase 2 funding.  DARPA's early 1970s funding cuts [ edit ]  During the 1960s, the Defense Advanced Research Projects Agency (then known as ""ARPA"", now known as ""DARPA"") provided millions of dollars for AI research with few strings attached. J. C. R. Licklider, the founding director of DARPA's computing division, believed in ""funding people, not projects""[23] and he and several successors allowed AI's leaders (such as Marvin Minsky, John McCarthy, Herbert A. Simon or Allen Newell) to spend it almost any way they liked.  This attitude changed after the passage of Mansfield Amendment in 1969, which required DARPA to fund ""mission-oriented direct research, rather than basic undirected research"".[24] Pure undirected research of the kind that had gone on in the 1960s would no longer be funded by DARPA. Researchers now had to show that their work would soon produce some useful military technology. AI research proposals were held to a very high standard. The situation was not helped when the Lighthill report and DARPA's own study (the American Study Group) suggested that most AI research was unlikely to produce anything truly useful in the foreseeable future. DARPA's money was directed at specific projects with identifiable goals, such as autonomous tanks and battle management systems. By 1974, funding for AI projects was hard to find.[24]  AI researcher Hans Moravec blamed the crisis on the unrealistic predictions of his colleagues: ""Many researchers were caught up in a web of increasing exaggeration. Their initial promises to DARPA had been much too optimistic. Of course, what they delivered stopped considerably short of that. But they felt they couldn't in their next proposal promise less than in the first one, so they promised more.""[25] The result, Moravec claims, is that some of the staff at DARPA had lost patience with AI research. ""It was literally phrased at DARPA that 'some of these people were going to be taught a lesson [by] having their two-million-dollar-a-year contracts cut to almost nothing!'"" Moravec told Daniel Crevier.[26]  While the autonomous tank project was a failure, the battle management system (the Dynamic Analysis and Replanning Tool) proved to be enormously successful, saving billions in the first Gulf War, repaying all of DARPAs investment in AI[27] and justifying DARPA's pragmatic policy.[28]  The SUR debacle [ edit ]  As described in:[29]  In 1971, the Defense Advanced Research Projects Agency (DARPA) began an ambitious five-year experiment in speech understanding. The goals of the project were to provide recognition of utterances from a limited vocabulary in near-real time. Three organizations finally demonstrated systems at the conclusion of the project in 1976. These were Carnegie-Mellon University (CMU), who actually demonstrated two system [HEARSAY-II and HARPY]; Bolt, Beranek and Newman (BBN); and System Development Corporation with Stanford Research Institute (SDC/SRI)  The system that came closest to satisfying the original project goals was the CMU HARPY system. The relatively high performance of the HARPY system was largely achieved through 'hard-wiring' information about possible utterances into the system's knowledge base. Although HARPY made some interesting contributions, its dependence on extensive pre-knowledge limited the applicability of the approach to other signal-understanding tasks.  DARPA was deeply disappointed with researchers working on the Speech Understanding Research program at Carnegie Mellon University. DARPA had hoped for, and felt it had been promised, a system that could respond to voice commands from a pilot. The SUR team had developed a system which could recognize spoken English, but only if the words were spoken in a particular order. DARPA felt it had been duped and, in 1974, they cancelled a three million dollar a year contract.[30]  Many years later, several successful commercial speech recognition systems would use the technology developed by the Carnegie Mellon team (such as hidden Markov models) and the market for speech recognition systems would reach $4 billion by 2001.[31]  For a description of Hearsay-II see Hearsay-II, The Hearsay-II Speech Understanding System: Integrating Knowledge to Resolve Uncertainty and A Retrospective View of the Hearsay-II Architecture which appear in Blackboard Systems[32]  Reddy gives a review of progress in speech understanding at the end of the DARPA project in a 1976 article in Proceedings of the IEEE.[33]  Contrary view [ edit ]  Thomas Haigh argues that activity in the domain of AI did not slow down, even as funding from DoD was being redirected, mostly in the wake of congressional legislation meant to separate military and academic activities.[34] That indeed professional interest was growing throughout the 70s. Using the membership count of ACM's SIGART, the Special Interest Group on Artificial Intelligence, as a proxy for interest in the subject, the author writes:[34]  (...) I located two data sources, neither of which supports the idea of a broadly based AI winter during the 1970s. One is membership of ACM's SIGART, the major venue for sharing news and research abstracts during the 1970s. When the Lighthill report was published in 1973 the fast-growing group had 1,241 members, approximately twice the level in 1969. The next five years are conventionally thought of as the darkest part of the first AI winter. Was the AI community shrinking? No! By mid-1978 SIGART membership had almost tripled, to 3,500. Not only was the group growing faster than ever, it was increasing proportionally faster than ACM as a whole which had begun to plateau (expanding by less than 50% over the entire period from 1969 to 1978). One in every 11 ACM members was in SIGART.  The setbacks of the late 1980s and early 1990s [ edit ]  The collapse of the LISP machine market [ edit ]  In the 1980s, a form of AI program called an ""expert system"" was adopted by corporations around the world. The first commercial expert system was XCON, developed at Carnegie Mellon for Digital Equipment Corporation, and it was an enormous success: it was estimated to have saved the company 40 million dollars over just six years of operation. Corporations around the world began to develop and deploy expert systems and by 1985 they were spending over a billion dollars on AI, most of it to in-house AI departments. An industry grew up to support them, including software companies like Teknowledge and Intellicorp (KEE), and hardware companies like Symbolics and LISP Machines Inc. who built specialized computers, called LISP machines, that were optimized to process the programming language LISP, the preferred language for AI research in the USA.[35][36]  In 1987, three years after Minsky and Schank's prediction, the market for specialized LISP-based AI hardware collapsed. Workstations by companies like Sun Microsystems offered a powerful alternative to LISP machines and companies like Lucid offered a LISP environment for this new class of workstations. The performance of these general workstations became an increasingly difficult challenge for LISP Machines. Companies like Lucid and Franz LISP offered increasingly powerful versions of LISP that were portable to all UNIX systems. For example, benchmarks were published showing workstations maintaining a performance advantage over LISP machines.[37] Later desktop computers built by Apple and IBM would also offer a simpler and more popular architecture to run LISP applications on. By 1987, some of them had become as powerful as the more expensive LISP machines. The desktop computers had rule-based engines such as CLIPS available.[38] These alternatives left consumers with no reason to buy an expensive machine specialized for running LISP. An entire industry worth half a billion dollars was replaced in a single year.[39]  By the early 1990s, most commercial LISP companies had failed, including Symbolics, LISP Machines Inc., Lucid Inc., etc. Other companies, like Texas Instruments and Xerox, abandoned the field. A small number of customer companies (that is, companies using systems written in LISP and developed on LISP machine platforms) continued to maintain systems. In some cases, this maintenance involved the assumption of the resulting support work.[40]  Slowdown in deployment of expert systems [ edit ]  By the early 1990s, the earliest successful expert systems, such as XCON, proved too expensive to maintain. They were difficult to update, they could not learn, they were ""brittle"" (i.e., they could make grotesque mistakes when given unusual inputs), and they fell prey to problems (such as the qualification problem) that had been identified years earlier in research in nonmonotonic logic. Expert systems proved useful, but only in a few special contexts.[41][42] Another problem dealt with the computational hardness of truth maintenance efforts for general knowledge. KEE used an assumption-based approach supporting multiple-world scenarios that was difficult to understand and apply.  The few remaining expert system shell companies were eventually forced to downsize and search for new markets and software paradigms, like case-based reasoning or universal database access. The maturation of Common Lisp saved many systems such as ICAD which found application in knowledge-based engineering. Other systems, such as Intellicorp's KEE, moved from LISP to a C++ (variant) on the PC and helped establish object-oriented technology (including providing major support for the development of UML (see UML Partners).  The end of the Fifth Generation project [ edit ]  In 1981, the Japanese Ministry of International Trade and Industry set aside $850 million for the Fifth Generation computer project. Their objectives were to write programs and build machines that could carry on conversations, translate languages, interpret pictures, and reason like human beings. By 1991, the impressive list of goals penned in 1981 had not been met. According to HP Newquist in The Brain Makers, ""On June 1, 1992, The Fifth Generation Project ended not with a successful roar, but with a whimper.""[40] As with other AI projects, expectations had run much higher than what was actually possible.[43][44]  Strategic Computing Initiative cutbacks [ edit ]  In 1983, in response to the fifth generation project, DARPA again began to fund AI research through the Strategic Computing Initiative. As originally proposed the project would begin with practical, achievable goals, which even included artificial general intelligence as long-term objective. The program was under the direction of the Information Processing Technology Office (IPTO) and was also directed at supercomputing and microelectronics. By 1985 it had spent $100 million and 92 projects were underway at 60 institutions, half in industry, half in universities and government labs. AI research was well-funded by the SCI.[45]  Jack Schwarz, who ascended to the leadership of IPTO in 1987, dismissed expert systems as ""clever programming"" and cut funding to AI ""deeply and brutally"", ""eviscerating"" SCI. Schwarz felt that DARPA should focus its funding only on those technologies which showed the most promise, in his words, DARPA should ""surf"", rather than ""dog paddle"", and he felt strongly AI was not ""the next wave"". Insiders in the program cited problems in communication, organization and integration. A few projects survived the funding cuts, including pilot's assistant and an autonomous land vehicle (which were never delivered) and the DART battle management system, which (as noted above) was successful.[46]  AI winter of the 1990's and early 2000's [ edit ]  A survey of reports from the early 2000's suggests that AI's reputation was still poor:  Alex Castro, quoted in The Economist , 7 June 2007: ""[Investors] were put off by the term 'voice recognition' which, like 'artificial intelligence', is associated with systems that have all too often failed to live up to their promises."" [ 47 ]  , 7 June 2007: ""[Investors] were put off by the term 'voice recognition' which, like 'artificial intelligence', is associated with systems that have all too often failed to live up to their promises."" Patty Tascarella in Pittsburgh Business Times , 2006: ""Some believe the word 'robotics' actually carries a stigma that hurts a company's chances at funding."" [ 48 ]  , 2006: ""Some believe the word 'robotics' actually carries a stigma that hurts a company's chances at funding."" John Markoff in the New York Times, 2005: ""At its low point, some computer scientists and software engineers avoided the term artificial intelligence for fear of being viewed as wild-eyed dreamers."" [ 49 ]  Many researchers in AI in the mid 2000's deliberately called their work by other names, such as informatics, machine learning, analytics, knowledge-based systems, business rules management, cognitive systems, intelligent systems, intelligent agents or computational intelligence, to indicate that their work emphasizes particular tools or is directed at a particular sub-problem. Although this may be partly because they consider their field to be fundamentally different from AI, it is also true that the new names help to procure funding by avoiding the stigma of false promises attached to the name ""artificial intelligence"".[49][50]  In the late 1990's and early 21st century, AI technology became widely used as elements of larger systems,[51] but the field is rarely credited for these successes. In 2006, Nick Bostrom explained that ""a lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.""[53] Rodney Brooks stated around the same time that ""there's this stupid myth out there that AI has failed, but AI is around you every second of the day.""  Current AI spring (2022–present) [ edit ]  AI has reached the highest levels of interest and funding in its history in the past few years by every possible measure, including: publications, patent applications,[56] total investment ($50 billion in 2022), and job openings (800,000 U.S. job openings in 2022). The successes of the current ""AI spring"" or ""AI boom"" are advances in language translation (in particular, Google Translate), image recognition (spurred by the ImageNet training database) as commercialized by Google Image Search, and in game-playing systems such as AlphaZero (chess champion) and AlphaGo (go champion), and Watson (Jeopardy champion). A turning point was in 2012 when AlexNet (a deep learning network) won the ImageNet Large Scale Visual Recognition Challenge with half as many errors as the second place winner.  The 2022 release of OpenAI's AI chatbot ChatGPT which as of January 2023 has over 100 million users,[60] has reinvigorated the discussion about artificial intelligence and its effects on the world.[61][62]  See also [ edit ]  Notes [ edit ]  References [ edit ]","research, translation, lisp, winter, funding, system, systems, intelligence, machine, ai, edit",
15,Discover thousands of collaborative articles on 2500+ skills,,https://www.linkedin.com/pulse/ai-winter-coming-sandro-skansi,"We can’t find the page you’re looking for. The page you’re looking for may have been moved, or may no longer exist. Try going back to the previous page or check out more articles in this collaborative article page.","We can’t find the page you’re looking for. The page you’re looking for may have been moved, or may no longer exist. Try going back to the previous page or check out more articles in this collaborative article page.","exist, try, longer, skills, articles, previous, discover, 2500, collaborative, page, going, thousands, moved, looking, youre",
16,AI Winter: Why we don’t care if they “fail” and lose money,Steve Jones,https://blog.metamirror.io/ai-winter-why-we-dont-care-if-they-fail-and-lose-money-772388321341,"AI Winter: Why we don’t care if they “fail” and lose moneyAn AI Winter for some means snowboarding for the rest of us Steve Jones · Follow 5 min read · Aug 5, 2024 -- 3 Listen ShareWill Silicon Valley companies and VC funds get a return on investment for the absolutely billions they are spending training ever bigger and bigger models? But I’ve also had another thought that:What if Venture Capital Firms and Silicon Valley behemoths losing money on creating massive models doesn’t matter to the successful adoption of AI in business? What if this is like the Apollo Space program, but more expensive , and it doesn’t “get to the moon”? Fly-by-wire business will be enoughOne of the largest impact technologies in aviation came out of the Apollo program — fly-by-wire. This in turn will mean that businesses will need to look more about the systemic management of risk with AI.","AI Winter: Why we don’t care if they “fail” and lose money  An AI Winter for some means snowboarding for the rest of us Steve Jones · Follow 5 min read · Aug 5, 2024 -- 3 Listen Share  Will Silicon Valley companies and VC funds get a return on investment for the absolutely billions they are spending training ever bigger and bigger models? Goldman Sachs aren’t sure and Sequoia Capital aren’t convinced, which I’m totally and utterly surprised by and didn’t mention a year or so ago.  We are talking huge sums of money  I’ve said recently that there is a massive problem that the whole AGI Hype is going to damage the AI opportunity and I do think that. But I’ve also had another thought that:  What if Venture Capital Firms and Silicon Valley behemoths losing money on creating massive models doesn’t matter to the successful adoption of AI in business?  What if this is like the Apollo Space program, but more expensive , and it doesn’t “get to the moon”? What if we end up with a small number of big companies (with huge cash reserves) being impacted because they can’t demonstrate the hyped returns, and by being impacted we just mean “still massively profitable, just not justifying $7bn a learn sort of profitable” would that matter? If, as many researchers have predicted, this current approach is absolutely not a route to AGI and so it never justifies the hype, and cannot justify ever greater spend for ever more marginal returns.  Would the rest of us care?  I don’t think so, and let me explain why, and why I think 1 & 3 of Gary Marcus’ prediction are positive for the rest of us, so he’s wrong about adoption.  Fly-by-wire business will be enough  One of the largest impact technologies in aviation came out of the Apollo program — fly-by-wire. A technology that placed computers in charge of control surfaces, interpreting commands from the astronauts into the actions on the surfaces, engines, etc.  This is now totally ubiquitous in military and commercial airliners. The idea of fixed cabling and direct connection has passed away, all based on a technology that was created as a side-effect of a $320bn (in today’s money) investment in getting man to the moon, and back again. There were many other advances as well, from new approaches to storing food to protecting buildings from earthquakes.  So do we care if the investments in AGI “only” result in a new “fly-by-wire” approach to business where agentic collaborations are able to be managed and directed by people, but more efficiently turn that guidance into action? If we are better able to speak to people in other countries and have content better prepared for accessibility needs? Will it be a massive problem for businesses if “all” we get out of this AGI Race is a new platform of AI technologies that can better predict the weather, better support drug research or “just” creates a new approach for human-machine interaction?  If you are making aeroplanes then the work of the Apollo program in creating fly-by-wire was an absolute boon. I think the pursuit of AGI is already doing the equivalent for a lot of business challenges and that just like the vast majority of people on the planet:  Most people don’t need to go to the moon  “And back again” — the challenge?  When JFK put the USA on the path to the moon he included a very important caveat:  “ then return it safely to Earth”  This is the one risk with our AGI Mars Shot program that the objective appears to be just get there rather than to get their safely. It is actually really easy to land someone on another planet if you aren’t worrying about them coming back. Right now it appears that the investment is disproportionately on the “getting there” part and very little on the “safely and back again”.  Focus on Risk  What this means is that as a company your focus doesn’t need to be on training great big models, someone else is funding that, you need to look at how you use them for specific purposes in your business. What you do need to focus on though is the risk, because if you are going to use these Mars Shot scale AIs then the safety of the business is going to be your challenge.  Ride the rocket, manage the risk  So this is why I think Gary Marcus is wrong, mainly because he is right. We will see diminishing returns for these massive investments in training big models, we will see a rise in price competition as they drive for more revenue. We will see a pivot away from “My model is bigger than yours” to “I can make my models work for your faster than them”.  Therefore we will see a rise in business adoption, because the number of use cases you will be able to afford to do will increase, significantly so if using a proper cost management approach. This in turn will mean that businesses will need to look more about the systemic management of risk with AI.  So I’m pretty confident that we will get a “massive funding for tiny improvements in massive models” AI winter, but I think there is a reasonable chance that this will be very different to previous generations as it will not be that compute capacity is not available to businesses to use it, just that spending $7bn on a single learn to get a 0.5% improvement in general performance, when you can spend $100k to get a 500% improvement in specific performance just won’t be justifiable.  Or to use the AI Winter analogy for a second, right now they are laying down a base of fresh powder, we’ve got 9ft of powder on a packed base, and the hype is claiming that you really need 20ft of powder. So when the exponential costs mean they stop, well we’ve all got 9ft of pristine powder to carve down, and lets me honest, even a couple of feet is an utter blast.","business, need, winter, models, dont, care, powder, think, massive, fail, program, money, lose, ai, agi",2024-08-05 13:42:12.685000+00:00
17,Autoscaling Kubernetes: Scale Down Kubernetes Infra,,https://devtron.ai/blog/winter-soldier-scale-down-kubernetes-made-easy/,"The time-based scaling perfectly aligns with our cost optimization goal, where by utilizing the pattern of our traffic we can scale down or scale up our infrastructure. In the case of Non-production, it's easy to track the pattern of traffic and time-based scaling can be a good option. Devtron provides a Helm chart for the deployment of the Winter Soldier in our Kubernetes cluster which makes the whole simpler. Winter Soldier in ActionIn this section, we will deploy Winter Soldier and configure it to scale down our dev environment. Step 4: Winter Soldier in ActionOur application from the dev environment is up and running fine before the deployment of Winter Soldier.","As organizations rush towards the cloud-native paradigm, most face an unexpected issue i.e. skyrocketing infrastructure expenses. Inefficient resource management and the lack of proper demand-driven provisioning result in continuously active resources, regardless of having actual usage patterns. These factors emerge as primary reasons the cloud provider's pay-as-you-go feature appears to be a nightmare for infrastructure costs to organizations. In this blog post, we will discuss an open-source tool Winter Soldier, a tool crafted by Devtron for time-based scaling of Kubernetes that can help us save some bucks and also how to optimize resource utilization.  The time-based autoscaling scales your workloads according to defined time, it can be used to execute batch processes like hibernating microservices. The most effective way to use time-based autoscaling is where we know the exact pattern of the incoming traffic for your services. The time-based scaling perfectly aligns with our cost optimization goal, where by utilizing the pattern of our traffic we can scale down or scale up our infrastructure.  So, now we will scale down the production environment to optimize the cost…? No, there is one hidden culprit that contributes significantly to cloud costs for every organization without even coming to notice. The “Non-Production Environments” (Dev, Staging, Testing, Preview), exist in every organization and in large quantities. All these environments keep running 24/7 and along with them, the cost meter keeps spinning.  In the case of Non-production, it's easy to track the pattern of traffic and time-based scaling can be a good option. For instance, Dev environments can be scaled down at night time and can be scaled up in the day time similarly all these environments can be scaled down at the start of the weekend say Friday evening, and can be scaled up on Monday morning. Scaling down these environments can have a significant impact on cost as every organization maintains multiple non-production servers behind a single production server.  For instance, imagine scaling down your non-production environments every Saturday and Sunday throughout the year, let’s do some calculations and get the numbers on how much we can save on the cost.  365 days a year.  104 weekend days (52*2)  Potential saving= little more than 3 months a year i.e. approximately 28% of annual non-production environment costs  Winter Soldier: A Metal Arm from Devtron to Optimize the Cost  Winter Soldier is an open-source tool from Devtron, it enables time-based scaling for Kubernetes workloads. The time-based scaling with Winter Soldier helps us to reduce the cloud cost, it can be deployed to execute things such as:  Batch deletion of the unused resources Scaling of Kubernetes workloads  Winter Soldier can be operated in three modes, “scale”, “sleep”, and “delete”. These three are the modes of actions that can be executed by the Winter Soldier, if the action for the Winter Soldier is defined as scale the Kubernetes workloads will be scaled up according to the pre-defined time.  As we have seen above non-production environments often contribute significantly to cloud costs. By implementing Winter Soldier for these environments, we can automatically scale down our non-production infrastructure during off-peak hours like nights and weekends.  The scaling down of the infrastructure can also be done manually but it takes a lot of effort and time of the system administrators.  Is time-based scaling the only way to do this? No, it can also be done with HPA or Event-Driven Autoscaling, but time-based is recommended as we know the correct incoming traffic pattern. The HPA and Event-Driven autoscaling scale the workloads when the request is made, so it takes time to scale up and down the workloads.  Let's explore how Winter Soldier can help and how to implement it in our Kubernetes infrastructure.  Winter Soldier comes as an operator for Kubernetes which requires the Custom Resource Definition (CRD) named Hibernator. Devtron provides a Helm chart for the deployment of the Winter Soldier in our Kubernetes cluster which makes the whole simpler.  Note: Winter Soldier can be used independently for any Kubernetes cluster. Still, I'll proceed with devtron for this blog as it provides multiple additional features, for managing my Kubernetes native application and provides support for seamless CI/CD operations including visibility for the Helm applications.  Winter Soldier in Action  In this section, we will deploy Winter Soldier and configure it to scale down our dev environment. Let's configure it to scale down the dev environment from Friday midnight to Monday morning. The current state of the dev environment can be seen in Figure 1, as of now we are having 2 applications up and running with their pods in the Resource Browser of Devtron.  [Fig. 1]: Resource Browser Pods    Step 1: Installation of Devtron  Devtron is an open-source modular Kubernetes dashboard, designed to ease the Kubernetes operations. It is built on top of some of the popular open source tools like ArgoCD, Grafana, Trivy, etc, and is built in a modular fashion where its capabilities can be extended from an advanced Kubernetes dashboard to Kubernetes-native CI/CD pipelines, DevSecOps, Continous Delivery, and GitOps, depending upon the requirements. Its installation is pretty straightforward.  helm repo add devtron https://helm.devtron.ai helm repo update devtron helm install devtron devtron/devtron-operator \ --create-namespace --namespace devtroncd  Check out the Devtron documentation for more details about the installation and integrations.  Step 2: Helm Chart for Winter Soldier  Navigate to the Chart Store.  [Fig. 2]: Chart Store  Select the Helm Chart for Winter Soldier: devtron/winter-soldier  [Fig. 3]: Winter Soldier Helm Chart  Once you click the chart, you will be able to see a brief description of the chart, README, and an option to Configure & Deploy.  [Fig 4]: Winter Soldier Readme  Step 3: Configuring Winter Soldier  [Fig. 5]: Configuring Winter Soldier  Let’s take a look at configurations of Winter Soldier for our environment, here we want the dev environment to scale down on weekends i.e. Friday night to Monday morning.  Default values for winter-soldier. replicaCount: 1 image: quay.io/devtron/winter-soldier:abf5a822-196-14744 graceperiod: 10 resources: {} limits: cpu: 100m memory: 128Mi requests: cpu: 100m memory: 128Mi nodeSelector: {} tolerations: [] affinity: {} Provide the list of Hibernator objects in the yaml format with your custom requirements. hibernator: [] - apiVersion: pincher.devtron.ai/v1alpha1 kind: Hibernator metadata: name: sleep-hibernator spec: timeRangesWithZone: timeZone: ""Asia/Kolkata"" timeRanges: - timeFrom: 00:00 timeTo: 06:59:59 weekdayFrom: Fri weekdayTo: Mon selectors: - inclusions: - objectSelector: name: ""all"" type: ""deployment,rollout,StatefulSet"" exclusions: - namespaceSelector: name: “devtron-ci,devtron-cd,argo,kube-system,devtroncd” objectSelector: name: """" type: ""deployment,rollout,StatefulSet"" action: sleep  In the resources section, you can set the resource limits and request for Winter Soldier itself. You can adjust these according to your cluster.  section, you can set the resource limits and request for Winter Soldier itself. You can adjust these according to your cluster. In the hibernator section, we define how Winter Soldier manages your resources i.e. in timeRangesWithZone we need to define timezone for instance, we are taking Asia/Kolkata . In timeRanges we need to define the start time in timeFrom and the end time in timeTo , similarly in weekdayFrom and weekdayTo . In selectors , in inclusions we specify which resources to manage and in the exclusions , we define the exceptions.  section, we define how Winter Soldier manages your resources i.e. in we need to define for instance, we are taking . In we need to define the start time in and the end time in , similarly in and . In , in we specify which resources to manage and in the , we define the exceptions. In the spec section of the hibernator we can define ` pause: true and `  pauseUntil: ""Jan 2, 2026 3:04pm"" . By defining the pause action we can put the already scheduled hibernator at pause for a specific time window.  section of the we can define ` and ` . By defining the action we can put the already scheduled at pause for a specific time window. In the action we define the goal for Winter Soldier, for the above example, we have set it as sleep. We can also set the action as delete or scale according to need.  Once the configurations are set we can proceed with the deployment of Winter Soldier.  Step 4: Winter Soldier in Action  Our application from the dev environment is up and running fine before the deployment of Winter Soldier.  [Fig. 6]: Apps detail page before Winter Soldier  Effect of Winter Soldier sleep action on our applications from the dev environment. Before the deployment of Winter Soldier, we were having frontend-app and backend-app deployed at the dev environment, both were up and running. Let’s see what actions are taken by Winter Soldier.  [Fig. 7]: frontend app scale down  Let’s look at other applications in the same environment.  [Fig 8]: Backend app scaled down  Now that Winter Soldier has been deployed and we have checked the events of applications, to verify let's navigate to the Resource Browser of Devtron to gain visibility for our dev environment.  [Fig. 9]: Resource Browser Pods  Figure 9 shows that Winter Soldier has scaled down the pods previously visible in Figure 1, resulting in no active pods currently running in the dev environment.  Conclusion  Time-based scaling, allows organizations to automatically adjust their resource allocation based on predictable traffic patterns, such as scaling down during nights and weekends. Winter Soldier is a powerful open-source tool that helps organizations implement time-based scaling for their environments. By leveraging Winter-Soldier, organizations can significantly reduce their cloud infrastructure cost by around 28% of the total cost of the year just by scaling workloads on weekends, particularly in non-production settings like development, staging, and testing environments.  If you liked the winter-soldier, feel free to give it a ⭐️ on GitHub. Join our actively growing Discord Community and ask your questions if you have any.","scaling, kubernetes, dev, winter, define, environment, resource, scale, timebased, autoscaling, soldier, infra",2024-08-16 15:40:00+00:00
18,AI winter is well on its way,,https://blog.piekniewski.info/2018/05/28/ai-winter-is-well-on-its-way/,"Deep learning (does not) scaleOne of the key slogans repeated about deep learning is that it scales almost effortlessly. Initially it was thought that end-to-end deep learning could somehow solve this problem, a premise particularly heavily promoted by Nvidia. In such situation it is easy to learn spurious relations, as exemplified by adversarial examples in deep learning. For those who missed it, he has excellent blog posts/papers ""Deep learning: A critical appraisal"" and ""In defense of skepticism about deep learning"", where he very meticulously deconstructs the deep learning hype. I respect Gary a lot, he behaves like a real scientist should, while most so called ""deep learning stars"" just behave like cheap celebrities.","Deep learning has been at the forefront of the so called AI revolution for quite a few years now, and many people had believed that it is the silver bullet that will take us to the world of wonders of technological singularity (general AI). Many bets were made in 2014, 2015 and 2016 when still new boundaries were pushed, such as the Alpha Go etc. Companies such as Tesla were announcing through the mouths of their CEO's that fully self driving car was very close, to the point that Tesla even started selling that option to customers [to be enabled by future software update].  We have now mid 2018 and things have changed. Not on the surface yet, NIPS conference is still oversold, the corporate PR still has AI all over its press releases, Elon Musk still keeps promising self driving cars and Google CEO keeps repeating Andrew Ng's slogan that AI is bigger than electricity. But this narrative begins to crack. And as I predicted in my older post, the place where the cracks are most visible is autonomous driving - an actual application of the technology in the real world.  The dust settled on deep learning  When the ImageNet was effectively solved (note this does not mean that vision is solved), many prominent researchers in the field (including even typically quiet Geoff Hinton) were actively giving press interviews, publicizing stuff on social media (e.g. Yann Lecun, Andrew Ng, Fei-Fei Li to name a few). The general tone was that we are in front of a gigantic revolution and from now on things can only accelerate. Well years have passed and the twitter feeds of those people became less active, as exemplified by Andrew Ng below:  2013 - 0.413 tweets per day  2014 - 0.605 tweets per day  2015 - 0.320 tweets per day  2016 - 0.802 tweets per day  2017 - 0.668 tweets per day  2018 - 0.263 tweets per day (until 24 May)  Perhaps this is because Andrew's outrageous claims are now put to more scrutiny by the community as indicated in a tweet below:  Visibly the sentiment has quite considerably declined, there are much fewer tweets praising deep learning as the ultimate algorithm, the papers are becoming less ""revolutionary"" and much more ""evolutionary"". Deepmind hasn't shown anything breathtaking since their Alpha Go zero [and even that wasn't that exciting, given the obscene amount of compute necessary and applicability to games only - see Moravec's paradox]. OpenAI was rather quiet, with their last media outburst being the Dota 2 playing agent [which I suppose was meant to create as much buzz as Alpha Go, but fizzled out rather quickly]. In fact articles began showing up that even Google in fact does not know what to do with Deepmind, as their results are apparently not as practical as originally expected... As for the prominent researchers, they've been generally touring around meeting with government officials in Canada or France to secure their future grants, Yann Lecun even stepped down (rather symbolically) from the Head of Research to Chief AI scientist at Facebook. This gradual shift from rich, big corporations to government sponsored institutes suggests to me that the interest in this kind of research within these corporations (I think of Google and Facebook) is actually slowly winding down. Again these are all early signs, nothing spoken out loud, just the body language.  Deep learning (does not) scale  One of the key slogans repeated about deep learning is that it scales almost effortlessly. We had the AlexNet in 2012 which had ~60M parameters, we probably now have models with at least 1000x that number right? Well probably we do, the question however is - are these things 1000x as capable? Or even 100x as capable? A study by openAI comes in handy:  So in terms of applications for vision we see that VGG and Resnets saturated somewhat around one order of magnitude of compute resources applied (in terms of number of parameters it is actually less). Xception is a variation of google inception architecture and actually only slightly outperforms inception on ImageNet, arguably actually slightly outperforms everyone else, because essentially AlexNet solved ImageNet. So at 100 times more compute than AlexNet we pretty much saturated architectures in terms of vision, or image classification to be precise. Neural machine translation is a big effort by all the big web search players and no wonder it takes all the compute it can take (and yet google translate still sucks, though has gotten arguably better). The latest three points on that graph, interestingly show reinforcement learning related projects, applied to games by Deepmind and OpenAI. Particularly AlphaGo Zero and slightly more general AlphaZero take ridiculous amount of compute, but are not applicable in the real world applications because much of that compute is needed to simulate and generate the data these data hungry models need. OK, so we can now train AlexNet in minutes rather than days, but can we train a 1000x bigger AlexNet in days and get qualitatively better results? Apparently not...  So in fact, this graph which was meant to show how well deep learning scales, indicates the exact opposite. We can't just scale up AlexNet and get respectively better results - we have to fiddle with specific architectures, and effectively additional compute does not buy much without order of magnitude more data samples, which are in practice only available in simulated game environments.  Self driving crashes  By far the biggest blow into deep learning fame is the domain of self driving vehicles (something I anticipated for a long time, see e.g. this post from 2016). Initially it was thought that end-to-end deep learning could somehow solve this problem, a premise particularly heavily promoted by Nvidia. I don't think there is a single person on Earth who still believes that, though I could be wrong. Looking at last year California DMV disengagement reports Nvidia car could not drive literally ten miles without a disengagement. In a separate post I discuss the general state of that development and comparison to human driver safety, which (spoiler alert) is not looking good. Since 2016 there were also several Tesla AutoPilot incidents [1, 2, 3] a few of which were fatal [1, 2]. Arguably Tesla Autopilot should not be confused with self driving (it is), but at least at the core it relies on the same kind of technology. As of today, aside from occasional spectacular errors [1], it still cannot stop at an intersection, recognize a traffic light, or even navigate through a roundabout. That is in May 2018, several months after the promised coast to coast Tesla autonomous drive (did not happen although the rumor is they've tried but could not get it to work without ~30 disengagements). Several months ago (in February 2018) Elon Musk repeated in a conference call when asked about the coast to coast drive that:  ""We could have done the coast-to-coast drive, but it would have required too much specialized code to effectively game it or make it somewhat brittle and that it would work for one particular route, but not the general solution. So I think we would be able to repeat it, but if it’s just not any other route, which is not really a true solution. (…)  I am pretty excited about how much progress we’re making on the neural net front. And it’s a little – it’s also one of those things that’s kind of exponential where the progress doesn’t seem – it doesn’t seem like much progress, it doesn’t seem like much progress, and suddenly wow. It will feel like well this is a lame driver, lame driver. Like okay, that’s a pretty good driver. Like ‘Holy Cow!’ this driver’s good. It’ll be like that,”  Well, looking at the graph above (from OpenAI) I seem not be seeing that exponential progress. Neither is it visible in miles before disengagement for pretty much every big player in this field. In essence the above statement should be interpreted: ""We currently don't have the technology that could safely drive us coast to coast, though we could have faked it if we really wanted to (maybe...). We deeply hope that some sort of exponential jump in capabilities of neural networks will soon happen and save us from disgrace and massive lawsuits"".  But by far the biggest pin punching through the AI bubble was the accident in which Uber self driving car killed a pedestrian in Arizona. From the preliminary report by the NTSB we can read some astonishing statements:  Aside from general system design failure apparent in this report, it is striking that the system spent long seconds trying to decide what exactly it sees in front (whether that be a pedestrian, bike, vehicle or whatever else) rather than making the only logical decision in these circumstances, which was to make sure not to hit it. There are several reasons for it: first, people will often verbalize their decisions post factum. So a human will typically say: ""I saw a cyclist therefore I veered to the left to avoid him"". Huge amount of psychophysical literature will suggest a quite different explanation: a human saw something which was very quickly interpreted as a obstacle by fast perceptual loops of his nervous systems and he performed a rapid action to avoid it, long seconds later realizing what happened and providing a verbal explanation"". There are tons of decisions we make every day that are not verbalized, and driving includes many of them. Verbalization is costly and takes time and reality often does not provide that time. These mechanisms have evolved for a billion years to keep us safe, and driving context (although modern) makes use of many such reflexes. And since these reflexes have not evolved specifically for driving, they may induce mistakes. A knee jerk reaction to a wasp buzzing in a car may have caused many crashes and deaths. But our general understanding of 3d space, speed, ability to predict the behavior of agents, behavior of physical objects traversing through our path are the primitive skills, that were just as useful 100 million years ago as they are today and they've been honed really sharp by evolution.  But because most of these things are not easily verbalizable, they are hard to measure, and consequently we don't optimize our machine learning systems on these aspects at all [see my earlier post for benchmark proposals that would address some of these capabilities]. Now this would speak in favor of Nvidia end-to-end approach - learn image->action mapping, skipping any verbalization, and in some ways this is the right way to do it but... the problem is that the input space is incredibly high dimensional, while the action space is very low dimensional. Hence the ""amount"" of ""label"" (readout) is extremely small compared to the amount of information coming in. In such situation it is easy to learn spurious relations, as exemplified by adversarial examples in deep learning. A different paradigm is needed and I postulate prediction of the entire perceptual input along with the action as a first step to make a system able to extract the semantics of the world, rather than spurious correlations [read more about my first proposed architecture called Predictive Vision Model].  In fact if there is anything at all we learned from the outburst of deep learning, is that (10k+ dimensional) image space has plenty enough spurious patterns in it, that they actually generalize across many images and make the impression like our classifiers actually understand what they are seeing. Nothing could be further from the truth, as admitted even by the top researchers who are heavily invested in this field. In fact many top researchers should not be too outraged by my observations, Yann Lecun warned about overexcitement and AI winter for a while, even Geoffrey Hinton - the father of the current outburst of backpropagation - admitted in an interview that this likely is all a dead end, and we need to start over. At this point though, the hype is so strong that nobody will listen, even to the founding fathers of the field.  Gary Marcus and his quest against the hype  I should mention that more top tier people are recognizing the hubris and have the courage to openly call it. One of the most active in that space is Gary Marcus. Although I don't think I agree with everything that Gary proposes in terms of AI, we certainly agree that it is not yet as powerful as painted by the deep learning hype-propaganda. In fact it is not even any close. For those who missed it, he has excellent blog posts/papers ""Deep learning: A critical appraisal"" and ""In defense of skepticism about deep learning"", where he very meticulously deconstructs the deep learning hype. I respect Gary a lot, he behaves like a real scientist should, while most so called ""deep learning stars"" just behave like cheap celebrities.  Conclusion  Predicting the A.I. winter is like predicting a stock market crash - impossible to tell precisely when it happens, but almost certain that it will at some point. Much like before a stock market crash, there are signs of the impending collapse, but the narrative is so strong that it is very easy to ignore them, even if they are in plain sight. In my opinion there are such signs of a huge decline in deep learning (and probably in AI in general as this term has been abused 'ad nauseam' by corporate propaganda) already visible. Visible in plain sight, yet hidden from the majority by an increasingly intense narrative. How ""deep"" will this winter be? I have no idea. What will come next? I have no idea. But I'm fairly positive it is coming, perhaps sooner rather than later.  If you found an error, highlight it and press Shift + Enter or click here to inform us.  Related  Comments  comments","driving, tweets, winter, general, coast, compute, way, alexnet, fact, learning, ai, deep",2018-05-28 00:00:00
19,AI winter is well on its way,,https://blog.piekniewski.info/2018/05/28/ai-winter-is-well-on-its-way/,"Deep learning (does not) scaleOne of the key slogans repeated about deep learning is that it scales almost effortlessly. Initially it was thought that end-to-end deep learning could somehow solve this problem, a premise particularly heavily promoted by Nvidia. In such situation it is easy to learn spurious relations, as exemplified by adversarial examples in deep learning. For those who missed it, he has excellent blog posts/papers ""Deep learning: A critical appraisal"" and ""In defense of skepticism about deep learning"", where he very meticulously deconstructs the deep learning hype. I respect Gary a lot, he behaves like a real scientist should, while most so called ""deep learning stars"" just behave like cheap celebrities.","Deep learning has been at the forefront of the so called AI revolution for quite a few years now, and many people had believed that it is the silver bullet that will take us to the world of wonders of technological singularity (general AI). Many bets were made in 2014, 2015 and 2016 when still new boundaries were pushed, such as the Alpha Go etc. Companies such as Tesla were announcing through the mouths of their CEO's that fully self driving car was very close, to the point that Tesla even started selling that option to customers [to be enabled by future software update].  We have now mid 2018 and things have changed. Not on the surface yet, NIPS conference is still oversold, the corporate PR still has AI all over its press releases, Elon Musk still keeps promising self driving cars and Google CEO keeps repeating Andrew Ng's slogan that AI is bigger than electricity. But this narrative begins to crack. And as I predicted in my older post, the place where the cracks are most visible is autonomous driving - an actual application of the technology in the real world.  The dust settled on deep learning  When the ImageNet was effectively solved (note this does not mean that vision is solved), many prominent researchers in the field (including even typically quiet Geoff Hinton) were actively giving press interviews, publicizing stuff on social media (e.g. Yann Lecun, Andrew Ng, Fei-Fei Li to name a few). The general tone was that we are in front of a gigantic revolution and from now on things can only accelerate. Well years have passed and the twitter feeds of those people became less active, as exemplified by Andrew Ng below:  2013 - 0.413 tweets per day  2014 - 0.605 tweets per day  2015 - 0.320 tweets per day  2016 - 0.802 tweets per day  2017 - 0.668 tweets per day  2018 - 0.263 tweets per day (until 24 May)  Perhaps this is because Andrew's outrageous claims are now put to more scrutiny by the community as indicated in a tweet below:  Visibly the sentiment has quite considerably declined, there are much fewer tweets praising deep learning as the ultimate algorithm, the papers are becoming less ""revolutionary"" and much more ""evolutionary"". Deepmind hasn't shown anything breathtaking since their Alpha Go zero [and even that wasn't that exciting, given the obscene amount of compute necessary and applicability to games only - see Moravec's paradox]. OpenAI was rather quiet, with their last media outburst being the Dota 2 playing agent [which I suppose was meant to create as much buzz as Alpha Go, but fizzled out rather quickly]. In fact articles began showing up that even Google in fact does not know what to do with Deepmind, as their results are apparently not as practical as originally expected... As for the prominent researchers, they've been generally touring around meeting with government officials in Canada or France to secure their future grants, Yann Lecun even stepped down (rather symbolically) from the Head of Research to Chief AI scientist at Facebook. This gradual shift from rich, big corporations to government sponsored institutes suggests to me that the interest in this kind of research within these corporations (I think of Google and Facebook) is actually slowly winding down. Again these are all early signs, nothing spoken out loud, just the body language.  Deep learning (does not) scale  One of the key slogans repeated about deep learning is that it scales almost effortlessly. We had the AlexNet in 2012 which had ~60M parameters, we probably now have models with at least 1000x that number right? Well probably we do, the question however is - are these things 1000x as capable? Or even 100x as capable? A study by openAI comes in handy:  So in terms of applications for vision we see that VGG and Resnets saturated somewhat around one order of magnitude of compute resources applied (in terms of number of parameters it is actually less). Xception is a variation of google inception architecture and actually only slightly outperforms inception on ImageNet, arguably actually slightly outperforms everyone else, because essentially AlexNet solved ImageNet. So at 100 times more compute than AlexNet we pretty much saturated architectures in terms of vision, or image classification to be precise. Neural machine translation is a big effort by all the big web search players and no wonder it takes all the compute it can take (and yet google translate still sucks, though has gotten arguably better). The latest three points on that graph, interestingly show reinforcement learning related projects, applied to games by Deepmind and OpenAI. Particularly AlphaGo Zero and slightly more general AlphaZero take ridiculous amount of compute, but are not applicable in the real world applications because much of that compute is needed to simulate and generate the data these data hungry models need. OK, so we can now train AlexNet in minutes rather than days, but can we train a 1000x bigger AlexNet in days and get qualitatively better results? Apparently not...  So in fact, this graph which was meant to show how well deep learning scales, indicates the exact opposite. We can't just scale up AlexNet and get respectively better results - we have to fiddle with specific architectures, and effectively additional compute does not buy much without order of magnitude more data samples, which are in practice only available in simulated game environments.  Self driving crashes  By far the biggest blow into deep learning fame is the domain of self driving vehicles (something I anticipated for a long time, see e.g. this post from 2016). Initially it was thought that end-to-end deep learning could somehow solve this problem, a premise particularly heavily promoted by Nvidia. I don't think there is a single person on Earth who still believes that, though I could be wrong. Looking at last year California DMV disengagement reports Nvidia car could not drive literally ten miles without a disengagement. In a separate post I discuss the general state of that development and comparison to human driver safety, which (spoiler alert) is not looking good. Since 2016 there were also several Tesla AutoPilot incidents [1, 2, 3] a few of which were fatal [1, 2]. Arguably Tesla Autopilot should not be confused with self driving (it is), but at least at the core it relies on the same kind of technology. As of today, aside from occasional spectacular errors [1], it still cannot stop at an intersection, recognize a traffic light, or even navigate through a roundabout. That is in May 2018, several months after the promised coast to coast Tesla autonomous drive (did not happen although the rumor is they've tried but could not get it to work without ~30 disengagements). Several months ago (in February 2018) Elon Musk repeated in a conference call when asked about the coast to coast drive that:  ""We could have done the coast-to-coast drive, but it would have required too much specialized code to effectively game it or make it somewhat brittle and that it would work for one particular route, but not the general solution. So I think we would be able to repeat it, but if it’s just not any other route, which is not really a true solution. (…)  I am pretty excited about how much progress we’re making on the neural net front. And it’s a little – it’s also one of those things that’s kind of exponential where the progress doesn’t seem – it doesn’t seem like much progress, it doesn’t seem like much progress, and suddenly wow. It will feel like well this is a lame driver, lame driver. Like okay, that’s a pretty good driver. Like ‘Holy Cow!’ this driver’s good. It’ll be like that,”  Well, looking at the graph above (from OpenAI) I seem not be seeing that exponential progress. Neither is it visible in miles before disengagement for pretty much every big player in this field. In essence the above statement should be interpreted: ""We currently don't have the technology that could safely drive us coast to coast, though we could have faked it if we really wanted to (maybe...). We deeply hope that some sort of exponential jump in capabilities of neural networks will soon happen and save us from disgrace and massive lawsuits"".  But by far the biggest pin punching through the AI bubble was the accident in which Uber self driving car killed a pedestrian in Arizona. From the preliminary report by the NTSB we can read some astonishing statements:  Aside from general system design failure apparent in this report, it is striking that the system spent long seconds trying to decide what exactly it sees in front (whether that be a pedestrian, bike, vehicle or whatever else) rather than making the only logical decision in these circumstances, which was to make sure not to hit it. There are several reasons for it: first, people will often verbalize their decisions post factum. So a human will typically say: ""I saw a cyclist therefore I veered to the left to avoid him"". Huge amount of psychophysical literature will suggest a quite different explanation: a human saw something which was very quickly interpreted as a obstacle by fast perceptual loops of his nervous systems and he performed a rapid action to avoid it, long seconds later realizing what happened and providing a verbal explanation"". There are tons of decisions we make every day that are not verbalized, and driving includes many of them. Verbalization is costly and takes time and reality often does not provide that time. These mechanisms have evolved for a billion years to keep us safe, and driving context (although modern) makes use of many such reflexes. And since these reflexes have not evolved specifically for driving, they may induce mistakes. A knee jerk reaction to a wasp buzzing in a car may have caused many crashes and deaths. But our general understanding of 3d space, speed, ability to predict the behavior of agents, behavior of physical objects traversing through our path are the primitive skills, that were just as useful 100 million years ago as they are today and they've been honed really sharp by evolution.  But because most of these things are not easily verbalizable, they are hard to measure, and consequently we don't optimize our machine learning systems on these aspects at all [see my earlier post for benchmark proposals that would address some of these capabilities]. Now this would speak in favor of Nvidia end-to-end approach - learn image->action mapping, skipping any verbalization, and in some ways this is the right way to do it but... the problem is that the input space is incredibly high dimensional, while the action space is very low dimensional. Hence the ""amount"" of ""label"" (readout) is extremely small compared to the amount of information coming in. In such situation it is easy to learn spurious relations, as exemplified by adversarial examples in deep learning. A different paradigm is needed and I postulate prediction of the entire perceptual input along with the action as a first step to make a system able to extract the semantics of the world, rather than spurious correlations [read more about my first proposed architecture called Predictive Vision Model].  In fact if there is anything at all we learned from the outburst of deep learning, is that (10k+ dimensional) image space has plenty enough spurious patterns in it, that they actually generalize across many images and make the impression like our classifiers actually understand what they are seeing. Nothing could be further from the truth, as admitted even by the top researchers who are heavily invested in this field. In fact many top researchers should not be too outraged by my observations, Yann Lecun warned about overexcitement and AI winter for a while, even Geoffrey Hinton - the father of the current outburst of backpropagation - admitted in an interview that this likely is all a dead end, and we need to start over. At this point though, the hype is so strong that nobody will listen, even to the founding fathers of the field.  Gary Marcus and his quest against the hype  I should mention that more top tier people are recognizing the hubris and have the courage to openly call it. One of the most active in that space is Gary Marcus. Although I don't think I agree with everything that Gary proposes in terms of AI, we certainly agree that it is not yet as powerful as painted by the deep learning hype-propaganda. In fact it is not even any close. For those who missed it, he has excellent blog posts/papers ""Deep learning: A critical appraisal"" and ""In defense of skepticism about deep learning"", where he very meticulously deconstructs the deep learning hype. I respect Gary a lot, he behaves like a real scientist should, while most so called ""deep learning stars"" just behave like cheap celebrities.  Conclusion  Predicting the A.I. winter is like predicting a stock market crash - impossible to tell precisely when it happens, but almost certain that it will at some point. Much like before a stock market crash, there are signs of the impending collapse, but the narrative is so strong that it is very easy to ignore them, even if they are in plain sight. In my opinion there are such signs of a huge decline in deep learning (and probably in AI in general as this term has been abused 'ad nauseam' by corporate propaganda) already visible. Visible in plain sight, yet hidden from the majority by an increasingly intense narrative. How ""deep"" will this winter be? I have no idea. What will come next? I have no idea. But I'm fairly positive it is coming, perhaps sooner rather than later.  If you found an error, highlight it and press Shift + Enter or click here to inform us.  Related  Comments  comments","driving, tweets, winter, general, coast, compute, way, alexnet, fact, learning, ai, deep",2018-05-28 00:00:00
20,Is Another AI Winter Coming?,,https://hackernoon.com/is-another-ai-winter-coming-ac552669e58c,"Such failures would terminate AI research for decades. It is worth mentioning that these invaluable tools indeed came from AI research, but they were rebranded since they failed to live up to their grander purposes. By 2015, “AI” research commanded huge budgets of many Fortune 500 companies. If you want a more thorough explanation, check out Grant Sanderson’s amazing video series on neural networks here:An AI Renaissance? It is for these reasons I think another AI Winter is coming.","And, has deep learning already hit its limitations?  Many believed an algorithm would transcend humanity with cognitive awareness. Machines would discern and learn tasks without human intervention and replace workers in droves. They quite literally would be able to “think”. Many people even raised the question whether we could have robots for spouses.  But I am not talking about today. What if I told you this idea was widely marketed in the 1960’s, and AI pioneers Jerome Wiesner, Oliver Selfridge, and Claude Shannon insisted this could happen in their near future? If you find this surprising, watch this video and be amazed how familiar these sentiments are.  Fast forward to 1973, and the hype and exaggeration of AI backfired. The U.K. Parliament sent Sir James Lighthill to get a status report of A.I. research in the U.K. The report criticized the failure of artificial intelligence research to live up to its sensational claims. Interestingly, Lighthill also pointed out how specialized programs (or people) performed better than their “AI” counterparts, and had no prospects in real-world environments. Consequently, AI research funding was cancelled by the British government.  Across the pond, the United States Department of Defense was invested heavily in AI research, but then cancelled nearly all funding over the same frustrations: exaggeration of AI ability, high costs with no return, and dubious value in the real world.  In the 1980’s, Japan enthusiastically attempted a bold stab at “AI” with the Fifth Generation Project (EDIT: Toby Walsh himself corrected me in the comments. UK research did pick up again in the 1980’s with the Alvey Project in response to Japan). However, that ended up being a costly $850 million failure as well.  The First AI Winter  The end of the 1980’s brought forth an A.I. Winter, a dark period in computer science where “artificial intelligence” research burned organizations and governments with delivery failures and sunk costs. Such failures would terminate AI research for decades.  Oftentimes, these companies were driven by FOMO rather than practical use cases, worried that they would be left behind by their automated competitors.  By the time the 1990’s rolled around, “AI” became a dirty word and continued to be in the 2000’s. It was widely accepted that “AI just didn’t work”. Software companies who wrote seemingly intelligent programs would use terms like “search algorithms”, “business rule engines”, “constraint solvers”, and “operations research”. It is worth mentioning that these invaluable tools indeed came from AI research, but they were rebranded since they failed to live up to their grander purposes.  But around 2010, something started to change. There was rapidly growing interest in AI again and competitions in categorizing images caught the media’s eye. Silicon Valley was sitting on huge amounts of data, and for the first time there was enough to possibly make neural networks useful.  By 2015, “AI” research commanded huge budgets of many Fortune 500 companies. Oftentimes, these companies were driven by FOMO rather than practical use cases, worried that they would be left behind by their automated competitors. After all, having a neural network is nothing short of impressive! To the layperson, SkyNet capabilities must surely be next.  But is this really a step towards true AI? Or is history repeating itself, but this time emboldened by a handful of successful use cases?  What is AI Anyway?  For a long time, I have never liked the term “artificial intelligence”. It is vague and far-reaching, and defined more by marketing folks than scientists. Of course, marketing and buzzwords are arguably necessary to spur positive change. However, buzzword campaigns inevitably lead to confusion. My new ASUS smart phone has an “AI Ringtone” feature that dynamically adjusts the ring volume to be just loud enough over ambient noise. I guess something that literally could be programmed with a series of `if` conditions, or a simple linear function, is called “AI”. Alrighty then.  In light of that, it is probably no surprise the definition of “AI” is widely disputed. I like Geoffrey De Smet’s definition, which states AI solutions are for problems with a nondeterministic answer and/or an inevitable margin of error. This would include a wide array of tools from machine learning to probability and search algorithms.  It can also be said that the definition of AI evolves and only includes ground-breaking developments, while yesterday’s successes (like optical character recognition or language translators) are no longer considered “AI”. So “artificial intelligence” can be a relative term and hardly absolute.  In recent years, “AI” has often been associated with “neural networks” which is what this article will focus on. There are other “AI” solutions out there, from other machine learning models (Naive Bayes, Support Vector Machines, XGBoost) to search algorithms. However, neural networks are arguably the hottest and most hyped technology at the moment. If you want to learn more about neural networks, I posted my video below.  If you want a more thorough explanation, check out Grant Sanderson’s amazing video series on neural networks here:  An AI Renaissance?  The resurgence of AI hype after 2010 is simply due to a new class of tasks being mastered: categorization. More specifically, scientists have developed effective ways to categorize most types of data including images and natural language thanks to neural networks. Even self-driving cars are categorization tasks, where each image of the surrounding road translates into a set of discrete actions (gas, break, turn left, turn right, etc). To get a simplified idea of how this works, watch .  In my opinion, Natural language processing is more impressive than pure categorization though. It is easy to believe these algorithms are sentient, but if you study them carefully you can tell they are relying on language patterns rather than consciously-constructed thoughts. These can lead to some entertaining results, like these bots that will troll scammers for you:  Probably the most impressive feat of natural language processing is Google Duplex, which allows your Android phone to make phone calls on your behalf, specifically for appointments. However, you have to consider that Google trained, structured, and perhaps even hardcoded the “AI” just for that task. And sure, the fake caller sounds natural with pauses, “ahhs”, and “uhms”… but again, this was done through operations on speech patterns, not actual reasoning and thoughts.  This is all very impressive, and definitely has some useful applications. But we really need to temper our expectations and stop hyping “deep learning” capabilities. If we don’t, we may find ourselves in another AI Winter.  History Repeats Itself  Gary Marcus at NYU wrote an interesting article on the limitations of deep learning, and poses several sobering points (he also wrote an equally interesting follow-up after the article went viral). Rodney Brooks is putting timelines together and keeping track of his AI hype cycle predictions, and predicts we will see “ The Era of Deep Learning is Over” headlines in 2020.  The skeptics generally share a few key points. Neural networks are data-hungry and even today, data is finite. This is also why “game” AI examples you see on YouTube (like as well as ) often require days of constant losing gameplay until the neural network finds a pattern that allows it to win.  We really need to temper our expectations and stop hyping “deep learning” capabilities. If we don’t, we may find ourselves in another AI Winter.  Neural networks are “deep” in that they technically have several layers of nodes, not because it develops deep understanding about the problem. These layers also make the neural networks difficult to understand, even for its developer. Most importantly, neural networks are experiencing diminishing return when they venture out into other problem spaces, such as the Traveling Salesman Problem. And this makes sense. Why in the world would I solve the Traveling Salesman Problem with a neural network when a search algorithm will be much more straightforward, effective, scalable, and economical (as shown in the video below)?  Using search algorithms like simulated annealing for the Traveling Salesman Problem  Nor would I use deep learning to solve other everyday “AI” problems, like solving Sudokus or packing events into a schedule, which I discuss how to do in a separate article:      Sudokus and Schedules_Solving Scheduling Problems with Tree Search_towardsdatascience.com  Of course, there are folks looking to generalize more problem spaces into neural networks, and while that is interesting it rarely seems to outperform any specialized algorithms.  Luke Hewitt at MIT puts it best in this article:  It is a bad idea to intuit how broadly intelligent a machine must be, or have the capacity to be, based solely on a single task. The checkers-playing machines of the 1950s amazed researchers and many considered these a huge leap towards human-level reasoning, yet we now appreciate that achieving human or superhuman performance in this game is far easier than achieving human-level general intelligence. In fact, even the best humans can easily be defeated by a search algorithm with simple heuristics. Human or superhuman performance in one task is not necessarily a stepping-stone towards near-human performance across most tasks.  — Luke Hewitt  I think it is also worth pointing out that neural networks require vast amounts of hardware and energy to train. To me, that just does not feel sustainable. Of course, a neural network will predict much more efficiently than it trains. However I do think the ambitions people have for neural networks will demand constant training and therefore require exponential energy and costs. And sure, computers keep getting faster but can chip manufacturers struggle past the failure of Moore’s Law?  A final point to consider is the P versus NP problem. To describe this in the simplest terms possible, proving P = NP would mean we could calculate solutions to very difficult problems (like machine learning, cryptography, and optimization) just as quickly as we can verify them. Such a breakthrough would expand the capabilities of AI algorithms drastically and maybe transform our world beyond recognition (Fun fact: there’s a 2012 intellectual thriller movie called The Travelling Salesman which explores this idea).  Here is a great video that explains the P versus NP problem, and it is worth the 10 minutes to watch:  An explanation of P versus NP  Sadly after 50 years since the problem was formalized, more computer scientists are coming to believe that P does not equal NP. In my opinion, this is an enormous barrier to AI research that we may never overcome, as this means complexity will always limit what we can do.  And this makes sense. Why in the world would I solve the Traveling Salesman Problem with a neural network when a search algorithm will be much more effective, scalable, and economical?  It is for these reasons I think another AI Winter is coming. In 2018, a growing number of experts, articles, forum posts, and bloggers came forward calling out these limitations. I think this skepticism trend is going to intensify in 2019 and will go mainstream as soon as 2020. Companies are still sparing little expense in getting the best “deep learning” and “AI” talent, but I think it is a matter of time before many companies realize deep learning is not what they need. Even worse, if your company does not have Google’s research budget, the PhD talent, or massive data store it collected from users, you can quickly find your practical “deep learning” prospects very limited. This was best captured in this scene from the HBO show Silicon Valley (WARNING: language):  Each AI Winter is preceded with scientists exaggerating and hyping the potential of their creations. It is not enough to say their algorithm can do one task well. They want it to ambitiously adapt to any task, or at least give the impression it can. For instance, AlphaZero makes a better chess playing algorithm. Media’s reaction is “Oh my gosh, general AI is here. Everybody run for cover! The robots are coming!” Then the scientists do not bother correcting them and actually encourage it using clever choices of words. Tempering expectations does not help VC funding after all. But there could be other reasons why AI researchers anthropomorphize algorithms despite their robotic limitations, and it is more philosophical than scientific. I will save that for the end of the article.  So What’s Next?  Of course, not every company using “machine learning” or “AI” is actually using “deep learning.” A good data scientist may have been hired to build a neural network, but when she actually studies the problem she more appropriately builds a instead. For the companies that are successfully using image recognition and language processing, they will continue to do so happily. But I do think neural networks are not going to progress far from those problem spaces.  Tempering expectations does not help VC funding after all.  The AI Winters of the past were devastating in pushing the boundaries of computer science. It is worth pointing out that useful things came out of such research, like search algorithms which can effectively win at chess or minimize costs in transportation problems. Simply put, innovative algorithms emerged that often excelled at one particular task.  The point I am making is there are many proven solutions out there for many types of problems. To avoid getting put out in the cold by an AI Winter, the best thing you can do is be specific about the problem you are trying to solve and understand its nature. After that, find approaches that provide an intuitive path to a solution for that particular problem. If you want to categorize text messages, you probably want to use . If you are trying to optimize your transportation network, you likely should use Discrete Optimization. No matter the peer pressure, you are allowed to approach convoluted models with a healthy amount of skepticism, and question whether it is the right approach.  Hopefully this article made it abundantly clear deep learning is not the right approach for most problems. There is no free lunch. Do not hit the obstacle of seeking a generalized AI solution for all your problems, because you are not going to find one.  Are Our Thoughts Really Dot Products? Philosophy vs Science  One last point I want to throw in this article, and it is more philosophical than scientific. Is every thought and feeling we have simply a bunch of numbers being multiplied and added in linear algebra fashion? Are our brains, in fact, simply a neural network doing dot products all day? That sounds almost like a Pythagorean philosophy that reduces our consciousness to a matrix of numbers. Perhaps this is why so many scientists believe general artificial intelligence is possible, as being human is no different than being a computer. (I’m just pointing this out, not commenting whether this worldview is right or wrong).  No matter the peer pressure, you are allowed to approach convoluted models with a healthy amount of skepticism, and question whether it is the right approach.  If you do not buy into this Pythagorean philosophy, then the best you can strive for is have AI “simulate” actions that give the illusion it has sentiments and thoughts. A translation program does not understand Chinese. It “simulates” the illusion of understanding Chinese by finding probabilistic patterns. When your smartphone “recognizes” a picture of a dog, does it really recognize a dog? Or does it see a grid of numbers it saw before?  This article was originally published on Towards Data Science.  Related Articles:      Data Science in Tech - Hacker Noon_Data science is an interdisciplinary field that uses scientific methods, processes, algorithms and systems to extract…_hackernoon.com      Are Our Thoughts Really Dot Products?_How AI Research Revived Pythagoreanism and Confused Science with Philosophy_towardsdatascience.com      How It Feels to Learn Data Science in 2019_Seeing the (Random) Forest Through the (Decision) Trees_towardsdatascience.com      Data - Hacker Noon_Read writing about Data in Hacker Noon. how hackers start their afternoons._hackernoon.com","neural, research, coming, algorithms, winter, deep, problem, problems, search, learning, ai, networks",
21,Self-driving cars are headed toward an AI roadblock,Russell Brandom,https://www.theverge.com/2018/7/3/17530232/self-driving-ai-winter-full-autonomy-waymo-tesla-uber,"There’s growing concern among AI experts that it may be years, if not decades, before self-driving systems can reliably avoid accidents. That leaves Tesla and other autonomy companies with a scary question: Will self-driving cars keep getting better, like image search, voice recognition, and the other AI success stories? But nearly every car accident involves some sort of unforeseen circumstance, and without the power to generalize, self-driving cars will have to confront each of these scenarios as if for the first time. “I see all these micro-improvements as extraordinary features on the journey towards full autonomy.”Still, it’s not clear how long self-driving cars can stay in their current limbo. One study by the Rand Corporation estimated that self-driving cars would have to drive 275 million miles without a fatality to prove they were as safe as human drivers.","Part of / The Real-World AI Issue  If you believe the CEOs, a fully autonomous car could be only months away. In 2015, Elon Musk predicted a fully autonomous Tesla by 2018; so did Google. Delphi and MobileEye’s Level 4 system is currently slated for 2019, the same year Nutonomy plans to deploy thousands of driverless taxis on the streets of Singapore. GM will put a fully autonomous car into production in 2019, with no steering wheel or ability for drivers to intervene. There’s real money behind these predictions, bets made on the assumption that the software will be able to catch up to the hype.  On its face, full autonomy seems closer than ever. Waymo is already testing cars on limited-but-public roads in Arizona. Tesla and a host of other imitators already sell a limited form of Autopilot, counting on drivers to intervene if anything unexpected happens. There have been a few crashes, some deadly, but as long as the systems keep improving, the logic goes, we can’t be that far from not having to intervene at all.  But the dream of a fully autonomous car may be further than we realize. There’s growing concern among AI experts that it may be years, if not decades, before self-driving systems can reliably avoid accidents. As self-trained systems grapple with the chaos of the real world, experts like NYU’s Gary Marcus are bracing for a painful recalibration in expectations, a correction sometimes called “AI winter.” That delay could have disastrous consequences for companies banking on self-driving technology, putting full autonomy out of reach for an entire generation.  “Driverless cars are like a scientific experiment where we don’t know the answer”  It’s easy to see why car companies are optimistic about autonomy. Over the past ten years, deep learning — a method that uses layered machine-learning algorithms to extract structured information from massive data sets — has driven almost unthinkable progress in AI and the tech industry. It powers Google Search, the Facebook News Feed, conversational speech-to-text algorithms, and champion Go-playing systems. Outside the internet, we use deep learning to detect earthquakes, predict heart disease, and flag suspicious behavior on a camera feed, along with countless other innovations that would have been impossible otherwise.  But deep learning requires massive amounts of training data to work properly, incorporating nearly every scenario the algorithm will encounter. Systems like Google Images, for instance, are great at recognizing animals as long as they have training data to show them what each animal looks like. Marcus describes this kind of task as “interpolation,” taking a survey of all the images labeled “ocelot” and deciding whether the new picture belongs in the group.  Engineers can get creative in where the data comes from and how it’s structured, but it places a hard limit on how far a given algorithm can reach. The same algorithm can’t recognize an ocelot unless it’s seen thousands of pictures of an ocelot — even if it’s seen pictures of housecats and jaguars, and knows ocelots are somewhere in between. That process, called “generalization,” requires a different set of skills.  For a long time, researchers thought they could improve generalization skills with the right algorithms, but recent research has shown that conventional deep learning is even worse at generalizing than we thought. One study found that conventional deep learning systems have a hard time even generalizing across different frames of a video, labeling the same polar bear as a baboon, mongoose, or weasel depending on minor shifts in the background. With each classification based on hundreds of factors in aggregate, even small changes to pictures can completely change the system’s judgment, something other researchers have taken advantage of in adversarial data sets.  Marcus points to the chat bot craze as the most recent example of hype running up against the generalization problem. “We were promised chat bots in 2015,” he says, “but they’re not any good because it’s not just a matter of collecting data.” When you’re talking to a person online, you don’t just want them to rehash earlier conversations. You want them to respond to what you’re saying, drawing on broader conversational skills to produce a response that’s unique to you. Deep learning just couldn’t make that kind of chat bot. Once the initial hype faded, companies lost faith in their chat bot projects, and there are very few still in active development.  That leaves Tesla and other autonomy companies with a scary question: Will self-driving cars keep getting better, like image search, voice recognition, and the other AI success stories? Or will they run into the generalization problem like chat bots? Is autonomy an interpolation problem or a generalization problem? How unpredictable is driving, really?  It may be too early to know. “Driverless cars are like a scientific experiment where we don’t know the answer,” Marcus says. We’ve never been able to automate driving at this level before, so we don’t know what kind of task it is. To the extent that it’s about identifying familiar objects and following rules, existing technologies should be up to the task. But Marcus worries that driving well in accident-prone scenarios may be more complicated than the industry wants to admit. “To the extent that surprising new things happen, it’s not a good thing for deep learning.”  “Safety isn’t just about the quality of the AI technology”  The experimental data we have comes from public accident reports, each of which offers some unusual wrinkle. A fatal 2016 crash saw a Model S drive full speed into the rear portion of a white tractor trailer, confused by the high ride height of the trailer and bright reflection of the sun. In March, a self-driving Uber crash killed a woman pushing a bicycle, after she emerged from an unauthorized crosswalk. According to the NTSB report, Uber’s software misidentified the woman as an unknown object, then a vehicle, then finally as a bicycle, updating its projections each time. In a California crash, a Model X steered toward a barrier and sped up in the moments before impact, for reasons that remain unclear.  Each accident seems like an edge case, the kind of thing engineers couldn’t be expected to predict in advance. But nearly every car accident involves some sort of unforeseen circumstance, and without the power to generalize, self-driving cars will have to confront each of these scenarios as if for the first time. The result would be a string of fluke-y accidents that don’t get less common or less dangerous as time goes on. For skeptics, a turn through the manual disengagement reports shows that scenario already well under way, with progress already reaching a plateau.  Andrew Ng — a former Baidu executive, Drive.AI board member, and one of the industry’s most prominent boosters — argues the problem is less about building a perfect driving system than training bystanders to anticipate self-driving behavior. In other words, we can make roads safe for the cars instead of the other way around. As an example of an unpredictable case, I asked him whether he thought modern systems could handle a pedestrian on a pogo stick, even if they had never seen one before. “I think many AV teams could handle a pogo stick user in pedestrian crosswalk,” Ng told me. “Having said that, bouncing on a pogo stick in the middle of a highway would be really dangerous.”  “Rather than building AI to solve the pogo stick problem, we should partner with the government to ask people to be lawful and considerate,” he said. “Safety isn’t just about the quality of the AI technology.”  “This is not an easily isolated problem”  Deep learning isn’t the only AI technique, and companies are already exploring alternatives. Though techniques are closely guarded within the industry (just look at Waymo’s recent lawsuit against Uber), many companies have shifted to rule-based AI, an older technique that lets engineers hard-code specific behaviors or logic into an otherwise self-directed system. It doesn’t have the same capacity to write its own behaviors just by studying data, which is what makes deep learning so exciting, but it would let companies avoid some of the deep learning’s limitations. But with the basic tasks of perception still profoundly shaped by deep learning techniques, it’s hard to say how successfully engineers can quarantine potential errors.  Ann Miura-Ko, a venture capitalist who sits on the board of Lyft, says she thinks part of the problem is high expectations for autonomous cars themselves, classifying anything less than full autonomy as a failure. “To expect them to go from zero to level five is a mismatch in expectations more than a failure of technology,” Miura-Ko says. “I see all these micro-improvements as extraordinary features on the journey towards full autonomy.”  Still, it’s not clear how long self-driving cars can stay in their current limbo. Semi-autonomous products like Tesla’s Autopilot are smart enough to handle most situations, but require human intervention if anything too unpredictable happens. When something does go wrong, it’s hard to know whether the car or the driver is to blame. For some critics, that hybrid is arguably less safe than a human driver, even if the errors are hard to blame entirely on the machine. One study by the Rand Corporation estimated that self-driving cars would have to drive 275 million miles without a fatality to prove they were as safe as human drivers. The first death linked to Tesla’s Autopilot came roughly 130 million miles into the project, well short of the mark.  But with deep learning sitting at the heart of how cars perceive objects and decide to respond, improving the accident rate may be harder than it looks. “This is not an easily isolated problem,” says Duke professor Mary Cummings, pointing to an Uber crash that killed a pedestrian earlier this year. “The perception-decision cycle is often linked, as in the case of the pedestrian death. A decision was made to do nothing based on ambiguity in perception, and the emergency braking was turned off because it got too many false alarms from the sensor”  That crash ended with Uber pausing its self-driving efforts for the summer, an ominous sign for other companies planning rollouts. Across the industry, companies are racing for more data to solve the problem, assuming the company with the most miles will build the strongest system. But where companies see a data problem, Marcus sees something much harder to solve. “They’re just using the techniques that they have in the hopes that it will work,” Marcus says. “They’re leaning on the big data because that’s the crutch that they have, but there’s no proof that ever gets you to the level of precision that we need.”","headed, companies, marcus, problem, cars, roadblock, systems, data, selfdriving, learning, ai, deep",2018-07-03 00:00:00
22,AI Winter Isn’t Coming,Will Knight,https://www.technologyreview.com/s/603062/ai-winter-isnt-coming/,"Andrew Ng, chief scientist at Baidu Research, and a major figure in the field of machine learning and AI, says improvements in computer processor design will keep performance advances and breakthroughs coming for the foreseeable future. The advances seen in recent years have come thanks to the development of powerful “deep learning” systems (see “10 Breakthrough Technologies 2013: Deep Learning”). This might not only increase the accuracy of existing deep learning tools, but also allow the technique to be leveraged in new areas, such as parsing and generating language. What’s more, Ng says, hardware advances will provide the fuel required to make emerging AI techniques feasible. The world’s leading AI experts convened in Barcelona this week for a prominent event called the Neural Information Processing Systems conference.","Andrew Ng, chief scientist at Baidu Research, and a major figure in the field of machine learning and AI, says improvements in computer processor design will keep performance advances and breakthroughs coming for the foreseeable future. “Multiple [hardware vendors] have been kind enough to share their roadmaps,” Ng says. “I feel very confident that they are credible and we will get more computational power and faster networks in the next several years.”  The field of AI has gone through phases of rapid progress and hype in the past, quickly followed by a cooling in investment and interest, often referred to as “AI winters.” The first chill occurred in the 1970s, as progress slowed and government funding dried up; another struck in the 1980s as the latest trends failed to have the expected commercial impact.  Then again, there’s perhaps been no boom to match the current one, propelled by rapid progress in training machines to do useful tasks. Artificial intelligence researchers are now offered huge wages to perform fundamental research, as companies build research teams on the assumption that commercially important breakthroughs will follow.  Andrew Ng, chief scientist at Baidu Research.  The advances seen in recent years have come thanks to the development of powerful “deep learning” systems (see “10 Breakthrough Technologies 2013: Deep Learning”). Starting a few years ago, researchers found that very large, or deep, neural networks could be trained, using labeled examples, to recognize all sorts of things with human-like accuracy. This has led to stunning advances in image and voice recognition and elsewhere.  Ng says these systems will only become more powerful. This might not only increase the accuracy of existing deep learning tools, but also allow the technique to be leveraged in new areas, such as parsing and generating language.  What’s more, Ng says, hardware advances will provide the fuel required to make emerging AI techniques feasible.  “There are multiple experiments I’d love to run if only we had a 10-x increase in performance,” Ng adds. For instance, he says, instead of having various different image-processing algorithms, greater computer power might make it possible to build a single algorithm capable of doing all sorts of image-related tasks.  The world’s leading AI experts convened in Barcelona this week for a prominent event called the Neural Information Processing Systems conference. The scale of the gathering, which has grown from several hundred people a few years ago to more than 6,000 this year, offers some sense of the huge interest there is in artificial intelligence.","research, coming, winter, scientist, progress, ai, advances, systems, researchers, learning, sorts, isnt, deep",
23,There Was No ‘First AI Winter’ – Communications of the ACM,"Thomas Haigh, Alex Williams, Logan Kugler, Alex Tray, About The Authors",https://cacm.acm.org/magazines/2023/12/278152-there-was-no-first-ai-winter/fulltext,"Military Origins of AI AI began as a Cold War project centered on a handful of well-connected researchers and institutions. This deep entanglement of early AI with the U.S. military is difficult to overlook. During the 1950s and 1960s it did not have a directorate focused on computer science. One factor was the concentration of early AI work in a handful of well-connected labs. This narrative blames the 1973 Lighthill Report, commissioned by the Science Research Council of the United Kingdom, for a collapse of British support for AI work.","As I concluded my June Historical Reflections column, artificial intelligence had matured from an intellectual brand invented to win funding for a summer research workshop to one of the most prestigious fields in the emerging discipline of computer science. Four of the first 10 Turing Award recipients were AI specialists: Marvin Minsky, Herb Simon, Allen Newell, and John McCarthy. These men founded the three leading AI labs and played central roles in building what are still the top three U.S. computer science programs at MIT, Stanford, and Carnegie Mellon. Conceptually AI was about uncovering and duplicating the processes behind human cognition; practically it was about figuring out how to program tasks that people could do but computers could not. Although connectionist approaches training networks of simulated neurons had been prominent in the primordial stew of cybernetics and automata research from which AI emerged, all four of the Turing Award recipients favored the rival symbolic approach, in which computers algorithmically manipulated symbols according to coded rules of logic.  A History of Failed Ideas? AI was born in hype, and its story is usually told as a series of cycles of fervent enthusiasm followed by bitter disappointment. Michael Wooldridge, himself an eminent AI researcher, began his recent introduction to the field by remembering when he told a colleague about his plan to tell “the story of AI through failed ideas.” In response, “she looked back at me, her smile now faded. ‘It’s going to be a bloody long book then.’”22 Major awards lag years behind research. By the time Newell and Simon shared the 1975 Turing Award the feasibility of their approaches to AI was being increasingly challenged. The AI community would have to wait 19 years for another winner. It was displaced as the intellectual high ground of the emerging discipline by theoretical computer science, a field centered on mathematical analysis of algorithms, which garnered nine awardees during the same period. This new focus bolstered the intellectual respectability of computer science with a body of theory that was impeccably mathematical yet unlike numerical analysis, which was falling out of computer science over the same period, not directly useful to or understood by other scholars in established disciplines. The problems AI researchers had taken as their test cases were difficult in a fundamental mathematical sense that dashed hopes of ingenious breakthroughs. Once AI researchers applied the new techniques of complexity analysis “Everywhere they looked—in problem solving, game playing, planning, learning, reasoning—it seemed that the key problems where NP-complete (or worse).”22 Progress would come slowly and painfully, with methods that worked in some cases but not others. The early practitioners of AI had consistently and spectacularly overestimated the potential of their methods to replicate generalized human thought. In 1960, for example, Herb Simon had declared “within the near future—much less than 25 years—we shall have the technical capability of substituting machines for any and all human functions in organizations.” He believed the “problem-solving and information handling capabilities of the brain” would be duplicated “within the next decade.” As professionals were replaced by machines “a larger part of the working population will be mahouts and wheelbarrow pushers and a smaller part will be scientists and executives.”20 The same processes of hype that gave AI a high profile for military sponsors and awards committees also made the field a topic of public debate. Promises made for intelligent computers tapped into longer-established myths and science fiction stories of thinking machines and mechanical servants. HAL, the murderous computer from the movie 2001: A Space Odyssey, whose name was said to be a contraction of heuristic and algorithmic, was one of many fictional depictions of the promises made by AI researchers. Minsky himself had been hired to get the details right. Meanwhile a series of books appeared criticizing those promises and challenging the feasibility of artificial intelligence.10 The AI boosters were wrong, of course, though their critics were not always right. Computers had always been sold with futuristic hype, but overly optimistic technical predictions made during the same era for other areas of computer science such as graphics, computer mediated communication, scientific computation, and databases were eventually met and surpassed. In contrast, the approaches adopted by the AI community conspicuously failed to deliver on Simon’s central promises.  Military Origins of AI AI began as a Cold War project centered on a handful of well-connected researchers and institutions. The original Dartmouth meeting was funded by the Rockefeller Foundation. A full-scale research program would require deeper pockets, which in the 1950s were usually found attached to military uniforms. When Newell and Simon met and began to collaborate on their famous theorem prover both were employed by the RAND Corporation, a non-profit set up to support the U.S. Air Force. This gave them access not just to RAND’s JONNIAC computer, one of the first modern style computers operational in the U.S., but also to RAND programmer Clifford Shaw who was responsible for squeezing the ambitious program into the machine’s tiny memory.5 Rosenblatt developed his perceptrons, the most important of the early neural networks, in a university lab funded by the U.S. Navy. At MIT, Minsky’s early work benefitted from the largess of the Air Force and the Office of Naval Research. This deep entanglement of early AI with the U.S. military is difficult to overlook. Johnnie Penn highlighted the military dimension in his recent dissertation, challenging the phrase “good old fashioned AI” (a common description of traditional symbolic work) as something that “misrepresents and obscures this legacy as apolitical.”18 Yarden Katz insists the apparent technical discontinuities in the history of AI are just distractions from a consistent history of service to militarism, American imperialism, and capitalism.13 Yet AI was not exceptional in this respect. Military agencies supplied approximately 80% of all Federal research and development funding during the 1950s, the first decade of the Cold War. This wave of money flowed disproportionately to MIT and Stanford, which were not only the two leading centers for both AI and computer science but also the primary exemplars of a new, and to many disturbing, model for the relationship between universities, the Federal government, and military needs. Stuart W. Leslie’s history book The Cold War and American Science focused entirely on those two institutions as prototypes for a new kind of university restructured around military priorities.14 Computing was, after all, an expensive endeavor and there were few alternative sources of support. Set in the larger picture of military investment in computing, including projects such as the SAGE air defense network and guidance systems for Minuteman missile, the sums spent on AI appear quite small. Most computing projects of the 1940s and 1950s were underwritten directly or indirectly by the U.S. military.8 ENIAC, the first programmable electronic computer, was commissioned in 1943 by the U.S. Army for use by its Ballistics Research Laboratory.12 Such relationships blossomed as the Second World War gave way to the Cold War. IBM, for example, received more money during the 1950s from developing custom military systems such as the hardware for the SAGE air defense network than it did from selling its new lines of standard computer models. And even the market for those standard projects was driven by the Cold War. IBM’s first commercial computer model, the 701, was known informally as the “Defense Calculator” and sold almost entirely to government agencies and defense contractors. It was the Federal government, not IBM itself, that managed the delivery schedule for early models to maximize their contribution to national security.11 The needs of military and aerospace projects kickstarted the semiconductor industry in what became Silicon Valley. AI remained heavily dependent on military funding in the 1960s, as labs at MIT and Stanford received generous funding through the newly established Advanced Research Projects Agency. ARPA reportedly spent more on AI than the rest of the world put together, most of which went to MIT and Stanford. Carnegie Mellon was not initially in the same league, but its early success in computing and artificial intelligence won substantial ARPA funding by the 1970s and fueled the rise of the university itself. The National Science Foundation, a civilian agency, was less important. During the 1950s and 1960s it did not have a directorate focused on computer science. It made few grants to support computing research (though it was active in funding computing facilities).3 ARPA supported well-connected research groups without formal competitive review or any commitment to provide specific deliverables. J.C.R. Licklider, the first director of APRA’s Information Processing Techniques Office, joined ARPA from military contractor BBN and had earlier been a member of the MIT faculty. After showering MIT with government money he eventually rejoined its faculty, to run the ARPA-funded Project MAC (into which Minsky and his AI group had been incorporated). Licklider then returned to ARPA for a second term as director. That might all seem a little too cozy by modern standards, but ARPA’s early success in fostering emerging computer technologies was spectacular: not just the Internet, which can be traced back to an idea of Licklider’s, but also computer graphics and timesharing.17 Paul Edwards summarized the early history of AI in his classic The Closed World, arguing that under the influence of ARPA it became “part of the increasingly desperate, impossible tasks of enclosing the U.S. within an impenetrable bubble of high technology.” He believed Licklider’s vision for interactive computing was shaped fundamentally by military concerns with command and control.6 Were the founders of AI who worked at RAND or took money from the Pentagon thereby coopted into an imperialistic effort to project American power globally? Did their work somehow come to embed the culture of the military industrial complex? Historians will likely be arguing these questions for generations to come. AI, like cybernetics, unquestionably benefitted from a powerful alignment with a more general faith of scientific and political elites in abstraction, modelling, and what has been called by historians of science Cold War rationality.7 Personally, though, I am inclined to see the founders of AI as brilliant boondogglers who diverted a few buckets of money from of a tsunami of cold war spending to advance their quirky obsessions. Steven Levy noted that a “very determined solipsism reigned” among the hackers of Minsky’s lab at MIT, even as the antiwar protesters forced them to work behind locked doors and barricades. He quoted Minsky as claiming that Defense Department funding was less intellectually corrosive than hypothetical money from the Commerce Department or the Education Department.4,16 On the Stanford side, John McCarthy was a proponent of scientific internationalism. He was raised communist and made five visits to the USSR during the 1960s, though his politics drifted rightward in later decades.21 Philip Agre, recalling the investments by the military industrial complex in his graduate training at MIT, wrote that “if the field of AI during those decades was a servant of the military then it enjoyed a wildly indulgent master.”2  Summers and Winters When scientists write histories they usually focus on intellectual and technical accomplishments, leaving professional historians and science studies scholars to raise indelicate questions about the influence of money. In contrast, the insider story of AI as told by experts such as Wooldridge, Nils J. Nilsson and Margaret Boden has been structured explicitly around shifts in government funding.17 Why was AI so vulnerable to the whims of government agencies? One factor was the concentration of early AI work in a handful of well-connected labs. Another was the reliance of AI researchers on large and expensive computers. Perhaps the most important was the failure of AI, during its first few decades, to produce technologies with clear commercial potential that might attract a broader range of sponsors. The health of AI as a field thus depended on the ability of researchers to persuade deep-pocketed sponsors that spectacular success was just around the corner. Relying on a handful of funding sources proved hazardous. Machine translation projects were an early beneficiary of military largess, but this made them vulnerable when their feasibility was questioned. American funding for this area dried up after a damning report was issued in 1966 by the ALPAC committee, a scientific panel sponsored by the Department of Defense, the National Science Foundation, and the CIA to investigate progress in the area.19 The late 1980s are universally seen as the beginning of the “AI Winter,” in which faith and funding for AI dwindled dramatically. I will tell that story later, but in a fuzzier way the period from 1974 to 1980 has increasingly been described as an earlier winter for AI. This narrative blames the 1973 Lighthill Report, commissioned by the Science Research Council of the United Kingdom, for a collapse of British support for AI work. Across the Atlantic, this is said to have inspired other funders to ask more difficult questions.22 Sir James Lighthill was commissioned to write his report with the specific intent of justifying the withdrawal of funding for Donald Michie’s lab at Edinburgh, the most important center for AI research in the U.K. Lighthill, an eminent applied mathematician, endorsed both practical work on industrial automation and work to support analysis of brain functions (often called cognitive science) but opposed funding for ambitious work intended to unite the two in a new science of machine intelligence. Jon Agar’s analysis makes clear that the creative blurring of categories which let AI researchers spin narrow achievements into vast promises also left them vulnerable to attacks that challenged implied connections between specific computational techniques and general intelligence.1 How, and indeed whether, Lighthill’s attack on one controversial lab precipitated a broad international funding collapse for AI remains unclear. It coincided with broader changes in U.S. support for science. In 1973 Congress, inspired by Vietnam-era concerns that the military was gaining too much power over university research, passed legislation explicitly prohibiting ARPA from funding work that was not directly related to military needs. ARPA was renamed to DARPA, the D standing for Defense.17 As responsibility for basic research funding shifting increasingly to the NSF, DARPA funding required more direct military justification and came with more strings attached.","research, military, winter, work, early, mit, funding, communications, war, computer, acm, science, ai",2023-12-27 00:00:00
24,AI winter is well on its way,,https://blog.piekniewski.info/2018/05/28/ai-winter-is-well-on-its-way/,"Deep learning (does not) scaleOne of the key slogans repeated about deep learning is that it scales almost effortlessly. Initially it was thought that end-to-end deep learning could somehow solve this problem, a premise particularly heavily promoted by Nvidia. In such situation it is easy to learn spurious relations, as exemplified by adversarial examples in deep learning. For those who missed it, he has excellent blog posts/papers ""Deep learning: A critical appraisal"" and ""In defense of skepticism about deep learning"", where he very meticulously deconstructs the deep learning hype. I respect Gary a lot, he behaves like a real scientist should, while most so called ""deep learning stars"" just behave like cheap celebrities.","Deep learning has been at the forefront of the so called AI revolution for quite a few years now, and many people had believed that it is the silver bullet that will take us to the world of wonders of technological singularity (general AI). Many bets were made in 2014, 2015 and 2016 when still new boundaries were pushed, such as the Alpha Go etc. Companies such as Tesla were announcing through the mouths of their CEO's that fully self driving car was very close, to the point that Tesla even started selling that option to customers [to be enabled by future software update].  We have now mid 2018 and things have changed. Not on the surface yet, NIPS conference is still oversold, the corporate PR still has AI all over its press releases, Elon Musk still keeps promising self driving cars and Google CEO keeps repeating Andrew Ng's slogan that AI is bigger than electricity. But this narrative begins to crack. And as I predicted in my older post, the place where the cracks are most visible is autonomous driving - an actual application of the technology in the real world.  The dust settled on deep learning  When the ImageNet was effectively solved (note this does not mean that vision is solved), many prominent researchers in the field (including even typically quiet Geoff Hinton) were actively giving press interviews, publicizing stuff on social media (e.g. Yann Lecun, Andrew Ng, Fei-Fei Li to name a few). The general tone was that we are in front of a gigantic revolution and from now on things can only accelerate. Well years have passed and the twitter feeds of those people became less active, as exemplified by Andrew Ng below:  2013 - 0.413 tweets per day  2014 - 0.605 tweets per day  2015 - 0.320 tweets per day  2016 - 0.802 tweets per day  2017 - 0.668 tweets per day  2018 - 0.263 tweets per day (until 24 May)  Perhaps this is because Andrew's outrageous claims are now put to more scrutiny by the community as indicated in a tweet below:  Visibly the sentiment has quite considerably declined, there are much fewer tweets praising deep learning as the ultimate algorithm, the papers are becoming less ""revolutionary"" and much more ""evolutionary"". Deepmind hasn't shown anything breathtaking since their Alpha Go zero [and even that wasn't that exciting, given the obscene amount of compute necessary and applicability to games only - see Moravec's paradox]. OpenAI was rather quiet, with their last media outburst being the Dota 2 playing agent [which I suppose was meant to create as much buzz as Alpha Go, but fizzled out rather quickly]. In fact articles began showing up that even Google in fact does not know what to do with Deepmind, as their results are apparently not as practical as originally expected... As for the prominent researchers, they've been generally touring around meeting with government officials in Canada or France to secure their future grants, Yann Lecun even stepped down (rather symbolically) from the Head of Research to Chief AI scientist at Facebook. This gradual shift from rich, big corporations to government sponsored institutes suggests to me that the interest in this kind of research within these corporations (I think of Google and Facebook) is actually slowly winding down. Again these are all early signs, nothing spoken out loud, just the body language.  Deep learning (does not) scale  One of the key slogans repeated about deep learning is that it scales almost effortlessly. We had the AlexNet in 2012 which had ~60M parameters, we probably now have models with at least 1000x that number right? Well probably we do, the question however is - are these things 1000x as capable? Or even 100x as capable? A study by openAI comes in handy:  So in terms of applications for vision we see that VGG and Resnets saturated somewhat around one order of magnitude of compute resources applied (in terms of number of parameters it is actually less). Xception is a variation of google inception architecture and actually only slightly outperforms inception on ImageNet, arguably actually slightly outperforms everyone else, because essentially AlexNet solved ImageNet. So at 100 times more compute than AlexNet we pretty much saturated architectures in terms of vision, or image classification to be precise. Neural machine translation is a big effort by all the big web search players and no wonder it takes all the compute it can take (and yet google translate still sucks, though has gotten arguably better). The latest three points on that graph, interestingly show reinforcement learning related projects, applied to games by Deepmind and OpenAI. Particularly AlphaGo Zero and slightly more general AlphaZero take ridiculous amount of compute, but are not applicable in the real world applications because much of that compute is needed to simulate and generate the data these data hungry models need. OK, so we can now train AlexNet in minutes rather than days, but can we train a 1000x bigger AlexNet in days and get qualitatively better results? Apparently not...  So in fact, this graph which was meant to show how well deep learning scales, indicates the exact opposite. We can't just scale up AlexNet and get respectively better results - we have to fiddle with specific architectures, and effectively additional compute does not buy much without order of magnitude more data samples, which are in practice only available in simulated game environments.  Self driving crashes  By far the biggest blow into deep learning fame is the domain of self driving vehicles (something I anticipated for a long time, see e.g. this post from 2016). Initially it was thought that end-to-end deep learning could somehow solve this problem, a premise particularly heavily promoted by Nvidia. I don't think there is a single person on Earth who still believes that, though I could be wrong. Looking at last year California DMV disengagement reports Nvidia car could not drive literally ten miles without a disengagement. In a separate post I discuss the general state of that development and comparison to human driver safety, which (spoiler alert) is not looking good. Since 2016 there were also several Tesla AutoPilot incidents [1, 2, 3] a few of which were fatal [1, 2]. Arguably Tesla Autopilot should not be confused with self driving (it is), but at least at the core it relies on the same kind of technology. As of today, aside from occasional spectacular errors [1], it still cannot stop at an intersection, recognize a traffic light, or even navigate through a roundabout. That is in May 2018, several months after the promised coast to coast Tesla autonomous drive (did not happen although the rumor is they've tried but could not get it to work without ~30 disengagements). Several months ago (in February 2018) Elon Musk repeated in a conference call when asked about the coast to coast drive that:  ""We could have done the coast-to-coast drive, but it would have required too much specialized code to effectively game it or make it somewhat brittle and that it would work for one particular route, but not the general solution. So I think we would be able to repeat it, but if it’s just not any other route, which is not really a true solution. (…)  I am pretty excited about how much progress we’re making on the neural net front. And it’s a little – it’s also one of those things that’s kind of exponential where the progress doesn’t seem – it doesn’t seem like much progress, it doesn’t seem like much progress, and suddenly wow. It will feel like well this is a lame driver, lame driver. Like okay, that’s a pretty good driver. Like ‘Holy Cow!’ this driver’s good. It’ll be like that,”  Well, looking at the graph above (from OpenAI) I seem not be seeing that exponential progress. Neither is it visible in miles before disengagement for pretty much every big player in this field. In essence the above statement should be interpreted: ""We currently don't have the technology that could safely drive us coast to coast, though we could have faked it if we really wanted to (maybe...). We deeply hope that some sort of exponential jump in capabilities of neural networks will soon happen and save us from disgrace and massive lawsuits"".  But by far the biggest pin punching through the AI bubble was the accident in which Uber self driving car killed a pedestrian in Arizona. From the preliminary report by the NTSB we can read some astonishing statements:  Aside from general system design failure apparent in this report, it is striking that the system spent long seconds trying to decide what exactly it sees in front (whether that be a pedestrian, bike, vehicle or whatever else) rather than making the only logical decision in these circumstances, which was to make sure not to hit it. There are several reasons for it: first, people will often verbalize their decisions post factum. So a human will typically say: ""I saw a cyclist therefore I veered to the left to avoid him"". Huge amount of psychophysical literature will suggest a quite different explanation: a human saw something which was very quickly interpreted as a obstacle by fast perceptual loops of his nervous systems and he performed a rapid action to avoid it, long seconds later realizing what happened and providing a verbal explanation"". There are tons of decisions we make every day that are not verbalized, and driving includes many of them. Verbalization is costly and takes time and reality often does not provide that time. These mechanisms have evolved for a billion years to keep us safe, and driving context (although modern) makes use of many such reflexes. And since these reflexes have not evolved specifically for driving, they may induce mistakes. A knee jerk reaction to a wasp buzzing in a car may have caused many crashes and deaths. But our general understanding of 3d space, speed, ability to predict the behavior of agents, behavior of physical objects traversing through our path are the primitive skills, that were just as useful 100 million years ago as they are today and they've been honed really sharp by evolution.  But because most of these things are not easily verbalizable, they are hard to measure, and consequently we don't optimize our machine learning systems on these aspects at all [see my earlier post for benchmark proposals that would address some of these capabilities]. Now this would speak in favor of Nvidia end-to-end approach - learn image->action mapping, skipping any verbalization, and in some ways this is the right way to do it but... the problem is that the input space is incredibly high dimensional, while the action space is very low dimensional. Hence the ""amount"" of ""label"" (readout) is extremely small compared to the amount of information coming in. In such situation it is easy to learn spurious relations, as exemplified by adversarial examples in deep learning. A different paradigm is needed and I postulate prediction of the entire perceptual input along with the action as a first step to make a system able to extract the semantics of the world, rather than spurious correlations [read more about my first proposed architecture called Predictive Vision Model].  In fact if there is anything at all we learned from the outburst of deep learning, is that (10k+ dimensional) image space has plenty enough spurious patterns in it, that they actually generalize across many images and make the impression like our classifiers actually understand what they are seeing. Nothing could be further from the truth, as admitted even by the top researchers who are heavily invested in this field. In fact many top researchers should not be too outraged by my observations, Yann Lecun warned about overexcitement and AI winter for a while, even Geoffrey Hinton - the father of the current outburst of backpropagation - admitted in an interview that this likely is all a dead end, and we need to start over. At this point though, the hype is so strong that nobody will listen, even to the founding fathers of the field.  Gary Marcus and his quest against the hype  I should mention that more top tier people are recognizing the hubris and have the courage to openly call it. One of the most active in that space is Gary Marcus. Although I don't think I agree with everything that Gary proposes in terms of AI, we certainly agree that it is not yet as powerful as painted by the deep learning hype-propaganda. In fact it is not even any close. For those who missed it, he has excellent blog posts/papers ""Deep learning: A critical appraisal"" and ""In defense of skepticism about deep learning"", where he very meticulously deconstructs the deep learning hype. I respect Gary a lot, he behaves like a real scientist should, while most so called ""deep learning stars"" just behave like cheap celebrities.  Conclusion  Predicting the A.I. winter is like predicting a stock market crash - impossible to tell precisely when it happens, but almost certain that it will at some point. Much like before a stock market crash, there are signs of the impending collapse, but the narrative is so strong that it is very easy to ignore them, even if they are in plain sight. In my opinion there are such signs of a huge decline in deep learning (and probably in AI in general as this term has been abused 'ad nauseam' by corporate propaganda) already visible. Visible in plain sight, yet hidden from the majority by an increasingly intense narrative. How ""deep"" will this winter be? I have no idea. What will come next? I have no idea. But I'm fairly positive it is coming, perhaps sooner rather than later.  If you found an error, highlight it and press Shift + Enter or click here to inform us.  Related  Comments  comments","driving, tweets, winter, general, coast, compute, way, alexnet, fact, learning, ai, deep",2018-05-28 00:00:00
25,Home,,https://github.com/kreeben/resin/wiki/The-database-bubble-and-the-AI-winter,"Saved searches Use saved searches to filter your results more quicklyWe read every piece of feedback, and take your input very seriously. You signed in with another tab or window. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window.","To see all available qualifiers, see our documentation .  Saved searches Use saved searches to filter your results more quickly  We read every piece of feedback, and take your input very seriously.  You signed in with another tab or window. Reload to refresh your session.  You signed out in another tab or window. Reload to refresh your session.  You switched accounts on another tab or window. Reload to refresh your session.  Dismiss alert","sessionyou, sessiondismiss, searches, refresh, saved, reload, window, tab, signed, switched",
26,AI winter,,http://en.wikipedia.org/wiki/AI_winter,"Period of reduced funding and interest in AI researchIn the history of artificial intelligence, an AI winter is a period of reduced funding and interest in artificial intelligence research. Roger Schank and Marvin Minsky—two leading AI researchers who experienced the ""winter"" of the 1970s—warned the business community that enthusiasm for AI had spiraled out of control in the 1980s and that disappointment would certainly follow. [6]Just like all AI booms that have been followed by desperate AI winters, the media tended to exaggerate the significance of these developments. ""[25] The result, Moravec claims, is that some of the staff at DARPA had lost patience with AI research. In 2006, Nick Bostrom explained that ""a lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.","Period of reduced funding and interest in AI research  In the history of artificial intelligence, an AI winter is a period of reduced funding and interest in artificial intelligence research.[1] The field has experienced several hype cycles, followed by disappointment and criticism, followed by funding cuts, followed by renewed interest years or even decades later.  The term first appeared in 1984 as the topic of a public debate at the annual meeting of AAAI (then called the ""American Association of Artificial Intelligence""). Roger Schank and Marvin Minsky—two leading AI researchers who experienced the ""winter"" of the 1970s—warned the business community that enthusiasm for AI had spiraled out of control in the 1980s and that disappointment would certainly follow. They described a chain reaction, similar to a ""nuclear winter"", that would begin with pessimism in the AI community, followed by pessimism in the press, followed by a severe cutback in funding, followed by the end of serious research. Three years later the billion-dollar AI industry began to collapse.  There were two major winters approximately 1974–1980 and 1987–2000,[3] and several smaller episodes, including the following:  Enthusiasm and optimism about AI has generally increased since its low point in the early 1990s. Beginning about 2012, interest in artificial intelligence (and especially the sub-field of machine learning) from the research and corporate communities led to a dramatic increase in funding and investment, leading to the current (as of 2024 ) AI boom.  Early episodes [ edit ]  Machine translation and the ALPAC report of 1966 [ edit ]  Natural language processing (NLP) research has its roots in the early 1930s and began its existence with the work on machine translation (MT).[4] However, significant advancements and applications began to emerge after the publication of Warren Weaver's influential memorandum in 1949.[5] The memorandum generated great excitement within the research community. In the following years, notable events unfolded: IBM embarked on the development of the first machine, MIT appointed its first full-time professor in machine translation, and several conferences dedicated to MT took place. The culmination came with the public demonstration of the IBM-Georgetown machine, which garnered widespread attention in respected newspapers in 1954.[6]  Just like all AI booms that have been followed by desperate AI winters, the media tended to exaggerate the significance of these developments. Headlines about the IBM-Georgetown experiment proclaimed phrases like ""The bilingual machine,"" ""Robot brain translates Russian into King's English,""[7] and ""Polyglot brainchild.""[8] However, the actual demonstration involved the translation of a curated set of only 49 Russian sentences into English, with the machine's vocabulary limited to just 250 words.[6] To put things into perspective, a 2006 study made by Paul Nation found that humans need a vocabulary of around 8,000 to 9,000-word families to comprehend written texts with 98% accuracy.[9]  During the Cold War, the US government was particularly interested in the automatic, instant translation of Russian documents and scientific reports. The government aggressively supported efforts at machine translation starting in 1954. Another factor that propelled the field of mechanical translation was the interest shown by the Central Intelligence Agency (CIA). During that period, the CIA firmly believed in the importance of developing machine translation capabilities and supported such initiatives. They also recognized that this program had implications that extended beyond the interests of the CIA and the intelligence community.[6]  At the outset, the researchers were optimistic. Noam Chomsky's new work in grammar was streamlining the translation process and there were ""many predictions of imminent 'breakthroughs'"".[10]  Briefing for US Vice President Gerald Ford in 1973 on the junction-grammar-based computer translation model  However, researchers had underestimated the profound difficulty of word-sense disambiguation. In order to translate a sentence, a machine needed to have some idea what the sentence was about, otherwise it made mistakes. An apocryphal[11] example is ""the spirit is willing but the flesh is weak."" Translated back and forth with Russian, it became ""the vodka is good but the meat is rotten."" Later researchers would call this the commonsense knowledge problem.  By 1964, the National Research Council had become concerned about the lack of progress and formed the Automatic Language Processing Advisory Committee (ALPAC) to look into the problem. They concluded, in a famous 1966 report, that machine translation was more expensive, less accurate and slower than human translation. After spending some 20 million dollars, the NRC ended all support. Careers were destroyed and research ended.[10]  Machine translation shared the same path with NLP from the rule-based approaches through the statistical approaches up to the neural network approaches, which have in 2023 culminated in large language models.  The failure of single-layer neural networks in 1969 [ edit ]  Simple networks or circuits of connected units, including Walter Pitts and Warren McCulloch's neural network for logic and Marvin Minsky's SNARC system, have failed to deliver the promised results and were abandoned in the late 1950s. Following the success of programs such as the Logic Theorist and the General Problem Solver,[13] algorithms for manipulating symbols seemed more promising at the time as means to achieve logical reasoning viewed at the time as the essence of intelligence, either natural or artificial.  Interest in perceptrons, invented by Frank Rosenblatt, was kept alive only by the sheer force of his personality.[14] He optimistically predicted that the perceptron ""may eventually be able to learn, make decisions, and translate languages"".[15] Mainstream research into perceptrons ended partially because the 1969 book Perceptrons by Marvin Minsky and Seymour Papert emphasized the limits of what perceptrons could do. While it was already known that multilayered perceptrons are not subject to the criticism, nobody in the 1960s knew how to train a multilayered perceptron. Backpropagation was still years away.[17]  Major funding for projects neural network approaches was difficult to find in the 1970s and early 1980s.[18] Important theoretical work continued despite the lack of funding. The ""winter"" of neural network approach came to an end in the middle 1980s, when the work of John Hopfield, David Rumelhart and others revived large scale interest.[19] Rosenblatt did not live to see this, however, as he died in a boating accident shortly after Perceptrons was published.[15]  The setbacks of 1974 [ edit ]  The Lighthill report [ edit ]  In 1973, professor Sir James Lighthill was asked by the UK Parliament to evaluate the state of AI research in the United Kingdom. His report, now called the Lighthill report, criticized the utter failure of AI to achieve its ""grandiose objectives"". He concluded that nothing being done in AI could not be done in other sciences. He specifically mentioned the problem of ""combinatorial explosion"" or ""intractability"", which implied that many of AI's most successful algorithms would grind to a halt on real world problems and were only suitable for solving ""toy"" versions.[20]  The report was contested in a debate broadcast in the BBC ""Controversy"" series in 1973. The debate ""The general purpose robot is a mirage"" from the Royal Institution was Lighthill versus the team of Donald Michie, John McCarthy and Richard Gregory.[21] McCarthy later wrote that ""the combinatorial explosion problem has been recognized in AI from the beginning"".[22]  The report led to the complete dismantling of AI research in the UK.[20] AI research continued in only a few universities (Edinburgh, Essex and Sussex). Research would not revive on a large scale until 1983, when Alvey (a research project of the British Government) began to fund AI again from a war chest of £350 million in response to the Japanese Fifth Generation Project (see below). Alvey had a number of UK-only requirements which did not sit well internationally, especially with US partners, and lost Phase 2 funding.  DARPA's early 1970s funding cuts [ edit ]  During the 1960s, the Defense Advanced Research Projects Agency (then known as ""ARPA"", now known as ""DARPA"") provided millions of dollars for AI research with few strings attached. J. C. R. Licklider, the founding director of DARPA's computing division, believed in ""funding people, not projects""[23] and he and several successors allowed AI's leaders (such as Marvin Minsky, John McCarthy, Herbert A. Simon or Allen Newell) to spend it almost any way they liked.  This attitude changed after the passage of Mansfield Amendment in 1969, which required DARPA to fund ""mission-oriented direct research, rather than basic undirected research"".[24] Pure undirected research of the kind that had gone on in the 1960s would no longer be funded by DARPA. Researchers now had to show that their work would soon produce some useful military technology. AI research proposals were held to a very high standard. The situation was not helped when the Lighthill report and DARPA's own study (the American Study Group) suggested that most AI research was unlikely to produce anything truly useful in the foreseeable future. DARPA's money was directed at specific projects with identifiable goals, such as autonomous tanks and battle management systems. By 1974, funding for AI projects was hard to find.[24]  AI researcher Hans Moravec blamed the crisis on the unrealistic predictions of his colleagues: ""Many researchers were caught up in a web of increasing exaggeration. Their initial promises to DARPA had been much too optimistic. Of course, what they delivered stopped considerably short of that. But they felt they couldn't in their next proposal promise less than in the first one, so they promised more.""[25] The result, Moravec claims, is that some of the staff at DARPA had lost patience with AI research. ""It was literally phrased at DARPA that 'some of these people were going to be taught a lesson [by] having their two-million-dollar-a-year contracts cut to almost nothing!'"" Moravec told Daniel Crevier.[26]  While the autonomous tank project was a failure, the battle management system (the Dynamic Analysis and Replanning Tool) proved to be enormously successful, saving billions in the first Gulf War, repaying all of DARPAs investment in AI[27] and justifying DARPA's pragmatic policy.[28]  The SUR debacle [ edit ]  As described in:[29]  In 1971, the Defense Advanced Research Projects Agency (DARPA) began an ambitious five-year experiment in speech understanding. The goals of the project were to provide recognition of utterances from a limited vocabulary in near-real time. Three organizations finally demonstrated systems at the conclusion of the project in 1976. These were Carnegie-Mellon University (CMU), who actually demonstrated two system [HEARSAY-II and HARPY]; Bolt, Beranek and Newman (BBN); and System Development Corporation with Stanford Research Institute (SDC/SRI)  The system that came closest to satisfying the original project goals was the CMU HARPY system. The relatively high performance of the HARPY system was largely achieved through 'hard-wiring' information about possible utterances into the system's knowledge base. Although HARPY made some interesting contributions, its dependence on extensive pre-knowledge limited the applicability of the approach to other signal-understanding tasks.  DARPA was deeply disappointed with researchers working on the Speech Understanding Research program at Carnegie Mellon University. DARPA had hoped for, and felt it had been promised, a system that could respond to voice commands from a pilot. The SUR team had developed a system which could recognize spoken English, but only if the words were spoken in a particular order. DARPA felt it had been duped and, in 1974, they cancelled a three million dollar a year contract.[30]  Many years later, several successful commercial speech recognition systems would use the technology developed by the Carnegie Mellon team (such as hidden Markov models) and the market for speech recognition systems would reach $4 billion by 2001.[31]  For a description of Hearsay-II see Hearsay-II, The Hearsay-II Speech Understanding System: Integrating Knowledge to Resolve Uncertainty and A Retrospective View of the Hearsay-II Architecture which appear in Blackboard Systems[32]  Reddy gives a review of progress in speech understanding at the end of the DARPA project in a 1976 article in Proceedings of the IEEE.[33]  Contrary view [ edit ]  Thomas Haigh argues that activity in the domain of AI did not slow down, even as funding from DoD was being redirected, mostly in the wake of congressional legislation meant to separate military and academic activities.[34] That indeed professional interest was growing throughout the 70s. Using the membership count of ACM's SIGART, the Special Interest Group on Artificial Intelligence, as a proxy for interest in the subject, the author writes:[34]  (...) I located two data sources, neither of which supports the idea of a broadly based AI winter during the 1970s. One is membership of ACM's SIGART, the major venue for sharing news and research abstracts during the 1970s. When the Lighthill report was published in 1973 the fast-growing group had 1,241 members, approximately twice the level in 1969. The next five years are conventionally thought of as the darkest part of the first AI winter. Was the AI community shrinking? No! By mid-1978 SIGART membership had almost tripled, to 3,500. Not only was the group growing faster than ever, it was increasing proportionally faster than ACM as a whole which had begun to plateau (expanding by less than 50% over the entire period from 1969 to 1978). One in every 11 ACM members was in SIGART.  The setbacks of the late 1980s and early 1990s [ edit ]  The collapse of the LISP machine market [ edit ]  In the 1980s, a form of AI program called an ""expert system"" was adopted by corporations around the world. The first commercial expert system was XCON, developed at Carnegie Mellon for Digital Equipment Corporation, and it was an enormous success: it was estimated to have saved the company 40 million dollars over just six years of operation. Corporations around the world began to develop and deploy expert systems and by 1985 they were spending over a billion dollars on AI, most of it to in-house AI departments. An industry grew up to support them, including software companies like Teknowledge and Intellicorp (KEE), and hardware companies like Symbolics and LISP Machines Inc. who built specialized computers, called LISP machines, that were optimized to process the programming language LISP, the preferred language for AI research in the USA.[35][36]  In 1987, three years after Minsky and Schank's prediction, the market for specialized LISP-based AI hardware collapsed. Workstations by companies like Sun Microsystems offered a powerful alternative to LISP machines and companies like Lucid offered a LISP environment for this new class of workstations. The performance of these general workstations became an increasingly difficult challenge for LISP Machines. Companies like Lucid and Franz LISP offered increasingly powerful versions of LISP that were portable to all UNIX systems. For example, benchmarks were published showing workstations maintaining a performance advantage over LISP machines.[37] Later desktop computers built by Apple and IBM would also offer a simpler and more popular architecture to run LISP applications on. By 1987, some of them had become as powerful as the more expensive LISP machines. The desktop computers had rule-based engines such as CLIPS available.[38] These alternatives left consumers with no reason to buy an expensive machine specialized for running LISP. An entire industry worth half a billion dollars was replaced in a single year.[39]  By the early 1990s, most commercial LISP companies had failed, including Symbolics, LISP Machines Inc., Lucid Inc., etc. Other companies, like Texas Instruments and Xerox, abandoned the field. A small number of customer companies (that is, companies using systems written in LISP and developed on LISP machine platforms) continued to maintain systems. In some cases, this maintenance involved the assumption of the resulting support work.[40]  Slowdown in deployment of expert systems [ edit ]  By the early 1990s, the earliest successful expert systems, such as XCON, proved too expensive to maintain. They were difficult to update, they could not learn, they were ""brittle"" (i.e., they could make grotesque mistakes when given unusual inputs), and they fell prey to problems (such as the qualification problem) that had been identified years earlier in research in nonmonotonic logic. Expert systems proved useful, but only in a few special contexts.[41][42] Another problem dealt with the computational hardness of truth maintenance efforts for general knowledge. KEE used an assumption-based approach supporting multiple-world scenarios that was difficult to understand and apply.  The few remaining expert system shell companies were eventually forced to downsize and search for new markets and software paradigms, like case-based reasoning or universal database access. The maturation of Common Lisp saved many systems such as ICAD which found application in knowledge-based engineering. Other systems, such as Intellicorp's KEE, moved from LISP to a C++ (variant) on the PC and helped establish object-oriented technology (including providing major support for the development of UML (see UML Partners).  The end of the Fifth Generation project [ edit ]  In 1981, the Japanese Ministry of International Trade and Industry set aside $850 million for the Fifth Generation computer project. Their objectives were to write programs and build machines that could carry on conversations, translate languages, interpret pictures, and reason like human beings. By 1991, the impressive list of goals penned in 1981 had not been met. According to HP Newquist in The Brain Makers, ""On June 1, 1992, The Fifth Generation Project ended not with a successful roar, but with a whimper.""[40] As with other AI projects, expectations had run much higher than what was actually possible.[43][44]  Strategic Computing Initiative cutbacks [ edit ]  In 1983, in response to the fifth generation project, DARPA again began to fund AI research through the Strategic Computing Initiative. As originally proposed the project would begin with practical, achievable goals, which even included artificial general intelligence as long-term objective. The program was under the direction of the Information Processing Technology Office (IPTO) and was also directed at supercomputing and microelectronics. By 1985 it had spent $100 million and 92 projects were underway at 60 institutions, half in industry, half in universities and government labs. AI research was well-funded by the SCI.[45]  Jack Schwarz, who ascended to the leadership of IPTO in 1987, dismissed expert systems as ""clever programming"" and cut funding to AI ""deeply and brutally"", ""eviscerating"" SCI. Schwarz felt that DARPA should focus its funding only on those technologies which showed the most promise, in his words, DARPA should ""surf"", rather than ""dog paddle"", and he felt strongly AI was not ""the next wave"". Insiders in the program cited problems in communication, organization and integration. A few projects survived the funding cuts, including pilot's assistant and an autonomous land vehicle (which were never delivered) and the DART battle management system, which (as noted above) was successful.[46]  AI winter of the 1990's and early 2000's [ edit ]  A survey of reports from the early 2000's suggests that AI's reputation was still poor:  Alex Castro, quoted in The Economist , 7 June 2007: ""[Investors] were put off by the term 'voice recognition' which, like 'artificial intelligence', is associated with systems that have all too often failed to live up to their promises."" [ 47 ]  , 7 June 2007: ""[Investors] were put off by the term 'voice recognition' which, like 'artificial intelligence', is associated with systems that have all too often failed to live up to their promises."" Patty Tascarella in Pittsburgh Business Times , 2006: ""Some believe the word 'robotics' actually carries a stigma that hurts a company's chances at funding."" [ 48 ]  , 2006: ""Some believe the word 'robotics' actually carries a stigma that hurts a company's chances at funding."" John Markoff in the New York Times, 2005: ""At its low point, some computer scientists and software engineers avoided the term artificial intelligence for fear of being viewed as wild-eyed dreamers."" [ 49 ]  Many researchers in AI in the mid 2000's deliberately called their work by other names, such as informatics, machine learning, analytics, knowledge-based systems, business rules management, cognitive systems, intelligent systems, intelligent agents or computational intelligence, to indicate that their work emphasizes particular tools or is directed at a particular sub-problem. Although this may be partly because they consider their field to be fundamentally different from AI, it is also true that the new names help to procure funding by avoiding the stigma of false promises attached to the name ""artificial intelligence"".[49][50]  In the late 1990's and early 21st century, AI technology became widely used as elements of larger systems,[51] but the field is rarely credited for these successes. In 2006, Nick Bostrom explained that ""a lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.""[53] Rodney Brooks stated around the same time that ""there's this stupid myth out there that AI has failed, but AI is around you every second of the day.""  Current AI spring (2022–present) [ edit ]  AI has reached the highest levels of interest and funding in its history in the past few years by every possible measure, including: publications, patent applications,[56] total investment ($50 billion in 2022), and job openings (800,000 U.S. job openings in 2022). The successes of the current ""AI spring"" or ""AI boom"" are advances in language translation (in particular, Google Translate), image recognition (spurred by the ImageNet training database) as commercialized by Google Image Search, and in game-playing systems such as AlphaZero (chess champion) and AlphaGo (go champion), and Watson (Jeopardy champion). A turning point was in 2012 when AlexNet (a deep learning network) won the ImageNet Large Scale Visual Recognition Challenge with half as many errors as the second place winner.  The 2022 release of OpenAI's AI chatbot ChatGPT which as of January 2023 has over 100 million users,[60] has reinvigorated the discussion about artificial intelligence and its effects on the world.[61][62]  See also [ edit ]  Notes [ edit ]  References [ edit ]","research, translation, lisp, winter, funding, system, systems, intelligence, machine, ai, edit",
27,"The AI Winter Is Coming In 2024, A Top Scientist Predicts","Tom Hale, Senior Journalist, Maddy Chapman, Editor",https://www.iflscience.com/the-ai-winter-is-coming-in-2024-a-top-scientist-predicts-72352,"2023 was the year when the hype around artificial intelligence (AI) went into hyperdrive. Advertisement AdvertisementRodney Brooks believes so. Brooks is a former director of the Computer Science and Artificial Intelligence Laboratory at MIT who regularly comments on technology's progress (or lack thereof). There may be yet another AI winter, and perhaps even a full-scale tech winter, just around the corner. While he believes these AI systems are capable of some impressive feats, he thinks they don’t have the capability to become an all-powerful, earth-shattering Artificial General Intelligence.","2023 was the year when the hype around artificial intelligence (AI) went into hyperdrive. Following its release in late 2022, ChatGPT-3 made AI technology accessible and genuinely useful to the general public, prompting the development of numerous other Large Language Models (LLM) by some of Silicon Valley’s mightiest giants. AI was the word on everyone’s lips last year, but could it be set to enter a period of stagnation?  Advertisement Advertisement  Rodney Brooks believes so. Brooks is a former director of the Computer Science and Artificial Intelligence Laboratory at MIT who regularly comments on technology's progress (or lack thereof). Since 2018, he has posted his predictions of self-driving cars, human space travel, and – last but not least – robotics, AI, and machine learning. He’s promised to keep making the forecasts each year until 2050 when he’ll turn 95.  In his latest scorecard, Brooks predicted that 2024 won’t be a golden age for AI, noting that the current fanfare is “following a well worn hype cycle that we have seen again, and again, during the 60+ year history of AI.”  “Get your thick coats now. There may be yet another AI winter, and perhaps even a full-scale tech winter, just around the corner. And it is going to be cold,” Brooks concluded.  Brooks is far from a pessimistic Luddite. He’s been studying AI since the 1970s and has been dubbed “one of the world’s most accomplished experts in robotics and artificial intelligence.” If he seems cynical, it’s simply because he’s seen it all before; all the publicity, letdowns, false promises, and setbacks. Take a look at his former predictions and you’ll see his technological prophecies are often right on the money.  When talking about AI in his 2024 scorecard, Brooks is referring to LLMs, chatbot systems like ChatGPT, and others made by the likes of Bing and Google’s Deep Mind. While he believes these AI systems are capable of some impressive feats, he thinks they don’t have the capability to become an all-powerful, earth-shattering Artificial General Intelligence. In his mind, these systems lack true imagination and genuine substance.  Advertisement Advertisement  “[I encourage] people to do good things with LLMs but to not believe the conceit that their existence means we are on the verge of Artificial General Intelligence,” Brooks added.  “There is much more to life than LLMs.”  Speaking in an interview with IEEE Spectrum, Brooks goes deeper into his criticism, explaining how advanced LLMs still make regular mistakes when tasked with relatively simple coding tasks.  “It answers with such confidence any question I ask. It gives an answer with complete confidence, and I sort of believe it. And half the time, it’s completely wrong. And I spend 2 or 3 hours using that hint, and then I say, ‘That didn’t work,’ and it just does this other thing. Now, that’s not the same as intelligence. It’s not the same as interacting. It’s looking it up,” said Brooks.  Advertisement Advertisement  Ultimately, he believes, LLMs have a long way to go before they can be considered anything like a fully-fledged Artificial General Intelligence because they are merely clever wordsmiths, not uber-intelligent beings. If his musings are accurate, the same could be true of GPT-5, GPT-6, and beyond.  “It doesn’t have any underlying model of the world. It doesn’t have any connection to the world. It is correlation between language,” Brooks explained.  “What the large language models are good at is saying what an answer should sound like, which is different from what an answer should be,” he added.","coming, winter, scientist, artificial, general, llms, brooks, 2024, believes, intelligence, systems, predicts, language, hes, ai",2024-01-08 17:29:08+00:00
28,Probability of an Approaching AI Winter,Sebastian Schuchmann,https://towardsdatascience.com/probability-of-an-approaching-ai-winter-c2d818fb338a,"Probability of an Approaching AI WinterThis article addresses the question of whether the field of Artificial Intelligence (AI) is approaching another AI winter or not. If another AI winter were to come about many people could lose their jobs, and many startups might have to shut down, as has happened before. Moreover, the economic difference between an approaching winter period or ongoing success is estimated to be at least tens of billions of dollars by 2025, according to McKinsey & Company. For a detailed overview of both AI winters check out my first and second medium article on the topic. In this section, the central causes of the AI winters are extracted from the above discussion of previous winters.","Probability of an Approaching AI Winter  This article addresses the question of whether the field of Artificial Intelligence (AI) is approaching another AI winter or not. Sebastian Schuchmann · Follow Published in Towards Data Science · 18 min read · Aug 17, 2019 -- Share  Motivation  Both industries and governments alike have invested significantly in the AI field, with many AI-related startups established in the last 5 years. If another AI winter were to come about many people could lose their jobs, and many startups might have to shut down, as has happened before. Moreover, the economic difference between an approaching winter period or ongoing success is estimated to be at least tens of billions of dollars by 2025, according to McKinsey & Company.  This paper does not aim to discuss whether progress in AI is to be desired or not. Instead, the purpose of the discussions and results presented herein is to to inform the reader of how likely progress in AI research is.  Analysis: What has Led to the AI Winters?  For a detailed overview of both AI winters check out my first and second medium article on the topic.  In this section, the central causes of the AI winters are extracted from the above discussion of previous winters.  First, a recurring pattern that can be observed are that promises that kindled initial excitement, but which later turned out to be inflated have been the leading cause for the AI winters. For…","winter, winters, field, probability, progress, approaching, wintersfor, winterthis, article, startups, ai",2019-08-17 19:22:23.241000+00:00
29,Business Process Automation Software,Parker Software,https://www.thinkautomation.com/bots-and-ai/the-ai-winter-is-coming/,"ThinkAutomation is a business process automation software solution for businesses of all sizes. Automate on-premises and cloud-based business processes to cut costs and save time. Automate incoming communication channels, monitor databases, react to incoming webhooks, web forms and AI powered chat bots. Parse and extract data from incoming messages and perform business process actions, such as outbound communications, updating on-premises and cloud databases, CRM systems and cloud services, document processing, ChatGPT automation, systems integration and much more. With ThinkAutomation, you get an open-ended studio to build your automated workflows without volume limitations, and without paying per process, workflow or 'bot'.","ThinkAutomation is a business process automation software solution for businesses of all sizes. Automate on-premises and cloud-based business processes to cut costs and save time.    Automate incoming communication channels, monitor databases, react to incoming webhooks, web forms and AI powered chat bots. Process documents, attachments, local files and other messages sources.    Parse and extract data from incoming messages and perform business process actions, such as outbound communications, updating on-premises and cloud databases, CRM systems and cloud services, document processing, ChatGPT automation, systems integration and much more.  With ThinkAutomation, you get an open-ended studio to build your automated workflows without volume limitations, and without paying per process, workflow or 'bot'.","automation, business, incoming, thinkautomation, messages, software, process, databases, systems, cloud, onpremises",
30,AI winter - update,,https://blog.piekniewski.info/2018/10/29/ai-winter-update/,"IntroductionAlmost six months ago (May 28th 2018) I posted the ""AI winter is well on its way"" post that went viral. First of all a bit of clarification: some readers have misinterpreted my claims, in that I predicted that the AI hype is declining. Andrew Ng is a rare example of a person who jumped from an academic bubble into an even bigger AI hype bubble. I'm pretty certain that following Hotz's lead, many of today's AI hype blowers will be screaming how they've been warning about AI winter all along, once the bubble bursts. Musk reiterated that he believes in the self driving Tesla fleet however full self driving remains ""off menu"" as it was too confusing (two years after introduction of the item?!","Introduction  Almost six months ago (May 28th 2018) I posted the ""AI winter is well on its way"" post that went viral. The post amassed nearly a quarter million views and got picked up in Bloomberg, Forbes, Politico, Venturebeat, BBC, Datascience Podcast and numerous other smaller media outlets and blogs [1, 2, 3, 4, ...], triggered violent debate on Hacker news and Reddit. I could not have anticipated this post to be so successful and hence I realized I touched on a very sensitive subject. One can agree with my claims or not, but the sheer popularity of the post almost itself serves as a proof that something is going on behind the scenes and people are actually curious and doubtful if there is anything solid behind the AI hype.  Since the post made a prediction, that the AI hype is cracking (particularly in the space of autonomous vehicles) and as a result we will have another ""AI winter"" episode, I decided to periodically go over those claims, see what has changed and bring some new evidence.  First of all a bit of clarification: some readers have misinterpreted my claims, in that I predicted that the AI hype is declining. In fact to the contrary, I started my original post by stating that on the surface still everything looks great. And that is still the case. The NIPS conference sold out in a matter of minutes, surprising even the biggest enthusiasts. In fact some known researchers have apparently missed out and will not be going (probably the most interesting part of this conference this year will be the whole drama with its name anyway). This does not contradict anything, hype is a lagging indicator of what is actually going on. I discussed that, as well as other immediate feedback in ""AI winter addendum"". Let's review what happened over that last several months.  Updates  The twitter activity of many of the prominent researchers I follow was rather sparse over the period in question. Dr Fei Fei Li gave testimony in front of congress, with a highlight in the tweet below (and a hilarious reply):  I'm not sure how long people can go for spreading such utter nonsense without any harm to their reputation. I suppose for quite a while, but hopefully not forever.  Andrew Ng, one of my favorite ""enthusiasts"" is busy building his swarm of startup companies, but luckily found the time to give a short interview to Fortune, in which he gloats about how he singlehandedly transformed Google and Baidu into AI companies. Andrew Ng is a rare example of a person who jumped from an academic bubble into an even bigger AI hype bubble. He combines certain amount of contagious enthusiasm and a fair amount of arrogance which apparently appeals to many young people today. His recent gig is about industrial inspection. In one video he presented an example in which a deep learning system recognizes a faulty solder pad. I actually worked for a company specializing in visual inspection of electronics, so I know rather well what is the state of the art. The machines routinely scan large PCB's and within fractions of a second create a full 3d reconstruction of the inspected part, analyze each solder joint for imperfections and a set of well defined defects, create interpretable scores which could be used for binning and so on. In face of that, the little video from Andrew seemed rather comical and if anything indicated Dunning-Kruger effect. Now I'm not saying that deep learning could not improve this industry in some way, but the entry level is set pretty high. And same is true for many other industries that rely on very well optimized and often elegant algorithmic solutions.  Now that we are on the subject of colorful characters in the AI scene, another one of my favorites, George Hotz from Comma.ai announced he is resigning from the role of CEO at comma. Hotz, known among other things for hacking playstation, made some ripples in the industry in 2015 when he teased Elon Musk, that he could build a self-driving-car system for Tesla, and he could do it much better and cheaper than Mobileye, which was the company that Tesla used at that time. Not too long after that, a first fatal autopilot accident happened (Joshua Brown) and Mobileye dropped the relationship with Tesla, apparently noting that their use of technology was irresponsible. Anyway, back then Hotz was confident he could build a self driving car in his garage with some old phone parts and deep learning magic. Since then his narrative has changed by some 180 degrees, and now George is on ""crusade against the 'scam' of self-driving cars""... I guess I could leave this one without a comment, though Picard's face palm meme would be in order here. I'm pretty certain that following Hotz's lead, many of today's AI hype blowers will be screaming how they've been warning about AI winter all along, once the bubble bursts.  Speaking of Tesla, as of yet the long promised autonomous coast-to-coast drive has not happened. Earlier this year Elon Musk - CEO of Tesla, stated that the drive would certainly happen by the summer 2018, but on August 1 2018 conference call he indicated that the coast-to-coast drive is actually not a priority anymore and will get delayed (no new timeframe was given). This is quite a change, particularly in the light of the conference call on which the so called full self driving feature was announced almost exactly two years ago. By August, first ""autonomous features"" were supposed to be unrolled in Tesla's via over the air update, but no such thing happened (noteworthy, in the beginning of 2018 Musk was rather confident that full self driving will noticeably depart from advanced autopilot by summer). Instead the new autopilot v9 can sometimes switch lanes but by many commentators, the new system actually requires even more attention than the previous one and still is buggy [1] and dangerous. In fact it is not uncommon to hear opinions that even the current Tesla autopilot with respect to stability and reliability barely approaches the Mobileye solution from several years ago. On October 18'th the full-self-driving (FSD) option disappeared from all Tesla models configurations, without much explanation other than that it was ""confusing"". At the time of writing this article it is not clear if the option will be coming back or if those who already payed for it will get a refund. It is still not clear to me if Elon Musk fell a victim of the hype himself or did he deliberately blew the AI bubble, but he certainly contributed substantially to the AI mania between 2015 and now, especially with that egregious stunt with Stephen Hawking.  I should add here perhaps, that I actually think Tesla autopilot is a pretty impressive piece of engineering, just not anywhere near being safely deployable beyond as fancy cruise control. The August Tesla quarterly conference call was also interesting in that a lot of time on the call was spent on the new hardware that Tesla was building that is supposed to offer an order of magnitude improvement over the current nVidia drive PX 2 system, and that this new system will finally allow for full autonomy. I remain skeptical, since having an order of magnitude more compute capabilities than drive PX is nothing extraordinary, a rig of two GTX 1080Ti's will accomplish it without any problem, and this is what many companies are putting on their test vehicles along with expensive lidars etc, and yet the full autonomy remains elusive. Secondly, it seems that cars which Tesla had sold as ""full self-driving compatible"" will require a computer swap, which will not be cheap. One can get at least some feel of what stage is Tesla at with respect to autonomy by taking a look at the slides (from May 10, 2018) of Andrej Karpathy, director of AI there. Half way through the presentation he realizes that the driving reality is full of corner cases, which are not only hard for neural nets, but are even not obvious to label by a human. The other half of his presentation is spent on what he calls software 2.0. It is a concept in which computers are no longer programmed but are trained. This reminds me a lot of the hype in the 80's, the fifth generation computers which were supposed to program themselves based on high level logical specification written in Prolog or similar functional language. This hype cycle was closely related to the expert system mania, which collapsed by the end of 80's causing an AI winter episode. I'm pretty certain this software 2.0 nonsense will share the same fate.  To conclude the Tesla case, the most recent Q3 conference call for Tesla which took place on October 24th did not bring much resolution to the above uncertainties. Musk reiterated that he believes in the self driving Tesla fleet however full self driving remains ""off menu"" as it was too confusing (two years after introduction of the item?!), Karpathy quickly mentioned that new hardware will support bigger neural networks which work ""very good"" and the new version of Autopilot will allow to navigate on the freeway, with the restriction that lane changes will require confirmation from the driver (read, any incidents will be blamed on the driver). No word about the coast to coast drive. No timeline on full self driving.  While on the self-driving car subject, one of the main criticisms of my original AI winter post was that I omitted Waymo from my discussion, them being the unquestionable leader in autonomy. This criticism was a bit unjustified in that I did include and discussed Waymo extensively in my other posts [1], but in these circumstances it appears prudent to mention what is going on there. Luckily a recent very good piece of investigative journalism shines some light on the matter. Apparently Waymo cars tested in Phoenix area had trouble with the most basic driving situations such as merging onto a freeway or making a left turn, [1]. The piece worth citing from the article:  There are times when it seems “autonomy is around the corner,” and the vehicle can go for a day without a human driver intervening, said a person familiar with Waymo. Other days reality sets in because “the edge cases are endless.”  Some independent observations appear to confirm this assessment. As much as I agree that Waymo is probably the most advanced in this game, this does not mean they are anywhere near to actually deploying anything seriously, and even further away from making such deployment economically feasible (contrary to what is suggested in occasional puff pieces such as this one). Aside from a periodic PR nonsense, Waymo does not seem to be revealing much, though recently some baffling reports of past shenanigans in google chauffeur (which later became Waymo) surfaced, involving Anthony Levandowski who is responsible for the whole Uber-Waymo fiasco. To add some comical aspect to the Waymo-Uber story, apparently an unrelated engineer managed to invalidate the patent that Uber got sued over, spending altogether 6000 dollars in fees. This is probably how much Uber payed their patent attorneys for a minute of their work...  Speaking of Uber they substantially slowed their self-driving program, practically killed their self driving truck program (same one that delivered a few crates of beer in Colorado in 2016 with great fanfares, a demo that later on turned out to be completely staged), and recent rumors indicate they might be even looking to sell the unit.  Generally the other self driving car projects are facing increasing headwinds, with some projects already getting shut down by the government agencies, and others going more low-key with respect to public announcements. Particularly interesting news came recently out of Cruise, the second in the race right after Waymo (at least according to California disengagement data). Some noteworthy bits from the Reuters article:  Those expectations are now hitting speed bumps, according to interviews with eight current and former GM and Cruise employees and executives, along with nine autonomous vehicle technology experts familiar with Cruise. These sources say that some unexpected technical challenges - including the difficulty that Cruise cars have identifying whether objects are in motion - mean putting GM’s driverless cars on the road in a large scale way in 2019 is looking highly unlikely. “Nothing is on schedule,” said one GM source, referring to certain mileage targets and other milestones the company has already missed.  And a few paragraphs further:  “Everyone in the industry is becoming more and more nervous that they will waste billions of dollars,” said Klaus Froehlich, a board member at BMW and its head of research and development.  The future of self driving cars is getting more and more uncertain, in perfect agreement with my original thesis expressed in 2016 [see also here].  Briefly on other players on the AI scene: DeepMind was rather quiet (last time I checked Montezuma's revenge remained unsolved for AI in the general see update below), but OpenAI had a small PR offensive with their DotA 2 playing agent. After the initial tournament in which the system won, it quickly became apparent that the game was in many ways restricted in favor of the computer. Hence another tournament was organized, in which most restrictions were lifted, and the tournament was ... spectacularly lost to humans... Bummer after OpenAI spent obscene amounts of money training their agents. Now I could not care less about results in game domains, since as I stated multiple times on this blog [1, 2], the only problem really worth solving in AI is the Moravec's paradox, which is exactly the opposite of what DeepMind or OpenAI are doing, but I nevertheless found this media misfire hilarious.  While touching on Moravec's paradox, one of the handful of companies that actually tried to move robotics forward, Rethink Robotics, shut down its operation. This shows that making robots do anything beyond what they already do very well in controlled factory production lines is not only difficult technically but also poses a challenging business case, even with the experience of Rodney Brooks. Unlike most other startups in this field, whose main asset is the ego of their founder fueled by some cheap VC money, Rethink actually accomplished many impressive technical achievements and the news saddened me quite a bit. Robotics will need to be rethought again, likely many times over.  Finally, as an additional indication of the changing sentiment, one can cite this tweet from François Chollet, author of Keras Deep learning framework, certainly a person in the know:  Today more people are working on deep learning than ever before -- around two orders of magnitude more than in 2014. And the rate of progress as I see it is the slowest in 5 years. Time for something new  This tweet at the time of writing of this post has been retweeted nearly 350 times and liked >1300 times.  Conclusion  So there you go: the state of AI towards the end of 2018 - it is borderline comical (hence Krusty the clown in the title image). Certain things have changed however, some of the smoke dissipated and some of the mirrors cracked. Since my original post, a lot more mainstream media articles have shown up, in which the reporters are at least willing to exercise a possibility that we are on top of a giant AI bubble that is already letting the air out [e.g. this one and many others cited in the text above]. This spike of skepticism is a natural next step in the inevitable disillusionment, but I think it will take a while before this bubble finally deflates. The next 6 months are likely to be particularly interesting in this AI circus.  Update [Nov 1, 2018]: Apparently openAI actually beat Deepmind to solving Montezuma's revenge by instead of rewarding for winning the game, rewarding for avoiding predictable states. This is very interesting since it highlights how much predicting the state of the world is important, a core topic of this blog and Predictive Vision Model [1,2,3,4].  Update 2 [Jan 9, 2019]: Melanie Mitchell pointed out that Uber AI lab also solved Montezuma's revenge, it seems however they were a few weeks late after OpenAI. Nevertheless that settles it for good, we solved Montezuma's revenge, Yay!  If you found an error, highlight it and press Shift + Enter or click here to inform us.  Related  Comments  comments","driving, winter, hype, 2018, update, post, actually, self, waymo, system, tesla, ai",2018-10-29 00:00:00
31,AI winter,,https://en.wikipedia.org/wiki/AI_winter,"Period of reduced funding and interest in AI researchIn the history of artificial intelligence, an AI winter is a period of reduced funding and interest in artificial intelligence research. Roger Schank and Marvin Minsky—two leading AI researchers who experienced the ""winter"" of the 1970s—warned the business community that enthusiasm for AI had spiraled out of control in the 1980s and that disappointment would certainly follow. [6]Just like all AI booms that have been followed by desperate AI winters, the media tended to exaggerate the significance of these developments. ""[25] The result, Moravec claims, is that some of the staff at DARPA had lost patience with AI research. In 2006, Nick Bostrom explained that ""a lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.","Period of reduced funding and interest in AI research  In the history of artificial intelligence, an AI winter is a period of reduced funding and interest in artificial intelligence research.[1] The field has experienced several hype cycles, followed by disappointment and criticism, followed by funding cuts, followed by renewed interest years or even decades later.  The term first appeared in 1984 as the topic of a public debate at the annual meeting of AAAI (then called the ""American Association of Artificial Intelligence""). Roger Schank and Marvin Minsky—two leading AI researchers who experienced the ""winter"" of the 1970s—warned the business community that enthusiasm for AI had spiraled out of control in the 1980s and that disappointment would certainly follow. They described a chain reaction, similar to a ""nuclear winter"", that would begin with pessimism in the AI community, followed by pessimism in the press, followed by a severe cutback in funding, followed by the end of serious research. Three years later the billion-dollar AI industry began to collapse.  There were two major winters approximately 1974–1980 and 1987–2000,[3] and several smaller episodes, including the following:  Enthusiasm and optimism about AI has generally increased since its low point in the early 1990s. Beginning about 2012, interest in artificial intelligence (and especially the sub-field of machine learning) from the research and corporate communities led to a dramatic increase in funding and investment, leading to the current (as of 2024 ) AI boom.  Early episodes [ edit ]  Machine translation and the ALPAC report of 1966 [ edit ]  Natural language processing (NLP) research has its roots in the early 1930s and began its existence with the work on machine translation (MT).[4] However, significant advancements and applications began to emerge after the publication of Warren Weaver's influential memorandum in 1949.[5] The memorandum generated great excitement within the research community. In the following years, notable events unfolded: IBM embarked on the development of the first machine, MIT appointed its first full-time professor in machine translation, and several conferences dedicated to MT took place. The culmination came with the public demonstration of the IBM-Georgetown machine, which garnered widespread attention in respected newspapers in 1954.[6]  Just like all AI booms that have been followed by desperate AI winters, the media tended to exaggerate the significance of these developments. Headlines about the IBM-Georgetown experiment proclaimed phrases like ""The bilingual machine,"" ""Robot brain translates Russian into King's English,""[7] and ""Polyglot brainchild.""[8] However, the actual demonstration involved the translation of a curated set of only 49 Russian sentences into English, with the machine's vocabulary limited to just 250 words.[6] To put things into perspective, a 2006 study made by Paul Nation found that humans need a vocabulary of around 8,000 to 9,000-word families to comprehend written texts with 98% accuracy.[9]  During the Cold War, the US government was particularly interested in the automatic, instant translation of Russian documents and scientific reports. The government aggressively supported efforts at machine translation starting in 1954. Another factor that propelled the field of mechanical translation was the interest shown by the Central Intelligence Agency (CIA). During that period, the CIA firmly believed in the importance of developing machine translation capabilities and supported such initiatives. They also recognized that this program had implications that extended beyond the interests of the CIA and the intelligence community.[6]  At the outset, the researchers were optimistic. Noam Chomsky's new work in grammar was streamlining the translation process and there were ""many predictions of imminent 'breakthroughs'"".[10]  Briefing for US Vice President Gerald Ford in 1973 on the junction-grammar-based computer translation model  However, researchers had underestimated the profound difficulty of word-sense disambiguation. In order to translate a sentence, a machine needed to have some idea what the sentence was about, otherwise it made mistakes. An apocryphal[11] example is ""the spirit is willing but the flesh is weak."" Translated back and forth with Russian, it became ""the vodka is good but the meat is rotten."" Later researchers would call this the commonsense knowledge problem.  By 1964, the National Research Council had become concerned about the lack of progress and formed the Automatic Language Processing Advisory Committee (ALPAC) to look into the problem. They concluded, in a famous 1966 report, that machine translation was more expensive, less accurate and slower than human translation. After spending some 20 million dollars, the NRC ended all support. Careers were destroyed and research ended.[10]  Machine translation shared the same path with NLP from the rule-based approaches through the statistical approaches up to the neural network approaches, which have in 2023 culminated in large language models.  The failure of single-layer neural networks in 1969 [ edit ]  Simple networks or circuits of connected units, including Walter Pitts and Warren McCulloch's neural network for logic and Marvin Minsky's SNARC system, have failed to deliver the promised results and were abandoned in the late 1950s. Following the success of programs such as the Logic Theorist and the General Problem Solver,[13] algorithms for manipulating symbols seemed more promising at the time as means to achieve logical reasoning viewed at the time as the essence of intelligence, either natural or artificial.  Interest in perceptrons, invented by Frank Rosenblatt, was kept alive only by the sheer force of his personality.[14] He optimistically predicted that the perceptron ""may eventually be able to learn, make decisions, and translate languages"".[15] Mainstream research into perceptrons ended partially because the 1969 book Perceptrons by Marvin Minsky and Seymour Papert emphasized the limits of what perceptrons could do. While it was already known that multilayered perceptrons are not subject to the criticism, nobody in the 1960s knew how to train a multilayered perceptron. Backpropagation was still years away.[17]  Major funding for projects neural network approaches was difficult to find in the 1970s and early 1980s.[18] Important theoretical work continued despite the lack of funding. The ""winter"" of neural network approach came to an end in the middle 1980s, when the work of John Hopfield, David Rumelhart and others revived large scale interest.[19] Rosenblatt did not live to see this, however, as he died in a boating accident shortly after Perceptrons was published.[15]  The setbacks of 1974 [ edit ]  The Lighthill report [ edit ]  In 1973, professor Sir James Lighthill was asked by the UK Parliament to evaluate the state of AI research in the United Kingdom. His report, now called the Lighthill report, criticized the utter failure of AI to achieve its ""grandiose objectives"". He concluded that nothing being done in AI could not be done in other sciences. He specifically mentioned the problem of ""combinatorial explosion"" or ""intractability"", which implied that many of AI's most successful algorithms would grind to a halt on real world problems and were only suitable for solving ""toy"" versions.[20]  The report was contested in a debate broadcast in the BBC ""Controversy"" series in 1973. The debate ""The general purpose robot is a mirage"" from the Royal Institution was Lighthill versus the team of Donald Michie, John McCarthy and Richard Gregory.[21] McCarthy later wrote that ""the combinatorial explosion problem has been recognized in AI from the beginning"".[22]  The report led to the complete dismantling of AI research in the UK.[20] AI research continued in only a few universities (Edinburgh, Essex and Sussex). Research would not revive on a large scale until 1983, when Alvey (a research project of the British Government) began to fund AI again from a war chest of £350 million in response to the Japanese Fifth Generation Project (see below). Alvey had a number of UK-only requirements which did not sit well internationally, especially with US partners, and lost Phase 2 funding.  DARPA's early 1970s funding cuts [ edit ]  During the 1960s, the Defense Advanced Research Projects Agency (then known as ""ARPA"", now known as ""DARPA"") provided millions of dollars for AI research with few strings attached. J. C. R. Licklider, the founding director of DARPA's computing division, believed in ""funding people, not projects""[23] and he and several successors allowed AI's leaders (such as Marvin Minsky, John McCarthy, Herbert A. Simon or Allen Newell) to spend it almost any way they liked.  This attitude changed after the passage of Mansfield Amendment in 1969, which required DARPA to fund ""mission-oriented direct research, rather than basic undirected research"".[24] Pure undirected research of the kind that had gone on in the 1960s would no longer be funded by DARPA. Researchers now had to show that their work would soon produce some useful military technology. AI research proposals were held to a very high standard. The situation was not helped when the Lighthill report and DARPA's own study (the American Study Group) suggested that most AI research was unlikely to produce anything truly useful in the foreseeable future. DARPA's money was directed at specific projects with identifiable goals, such as autonomous tanks and battle management systems. By 1974, funding for AI projects was hard to find.[24]  AI researcher Hans Moravec blamed the crisis on the unrealistic predictions of his colleagues: ""Many researchers were caught up in a web of increasing exaggeration. Their initial promises to DARPA had been much too optimistic. Of course, what they delivered stopped considerably short of that. But they felt they couldn't in their next proposal promise less than in the first one, so they promised more.""[25] The result, Moravec claims, is that some of the staff at DARPA had lost patience with AI research. ""It was literally phrased at DARPA that 'some of these people were going to be taught a lesson [by] having their two-million-dollar-a-year contracts cut to almost nothing!'"" Moravec told Daniel Crevier.[26]  While the autonomous tank project was a failure, the battle management system (the Dynamic Analysis and Replanning Tool) proved to be enormously successful, saving billions in the first Gulf War, repaying all of DARPAs investment in AI[27] and justifying DARPA's pragmatic policy.[28]  The SUR debacle [ edit ]  As described in:[29]  In 1971, the Defense Advanced Research Projects Agency (DARPA) began an ambitious five-year experiment in speech understanding. The goals of the project were to provide recognition of utterances from a limited vocabulary in near-real time. Three organizations finally demonstrated systems at the conclusion of the project in 1976. These were Carnegie-Mellon University (CMU), who actually demonstrated two system [HEARSAY-II and HARPY]; Bolt, Beranek and Newman (BBN); and System Development Corporation with Stanford Research Institute (SDC/SRI)  The system that came closest to satisfying the original project goals was the CMU HARPY system. The relatively high performance of the HARPY system was largely achieved through 'hard-wiring' information about possible utterances into the system's knowledge base. Although HARPY made some interesting contributions, its dependence on extensive pre-knowledge limited the applicability of the approach to other signal-understanding tasks.  DARPA was deeply disappointed with researchers working on the Speech Understanding Research program at Carnegie Mellon University. DARPA had hoped for, and felt it had been promised, a system that could respond to voice commands from a pilot. The SUR team had developed a system which could recognize spoken English, but only if the words were spoken in a particular order. DARPA felt it had been duped and, in 1974, they cancelled a three million dollar a year contract.[30]  Many years later, several successful commercial speech recognition systems would use the technology developed by the Carnegie Mellon team (such as hidden Markov models) and the market for speech recognition systems would reach $4 billion by 2001.[31]  For a description of Hearsay-II see Hearsay-II, The Hearsay-II Speech Understanding System: Integrating Knowledge to Resolve Uncertainty and A Retrospective View of the Hearsay-II Architecture which appear in Blackboard Systems[32]  Reddy gives a review of progress in speech understanding at the end of the DARPA project in a 1976 article in Proceedings of the IEEE.[33]  Contrary view [ edit ]  Thomas Haigh argues that activity in the domain of AI did not slow down, even as funding from DoD was being redirected, mostly in the wake of congressional legislation meant to separate military and academic activities.[34] That indeed professional interest was growing throughout the 70s. Using the membership count of ACM's SIGART, the Special Interest Group on Artificial Intelligence, as a proxy for interest in the subject, the author writes:[34]  (...) I located two data sources, neither of which supports the idea of a broadly based AI winter during the 1970s. One is membership of ACM's SIGART, the major venue for sharing news and research abstracts during the 1970s. When the Lighthill report was published in 1973 the fast-growing group had 1,241 members, approximately twice the level in 1969. The next five years are conventionally thought of as the darkest part of the first AI winter. Was the AI community shrinking? No! By mid-1978 SIGART membership had almost tripled, to 3,500. Not only was the group growing faster than ever, it was increasing proportionally faster than ACM as a whole which had begun to plateau (expanding by less than 50% over the entire period from 1969 to 1978). One in every 11 ACM members was in SIGART.  The setbacks of the late 1980s and early 1990s [ edit ]  The collapse of the LISP machine market [ edit ]  In the 1980s, a form of AI program called an ""expert system"" was adopted by corporations around the world. The first commercial expert system was XCON, developed at Carnegie Mellon for Digital Equipment Corporation, and it was an enormous success: it was estimated to have saved the company 40 million dollars over just six years of operation. Corporations around the world began to develop and deploy expert systems and by 1985 they were spending over a billion dollars on AI, most of it to in-house AI departments. An industry grew up to support them, including software companies like Teknowledge and Intellicorp (KEE), and hardware companies like Symbolics and LISP Machines Inc. who built specialized computers, called LISP machines, that were optimized to process the programming language LISP, the preferred language for AI research in the USA.[35][36]  In 1987, three years after Minsky and Schank's prediction, the market for specialized LISP-based AI hardware collapsed. Workstations by companies like Sun Microsystems offered a powerful alternative to LISP machines and companies like Lucid offered a LISP environment for this new class of workstations. The performance of these general workstations became an increasingly difficult challenge for LISP Machines. Companies like Lucid and Franz LISP offered increasingly powerful versions of LISP that were portable to all UNIX systems. For example, benchmarks were published showing workstations maintaining a performance advantage over LISP machines.[37] Later desktop computers built by Apple and IBM would also offer a simpler and more popular architecture to run LISP applications on. By 1987, some of them had become as powerful as the more expensive LISP machines. The desktop computers had rule-based engines such as CLIPS available.[38] These alternatives left consumers with no reason to buy an expensive machine specialized for running LISP. An entire industry worth half a billion dollars was replaced in a single year.[39]  By the early 1990s, most commercial LISP companies had failed, including Symbolics, LISP Machines Inc., Lucid Inc., etc. Other companies, like Texas Instruments and Xerox, abandoned the field. A small number of customer companies (that is, companies using systems written in LISP and developed on LISP machine platforms) continued to maintain systems. In some cases, this maintenance involved the assumption of the resulting support work.[40]  Slowdown in deployment of expert systems [ edit ]  By the early 1990s, the earliest successful expert systems, such as XCON, proved too expensive to maintain. They were difficult to update, they could not learn, they were ""brittle"" (i.e., they could make grotesque mistakes when given unusual inputs), and they fell prey to problems (such as the qualification problem) that had been identified years earlier in research in nonmonotonic logic. Expert systems proved useful, but only in a few special contexts.[41][42] Another problem dealt with the computational hardness of truth maintenance efforts for general knowledge. KEE used an assumption-based approach supporting multiple-world scenarios that was difficult to understand and apply.  The few remaining expert system shell companies were eventually forced to downsize and search for new markets and software paradigms, like case-based reasoning or universal database access. The maturation of Common Lisp saved many systems such as ICAD which found application in knowledge-based engineering. Other systems, such as Intellicorp's KEE, moved from LISP to a C++ (variant) on the PC and helped establish object-oriented technology (including providing major support for the development of UML (see UML Partners).  The end of the Fifth Generation project [ edit ]  In 1981, the Japanese Ministry of International Trade and Industry set aside $850 million for the Fifth Generation computer project. Their objectives were to write programs and build machines that could carry on conversations, translate languages, interpret pictures, and reason like human beings. By 1991, the impressive list of goals penned in 1981 had not been met. According to HP Newquist in The Brain Makers, ""On June 1, 1992, The Fifth Generation Project ended not with a successful roar, but with a whimper.""[40] As with other AI projects, expectations had run much higher than what was actually possible.[43][44]  Strategic Computing Initiative cutbacks [ edit ]  In 1983, in response to the fifth generation project, DARPA again began to fund AI research through the Strategic Computing Initiative. As originally proposed the project would begin with practical, achievable goals, which even included artificial general intelligence as long-term objective. The program was under the direction of the Information Processing Technology Office (IPTO) and was also directed at supercomputing and microelectronics. By 1985 it had spent $100 million and 92 projects were underway at 60 institutions, half in industry, half in universities and government labs. AI research was well-funded by the SCI.[45]  Jack Schwarz, who ascended to the leadership of IPTO in 1987, dismissed expert systems as ""clever programming"" and cut funding to AI ""deeply and brutally"", ""eviscerating"" SCI. Schwarz felt that DARPA should focus its funding only on those technologies which showed the most promise, in his words, DARPA should ""surf"", rather than ""dog paddle"", and he felt strongly AI was not ""the next wave"". Insiders in the program cited problems in communication, organization and integration. A few projects survived the funding cuts, including pilot's assistant and an autonomous land vehicle (which were never delivered) and the DART battle management system, which (as noted above) was successful.[46]  AI winter of the 1990's and early 2000's [ edit ]  A survey of reports from the early 2000's suggests that AI's reputation was still poor:  Alex Castro, quoted in The Economist , 7 June 2007: ""[Investors] were put off by the term 'voice recognition' which, like 'artificial intelligence', is associated with systems that have all too often failed to live up to their promises."" [ 47 ]  , 7 June 2007: ""[Investors] were put off by the term 'voice recognition' which, like 'artificial intelligence', is associated with systems that have all too often failed to live up to their promises."" Patty Tascarella in Pittsburgh Business Times , 2006: ""Some believe the word 'robotics' actually carries a stigma that hurts a company's chances at funding."" [ 48 ]  , 2006: ""Some believe the word 'robotics' actually carries a stigma that hurts a company's chances at funding."" John Markoff in the New York Times, 2005: ""At its low point, some computer scientists and software engineers avoided the term artificial intelligence for fear of being viewed as wild-eyed dreamers."" [ 49 ]  Many researchers in AI in the mid 2000's deliberately called their work by other names, such as informatics, machine learning, analytics, knowledge-based systems, business rules management, cognitive systems, intelligent systems, intelligent agents or computational intelligence, to indicate that their work emphasizes particular tools or is directed at a particular sub-problem. Although this may be partly because they consider their field to be fundamentally different from AI, it is also true that the new names help to procure funding by avoiding the stigma of false promises attached to the name ""artificial intelligence"".[49][50]  In the late 1990's and early 21st century, AI technology became widely used as elements of larger systems,[51] but the field is rarely credited for these successes. In 2006, Nick Bostrom explained that ""a lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.""[53] Rodney Brooks stated around the same time that ""there's this stupid myth out there that AI has failed, but AI is around you every second of the day.""  Current AI spring (2022–present) [ edit ]  AI has reached the highest levels of interest and funding in its history in the past few years by every possible measure, including: publications, patent applications,[56] total investment ($50 billion in 2022), and job openings (800,000 U.S. job openings in 2022). The successes of the current ""AI spring"" or ""AI boom"" are advances in language translation (in particular, Google Translate), image recognition (spurred by the ImageNet training database) as commercialized by Google Image Search, and in game-playing systems such as AlphaZero (chess champion) and AlphaGo (go champion), and Watson (Jeopardy champion). A turning point was in 2012 when AlexNet (a deep learning network) won the ImageNet Large Scale Visual Recognition Challenge with half as many errors as the second place winner.  The 2022 release of OpenAI's AI chatbot ChatGPT which as of January 2023 has over 100 million users,[60] has reinvigorated the discussion about artificial intelligence and its effects on the world.[61][62]  See also [ edit ]  Notes [ edit ]  References [ edit ]","research, translation, lisp, winter, funding, system, systems, intelligence, machine, ai, edit",
32,AI winter is well on its way,,https://blog.piekniewski.info/2018/05/28/ai-winter-is-well-on-its-way/,"Deep learning (does not) scaleOne of the key slogans repeated about deep learning is that it scales almost effortlessly. Initially it was thought that end-to-end deep learning could somehow solve this problem, a premise particularly heavily promoted by Nvidia. In such situation it is easy to learn spurious relations, as exemplified by adversarial examples in deep learning. For those who missed it, he has excellent blog posts/papers ""Deep learning: A critical appraisal"" and ""In defense of skepticism about deep learning"", where he very meticulously deconstructs the deep learning hype. I respect Gary a lot, he behaves like a real scientist should, while most so called ""deep learning stars"" just behave like cheap celebrities.","Deep learning has been at the forefront of the so called AI revolution for quite a few years now, and many people had believed that it is the silver bullet that will take us to the world of wonders of technological singularity (general AI). Many bets were made in 2014, 2015 and 2016 when still new boundaries were pushed, such as the Alpha Go etc. Companies such as Tesla were announcing through the mouths of their CEO's that fully self driving car was very close, to the point that Tesla even started selling that option to customers [to be enabled by future software update].  We have now mid 2018 and things have changed. Not on the surface yet, NIPS conference is still oversold, the corporate PR still has AI all over its press releases, Elon Musk still keeps promising self driving cars and Google CEO keeps repeating Andrew Ng's slogan that AI is bigger than electricity. But this narrative begins to crack. And as I predicted in my older post, the place where the cracks are most visible is autonomous driving - an actual application of the technology in the real world.  The dust settled on deep learning  When the ImageNet was effectively solved (note this does not mean that vision is solved), many prominent researchers in the field (including even typically quiet Geoff Hinton) were actively giving press interviews, publicizing stuff on social media (e.g. Yann Lecun, Andrew Ng, Fei-Fei Li to name a few). The general tone was that we are in front of a gigantic revolution and from now on things can only accelerate. Well years have passed and the twitter feeds of those people became less active, as exemplified by Andrew Ng below:  2013 - 0.413 tweets per day  2014 - 0.605 tweets per day  2015 - 0.320 tweets per day  2016 - 0.802 tweets per day  2017 - 0.668 tweets per day  2018 - 0.263 tweets per day (until 24 May)  Perhaps this is because Andrew's outrageous claims are now put to more scrutiny by the community as indicated in a tweet below:  Visibly the sentiment has quite considerably declined, there are much fewer tweets praising deep learning as the ultimate algorithm, the papers are becoming less ""revolutionary"" and much more ""evolutionary"". Deepmind hasn't shown anything breathtaking since their Alpha Go zero [and even that wasn't that exciting, given the obscene amount of compute necessary and applicability to games only - see Moravec's paradox]. OpenAI was rather quiet, with their last media outburst being the Dota 2 playing agent [which I suppose was meant to create as much buzz as Alpha Go, but fizzled out rather quickly]. In fact articles began showing up that even Google in fact does not know what to do with Deepmind, as their results are apparently not as practical as originally expected... As for the prominent researchers, they've been generally touring around meeting with government officials in Canada or France to secure their future grants, Yann Lecun even stepped down (rather symbolically) from the Head of Research to Chief AI scientist at Facebook. This gradual shift from rich, big corporations to government sponsored institutes suggests to me that the interest in this kind of research within these corporations (I think of Google and Facebook) is actually slowly winding down. Again these are all early signs, nothing spoken out loud, just the body language.  Deep learning (does not) scale  One of the key slogans repeated about deep learning is that it scales almost effortlessly. We had the AlexNet in 2012 which had ~60M parameters, we probably now have models with at least 1000x that number right? Well probably we do, the question however is - are these things 1000x as capable? Or even 100x as capable? A study by openAI comes in handy:  So in terms of applications for vision we see that VGG and Resnets saturated somewhat around one order of magnitude of compute resources applied (in terms of number of parameters it is actually less). Xception is a variation of google inception architecture and actually only slightly outperforms inception on ImageNet, arguably actually slightly outperforms everyone else, because essentially AlexNet solved ImageNet. So at 100 times more compute than AlexNet we pretty much saturated architectures in terms of vision, or image classification to be precise. Neural machine translation is a big effort by all the big web search players and no wonder it takes all the compute it can take (and yet google translate still sucks, though has gotten arguably better). The latest three points on that graph, interestingly show reinforcement learning related projects, applied to games by Deepmind and OpenAI. Particularly AlphaGo Zero and slightly more general AlphaZero take ridiculous amount of compute, but are not applicable in the real world applications because much of that compute is needed to simulate and generate the data these data hungry models need. OK, so we can now train AlexNet in minutes rather than days, but can we train a 1000x bigger AlexNet in days and get qualitatively better results? Apparently not...  So in fact, this graph which was meant to show how well deep learning scales, indicates the exact opposite. We can't just scale up AlexNet and get respectively better results - we have to fiddle with specific architectures, and effectively additional compute does not buy much without order of magnitude more data samples, which are in practice only available in simulated game environments.  Self driving crashes  By far the biggest blow into deep learning fame is the domain of self driving vehicles (something I anticipated for a long time, see e.g. this post from 2016). Initially it was thought that end-to-end deep learning could somehow solve this problem, a premise particularly heavily promoted by Nvidia. I don't think there is a single person on Earth who still believes that, though I could be wrong. Looking at last year California DMV disengagement reports Nvidia car could not drive literally ten miles without a disengagement. In a separate post I discuss the general state of that development and comparison to human driver safety, which (spoiler alert) is not looking good. Since 2016 there were also several Tesla AutoPilot incidents [1, 2, 3] a few of which were fatal [1, 2]. Arguably Tesla Autopilot should not be confused with self driving (it is), but at least at the core it relies on the same kind of technology. As of today, aside from occasional spectacular errors [1], it still cannot stop at an intersection, recognize a traffic light, or even navigate through a roundabout. That is in May 2018, several months after the promised coast to coast Tesla autonomous drive (did not happen although the rumor is they've tried but could not get it to work without ~30 disengagements). Several months ago (in February 2018) Elon Musk repeated in a conference call when asked about the coast to coast drive that:  ""We could have done the coast-to-coast drive, but it would have required too much specialized code to effectively game it or make it somewhat brittle and that it would work for one particular route, but not the general solution. So I think we would be able to repeat it, but if it’s just not any other route, which is not really a true solution. (…)  I am pretty excited about how much progress we’re making on the neural net front. And it’s a little – it’s also one of those things that’s kind of exponential where the progress doesn’t seem – it doesn’t seem like much progress, it doesn’t seem like much progress, and suddenly wow. It will feel like well this is a lame driver, lame driver. Like okay, that’s a pretty good driver. Like ‘Holy Cow!’ this driver’s good. It’ll be like that,”  Well, looking at the graph above (from OpenAI) I seem not be seeing that exponential progress. Neither is it visible in miles before disengagement for pretty much every big player in this field. In essence the above statement should be interpreted: ""We currently don't have the technology that could safely drive us coast to coast, though we could have faked it if we really wanted to (maybe...). We deeply hope that some sort of exponential jump in capabilities of neural networks will soon happen and save us from disgrace and massive lawsuits"".  But by far the biggest pin punching through the AI bubble was the accident in which Uber self driving car killed a pedestrian in Arizona. From the preliminary report by the NTSB we can read some astonishing statements:  Aside from general system design failure apparent in this report, it is striking that the system spent long seconds trying to decide what exactly it sees in front (whether that be a pedestrian, bike, vehicle or whatever else) rather than making the only logical decision in these circumstances, which was to make sure not to hit it. There are several reasons for it: first, people will often verbalize their decisions post factum. So a human will typically say: ""I saw a cyclist therefore I veered to the left to avoid him"". Huge amount of psychophysical literature will suggest a quite different explanation: a human saw something which was very quickly interpreted as a obstacle by fast perceptual loops of his nervous systems and he performed a rapid action to avoid it, long seconds later realizing what happened and providing a verbal explanation"". There are tons of decisions we make every day that are not verbalized, and driving includes many of them. Verbalization is costly and takes time and reality often does not provide that time. These mechanisms have evolved for a billion years to keep us safe, and driving context (although modern) makes use of many such reflexes. And since these reflexes have not evolved specifically for driving, they may induce mistakes. A knee jerk reaction to a wasp buzzing in a car may have caused many crashes and deaths. But our general understanding of 3d space, speed, ability to predict the behavior of agents, behavior of physical objects traversing through our path are the primitive skills, that were just as useful 100 million years ago as they are today and they've been honed really sharp by evolution.  But because most of these things are not easily verbalizable, they are hard to measure, and consequently we don't optimize our machine learning systems on these aspects at all [see my earlier post for benchmark proposals that would address some of these capabilities]. Now this would speak in favor of Nvidia end-to-end approach - learn image->action mapping, skipping any verbalization, and in some ways this is the right way to do it but... the problem is that the input space is incredibly high dimensional, while the action space is very low dimensional. Hence the ""amount"" of ""label"" (readout) is extremely small compared to the amount of information coming in. In such situation it is easy to learn spurious relations, as exemplified by adversarial examples in deep learning. A different paradigm is needed and I postulate prediction of the entire perceptual input along with the action as a first step to make a system able to extract the semantics of the world, rather than spurious correlations [read more about my first proposed architecture called Predictive Vision Model].  In fact if there is anything at all we learned from the outburst of deep learning, is that (10k+ dimensional) image space has plenty enough spurious patterns in it, that they actually generalize across many images and make the impression like our classifiers actually understand what they are seeing. Nothing could be further from the truth, as admitted even by the top researchers who are heavily invested in this field. In fact many top researchers should not be too outraged by my observations, Yann Lecun warned about overexcitement and AI winter for a while, even Geoffrey Hinton - the father of the current outburst of backpropagation - admitted in an interview that this likely is all a dead end, and we need to start over. At this point though, the hype is so strong that nobody will listen, even to the founding fathers of the field.  Gary Marcus and his quest against the hype  I should mention that more top tier people are recognizing the hubris and have the courage to openly call it. One of the most active in that space is Gary Marcus. Although I don't think I agree with everything that Gary proposes in terms of AI, we certainly agree that it is not yet as powerful as painted by the deep learning hype-propaganda. In fact it is not even any close. For those who missed it, he has excellent blog posts/papers ""Deep learning: A critical appraisal"" and ""In defense of skepticism about deep learning"", where he very meticulously deconstructs the deep learning hype. I respect Gary a lot, he behaves like a real scientist should, while most so called ""deep learning stars"" just behave like cheap celebrities.  Conclusion  Predicting the A.I. winter is like predicting a stock market crash - impossible to tell precisely when it happens, but almost certain that it will at some point. Much like before a stock market crash, there are signs of the impending collapse, but the narrative is so strong that it is very easy to ignore them, even if they are in plain sight. In my opinion there are such signs of a huge decline in deep learning (and probably in AI in general as this term has been abused 'ad nauseam' by corporate propaganda) already visible. Visible in plain sight, yet hidden from the majority by an increasingly intense narrative. How ""deep"" will this winter be? I have no idea. What will come next? I have no idea. But I'm fairly positive it is coming, perhaps sooner rather than later.  If you found an error, highlight it and press Shift + Enter or click here to inform us.  Related  Comments  comments","driving, tweets, winter, general, coast, compute, way, alexnet, fact, learning, ai, deep",2018-05-28 00:00:00
33,AI winter,,https://en.wikipedia.org/wiki/AI_winter,"Period of reduced funding and interest in AI researchIn the history of artificial intelligence, an AI winter is a period of reduced funding and interest in artificial intelligence research. Roger Schank and Marvin Minsky—two leading AI researchers who experienced the ""winter"" of the 1970s—warned the business community that enthusiasm for AI had spiraled out of control in the 1980s and that disappointment would certainly follow. [6]Just like all AI booms that have been followed by desperate AI winters, the media tended to exaggerate the significance of these developments. ""[25] The result, Moravec claims, is that some of the staff at DARPA had lost patience with AI research. In 2006, Nick Bostrom explained that ""a lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.","Period of reduced funding and interest in AI research  In the history of artificial intelligence, an AI winter is a period of reduced funding and interest in artificial intelligence research.[1] The field has experienced several hype cycles, followed by disappointment and criticism, followed by funding cuts, followed by renewed interest years or even decades later.  The term first appeared in 1984 as the topic of a public debate at the annual meeting of AAAI (then called the ""American Association of Artificial Intelligence""). Roger Schank and Marvin Minsky—two leading AI researchers who experienced the ""winter"" of the 1970s—warned the business community that enthusiasm for AI had spiraled out of control in the 1980s and that disappointment would certainly follow. They described a chain reaction, similar to a ""nuclear winter"", that would begin with pessimism in the AI community, followed by pessimism in the press, followed by a severe cutback in funding, followed by the end of serious research. Three years later the billion-dollar AI industry began to collapse.  There were two major winters approximately 1974–1980 and 1987–2000,[3] and several smaller episodes, including the following:  Enthusiasm and optimism about AI has generally increased since its low point in the early 1990s. Beginning about 2012, interest in artificial intelligence (and especially the sub-field of machine learning) from the research and corporate communities led to a dramatic increase in funding and investment, leading to the current (as of 2024 ) AI boom.  Early episodes [ edit ]  Machine translation and the ALPAC report of 1966 [ edit ]  Natural language processing (NLP) research has its roots in the early 1930s and began its existence with the work on machine translation (MT).[4] However, significant advancements and applications began to emerge after the publication of Warren Weaver's influential memorandum in 1949.[5] The memorandum generated great excitement within the research community. In the following years, notable events unfolded: IBM embarked on the development of the first machine, MIT appointed its first full-time professor in machine translation, and several conferences dedicated to MT took place. The culmination came with the public demonstration of the IBM-Georgetown machine, which garnered widespread attention in respected newspapers in 1954.[6]  Just like all AI booms that have been followed by desperate AI winters, the media tended to exaggerate the significance of these developments. Headlines about the IBM-Georgetown experiment proclaimed phrases like ""The bilingual machine,"" ""Robot brain translates Russian into King's English,""[7] and ""Polyglot brainchild.""[8] However, the actual demonstration involved the translation of a curated set of only 49 Russian sentences into English, with the machine's vocabulary limited to just 250 words.[6] To put things into perspective, a 2006 study made by Paul Nation found that humans need a vocabulary of around 8,000 to 9,000-word families to comprehend written texts with 98% accuracy.[9]  During the Cold War, the US government was particularly interested in the automatic, instant translation of Russian documents and scientific reports. The government aggressively supported efforts at machine translation starting in 1954. Another factor that propelled the field of mechanical translation was the interest shown by the Central Intelligence Agency (CIA). During that period, the CIA firmly believed in the importance of developing machine translation capabilities and supported such initiatives. They also recognized that this program had implications that extended beyond the interests of the CIA and the intelligence community.[6]  At the outset, the researchers were optimistic. Noam Chomsky's new work in grammar was streamlining the translation process and there were ""many predictions of imminent 'breakthroughs'"".[10]  Briefing for US Vice President Gerald Ford in 1973 on the junction-grammar-based computer translation model  However, researchers had underestimated the profound difficulty of word-sense disambiguation. In order to translate a sentence, a machine needed to have some idea what the sentence was about, otherwise it made mistakes. An apocryphal[11] example is ""the spirit is willing but the flesh is weak."" Translated back and forth with Russian, it became ""the vodka is good but the meat is rotten."" Later researchers would call this the commonsense knowledge problem.  By 1964, the National Research Council had become concerned about the lack of progress and formed the Automatic Language Processing Advisory Committee (ALPAC) to look into the problem. They concluded, in a famous 1966 report, that machine translation was more expensive, less accurate and slower than human translation. After spending some 20 million dollars, the NRC ended all support. Careers were destroyed and research ended.[10]  Machine translation shared the same path with NLP from the rule-based approaches through the statistical approaches up to the neural network approaches, which have in 2023 culminated in large language models.  The failure of single-layer neural networks in 1969 [ edit ]  Simple networks or circuits of connected units, including Walter Pitts and Warren McCulloch's neural network for logic and Marvin Minsky's SNARC system, have failed to deliver the promised results and were abandoned in the late 1950s. Following the success of programs such as the Logic Theorist and the General Problem Solver,[13] algorithms for manipulating symbols seemed more promising at the time as means to achieve logical reasoning viewed at the time as the essence of intelligence, either natural or artificial.  Interest in perceptrons, invented by Frank Rosenblatt, was kept alive only by the sheer force of his personality.[14] He optimistically predicted that the perceptron ""may eventually be able to learn, make decisions, and translate languages"".[15] Mainstream research into perceptrons ended partially because the 1969 book Perceptrons by Marvin Minsky and Seymour Papert emphasized the limits of what perceptrons could do. While it was already known that multilayered perceptrons are not subject to the criticism, nobody in the 1960s knew how to train a multilayered perceptron. Backpropagation was still years away.[17]  Major funding for projects neural network approaches was difficult to find in the 1970s and early 1980s.[18] Important theoretical work continued despite the lack of funding. The ""winter"" of neural network approach came to an end in the middle 1980s, when the work of John Hopfield, David Rumelhart and others revived large scale interest.[19] Rosenblatt did not live to see this, however, as he died in a boating accident shortly after Perceptrons was published.[15]  The setbacks of 1974 [ edit ]  The Lighthill report [ edit ]  In 1973, professor Sir James Lighthill was asked by the UK Parliament to evaluate the state of AI research in the United Kingdom. His report, now called the Lighthill report, criticized the utter failure of AI to achieve its ""grandiose objectives"". He concluded that nothing being done in AI could not be done in other sciences. He specifically mentioned the problem of ""combinatorial explosion"" or ""intractability"", which implied that many of AI's most successful algorithms would grind to a halt on real world problems and were only suitable for solving ""toy"" versions.[20]  The report was contested in a debate broadcast in the BBC ""Controversy"" series in 1973. The debate ""The general purpose robot is a mirage"" from the Royal Institution was Lighthill versus the team of Donald Michie, John McCarthy and Richard Gregory.[21] McCarthy later wrote that ""the combinatorial explosion problem has been recognized in AI from the beginning"".[22]  The report led to the complete dismantling of AI research in the UK.[20] AI research continued in only a few universities (Edinburgh, Essex and Sussex). Research would not revive on a large scale until 1983, when Alvey (a research project of the British Government) began to fund AI again from a war chest of £350 million in response to the Japanese Fifth Generation Project (see below). Alvey had a number of UK-only requirements which did not sit well internationally, especially with US partners, and lost Phase 2 funding.  DARPA's early 1970s funding cuts [ edit ]  During the 1960s, the Defense Advanced Research Projects Agency (then known as ""ARPA"", now known as ""DARPA"") provided millions of dollars for AI research with few strings attached. J. C. R. Licklider, the founding director of DARPA's computing division, believed in ""funding people, not projects""[23] and he and several successors allowed AI's leaders (such as Marvin Minsky, John McCarthy, Herbert A. Simon or Allen Newell) to spend it almost any way they liked.  This attitude changed after the passage of Mansfield Amendment in 1969, which required DARPA to fund ""mission-oriented direct research, rather than basic undirected research"".[24] Pure undirected research of the kind that had gone on in the 1960s would no longer be funded by DARPA. Researchers now had to show that their work would soon produce some useful military technology. AI research proposals were held to a very high standard. The situation was not helped when the Lighthill report and DARPA's own study (the American Study Group) suggested that most AI research was unlikely to produce anything truly useful in the foreseeable future. DARPA's money was directed at specific projects with identifiable goals, such as autonomous tanks and battle management systems. By 1974, funding for AI projects was hard to find.[24]  AI researcher Hans Moravec blamed the crisis on the unrealistic predictions of his colleagues: ""Many researchers were caught up in a web of increasing exaggeration. Their initial promises to DARPA had been much too optimistic. Of course, what they delivered stopped considerably short of that. But they felt they couldn't in their next proposal promise less than in the first one, so they promised more.""[25] The result, Moravec claims, is that some of the staff at DARPA had lost patience with AI research. ""It was literally phrased at DARPA that 'some of these people were going to be taught a lesson [by] having their two-million-dollar-a-year contracts cut to almost nothing!'"" Moravec told Daniel Crevier.[26]  While the autonomous tank project was a failure, the battle management system (the Dynamic Analysis and Replanning Tool) proved to be enormously successful, saving billions in the first Gulf War, repaying all of DARPAs investment in AI[27] and justifying DARPA's pragmatic policy.[28]  The SUR debacle [ edit ]  As described in:[29]  In 1971, the Defense Advanced Research Projects Agency (DARPA) began an ambitious five-year experiment in speech understanding. The goals of the project were to provide recognition of utterances from a limited vocabulary in near-real time. Three organizations finally demonstrated systems at the conclusion of the project in 1976. These were Carnegie-Mellon University (CMU), who actually demonstrated two system [HEARSAY-II and HARPY]; Bolt, Beranek and Newman (BBN); and System Development Corporation with Stanford Research Institute (SDC/SRI)  The system that came closest to satisfying the original project goals was the CMU HARPY system. The relatively high performance of the HARPY system was largely achieved through 'hard-wiring' information about possible utterances into the system's knowledge base. Although HARPY made some interesting contributions, its dependence on extensive pre-knowledge limited the applicability of the approach to other signal-understanding tasks.  DARPA was deeply disappointed with researchers working on the Speech Understanding Research program at Carnegie Mellon University. DARPA had hoped for, and felt it had been promised, a system that could respond to voice commands from a pilot. The SUR team had developed a system which could recognize spoken English, but only if the words were spoken in a particular order. DARPA felt it had been duped and, in 1974, they cancelled a three million dollar a year contract.[30]  Many years later, several successful commercial speech recognition systems would use the technology developed by the Carnegie Mellon team (such as hidden Markov models) and the market for speech recognition systems would reach $4 billion by 2001.[31]  For a description of Hearsay-II see Hearsay-II, The Hearsay-II Speech Understanding System: Integrating Knowledge to Resolve Uncertainty and A Retrospective View of the Hearsay-II Architecture which appear in Blackboard Systems[32]  Reddy gives a review of progress in speech understanding at the end of the DARPA project in a 1976 article in Proceedings of the IEEE.[33]  Contrary view [ edit ]  Thomas Haigh argues that activity in the domain of AI did not slow down, even as funding from DoD was being redirected, mostly in the wake of congressional legislation meant to separate military and academic activities.[34] That indeed professional interest was growing throughout the 70s. Using the membership count of ACM's SIGART, the Special Interest Group on Artificial Intelligence, as a proxy for interest in the subject, the author writes:[34]  (...) I located two data sources, neither of which supports the idea of a broadly based AI winter during the 1970s. One is membership of ACM's SIGART, the major venue for sharing news and research abstracts during the 1970s. When the Lighthill report was published in 1973 the fast-growing group had 1,241 members, approximately twice the level in 1969. The next five years are conventionally thought of as the darkest part of the first AI winter. Was the AI community shrinking? No! By mid-1978 SIGART membership had almost tripled, to 3,500. Not only was the group growing faster than ever, it was increasing proportionally faster than ACM as a whole which had begun to plateau (expanding by less than 50% over the entire period from 1969 to 1978). One in every 11 ACM members was in SIGART.  The setbacks of the late 1980s and early 1990s [ edit ]  The collapse of the LISP machine market [ edit ]  In the 1980s, a form of AI program called an ""expert system"" was adopted by corporations around the world. The first commercial expert system was XCON, developed at Carnegie Mellon for Digital Equipment Corporation, and it was an enormous success: it was estimated to have saved the company 40 million dollars over just six years of operation. Corporations around the world began to develop and deploy expert systems and by 1985 they were spending over a billion dollars on AI, most of it to in-house AI departments. An industry grew up to support them, including software companies like Teknowledge and Intellicorp (KEE), and hardware companies like Symbolics and LISP Machines Inc. who built specialized computers, called LISP machines, that were optimized to process the programming language LISP, the preferred language for AI research in the USA.[35][36]  In 1987, three years after Minsky and Schank's prediction, the market for specialized LISP-based AI hardware collapsed. Workstations by companies like Sun Microsystems offered a powerful alternative to LISP machines and companies like Lucid offered a LISP environment for this new class of workstations. The performance of these general workstations became an increasingly difficult challenge for LISP Machines. Companies like Lucid and Franz LISP offered increasingly powerful versions of LISP that were portable to all UNIX systems. For example, benchmarks were published showing workstations maintaining a performance advantage over LISP machines.[37] Later desktop computers built by Apple and IBM would also offer a simpler and more popular architecture to run LISP applications on. By 1987, some of them had become as powerful as the more expensive LISP machines. The desktop computers had rule-based engines such as CLIPS available.[38] These alternatives left consumers with no reason to buy an expensive machine specialized for running LISP. An entire industry worth half a billion dollars was replaced in a single year.[39]  By the early 1990s, most commercial LISP companies had failed, including Symbolics, LISP Machines Inc., Lucid Inc., etc. Other companies, like Texas Instruments and Xerox, abandoned the field. A small number of customer companies (that is, companies using systems written in LISP and developed on LISP machine platforms) continued to maintain systems. In some cases, this maintenance involved the assumption of the resulting support work.[40]  Slowdown in deployment of expert systems [ edit ]  By the early 1990s, the earliest successful expert systems, such as XCON, proved too expensive to maintain. They were difficult to update, they could not learn, they were ""brittle"" (i.e., they could make grotesque mistakes when given unusual inputs), and they fell prey to problems (such as the qualification problem) that had been identified years earlier in research in nonmonotonic logic. Expert systems proved useful, but only in a few special contexts.[41][42] Another problem dealt with the computational hardness of truth maintenance efforts for general knowledge. KEE used an assumption-based approach supporting multiple-world scenarios that was difficult to understand and apply.  The few remaining expert system shell companies were eventually forced to downsize and search for new markets and software paradigms, like case-based reasoning or universal database access. The maturation of Common Lisp saved many systems such as ICAD which found application in knowledge-based engineering. Other systems, such as Intellicorp's KEE, moved from LISP to a C++ (variant) on the PC and helped establish object-oriented technology (including providing major support for the development of UML (see UML Partners).  The end of the Fifth Generation project [ edit ]  In 1981, the Japanese Ministry of International Trade and Industry set aside $850 million for the Fifth Generation computer project. Their objectives were to write programs and build machines that could carry on conversations, translate languages, interpret pictures, and reason like human beings. By 1991, the impressive list of goals penned in 1981 had not been met. According to HP Newquist in The Brain Makers, ""On June 1, 1992, The Fifth Generation Project ended not with a successful roar, but with a whimper.""[40] As with other AI projects, expectations had run much higher than what was actually possible.[43][44]  Strategic Computing Initiative cutbacks [ edit ]  In 1983, in response to the fifth generation project, DARPA again began to fund AI research through the Strategic Computing Initiative. As originally proposed the project would begin with practical, achievable goals, which even included artificial general intelligence as long-term objective. The program was under the direction of the Information Processing Technology Office (IPTO) and was also directed at supercomputing and microelectronics. By 1985 it had spent $100 million and 92 projects were underway at 60 institutions, half in industry, half in universities and government labs. AI research was well-funded by the SCI.[45]  Jack Schwarz, who ascended to the leadership of IPTO in 1987, dismissed expert systems as ""clever programming"" and cut funding to AI ""deeply and brutally"", ""eviscerating"" SCI. Schwarz felt that DARPA should focus its funding only on those technologies which showed the most promise, in his words, DARPA should ""surf"", rather than ""dog paddle"", and he felt strongly AI was not ""the next wave"". Insiders in the program cited problems in communication, organization and integration. A few projects survived the funding cuts, including pilot's assistant and an autonomous land vehicle (which were never delivered) and the DART battle management system, which (as noted above) was successful.[46]  AI winter of the 1990's and early 2000's [ edit ]  A survey of reports from the early 2000's suggests that AI's reputation was still poor:  Alex Castro, quoted in The Economist , 7 June 2007: ""[Investors] were put off by the term 'voice recognition' which, like 'artificial intelligence', is associated with systems that have all too often failed to live up to their promises."" [ 47 ]  , 7 June 2007: ""[Investors] were put off by the term 'voice recognition' which, like 'artificial intelligence', is associated with systems that have all too often failed to live up to their promises."" Patty Tascarella in Pittsburgh Business Times , 2006: ""Some believe the word 'robotics' actually carries a stigma that hurts a company's chances at funding."" [ 48 ]  , 2006: ""Some believe the word 'robotics' actually carries a stigma that hurts a company's chances at funding."" John Markoff in the New York Times, 2005: ""At its low point, some computer scientists and software engineers avoided the term artificial intelligence for fear of being viewed as wild-eyed dreamers."" [ 49 ]  Many researchers in AI in the mid 2000's deliberately called their work by other names, such as informatics, machine learning, analytics, knowledge-based systems, business rules management, cognitive systems, intelligent systems, intelligent agents or computational intelligence, to indicate that their work emphasizes particular tools or is directed at a particular sub-problem. Although this may be partly because they consider their field to be fundamentally different from AI, it is also true that the new names help to procure funding by avoiding the stigma of false promises attached to the name ""artificial intelligence"".[49][50]  In the late 1990's and early 21st century, AI technology became widely used as elements of larger systems,[51] but the field is rarely credited for these successes. In 2006, Nick Bostrom explained that ""a lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.""[53] Rodney Brooks stated around the same time that ""there's this stupid myth out there that AI has failed, but AI is around you every second of the day.""  Current AI spring (2022–present) [ edit ]  AI has reached the highest levels of interest and funding in its history in the past few years by every possible measure, including: publications, patent applications,[56] total investment ($50 billion in 2022), and job openings (800,000 U.S. job openings in 2022). The successes of the current ""AI spring"" or ""AI boom"" are advances in language translation (in particular, Google Translate), image recognition (spurred by the ImageNet training database) as commercialized by Google Image Search, and in game-playing systems such as AlphaZero (chess champion) and AlphaGo (go champion), and Watson (Jeopardy champion). A turning point was in 2012 when AlexNet (a deep learning network) won the ImageNet Large Scale Visual Recognition Challenge with half as many errors as the second place winner.  The 2022 release of OpenAI's AI chatbot ChatGPT which as of January 2023 has over 100 million users,[60] has reinvigorated the discussion about artificial intelligence and its effects on the world.[61][62]  See also [ edit ]  Notes [ edit ]  References [ edit ]","research, translation, lisp, winter, funding, system, systems, intelligence, machine, ai, edit",
34,"Hey ChatGPT, how do I survive until the next AI winter?","Chelsea Troy, Kye Fox",https://kyefox.com/waiting-for-ai-winter/,"They’re still in the zeitgeist of tech. Just a little better than what I could get working a help desk or bagging groceries. Respectable work, but I knew a lot of people who worked it and knew how abusive management could get. I got good at writing and music and art and learned to blend creative and technical skills into something coherent. The AI winter always comes.","As someone who finished a 2 year AAS in Linux network administration in 2008 just as the economy was taking a dive and companies were rushing to replace their expensive and needy IT departments with Google's growing suite of cloud products, I empathize. At least a scholarship paid for most of it and I'm completely debt free.  These students aren’t in tech yet. They’re still in the zeitgeist of tech.  I didn't even expect much. Just a little better than what I could get working a help desk or bagging groceries. Respectable work, but I knew a lot of people who worked it and knew how abusive management could get. I thought tech was better at the time. One of the first phases of naivety I went through.  Zero jobs. I tried volunteering with the library's IT guy to maybe support a barren resume, and it seemed to go great, but I never got a call back. I think I may have made the dude feel threatened by how current my knowledge was and how natural I was with the tech, while he was somewhat older and likely struggling to keep up.  ""I don't know why it crashes when the screensaver comes on."" He was trying this hip new operating system called Linux. One of the early Ubuntus.  ""You need the right driver for the 3D stuff. Most auto-installed drivers choke on software rendering.""  ""Ah.""  He learned about 3D acceleration. I learned about CMOS batteries. It was a good day. Or so I thought.  I doubted my feeling that he saw me as a threat until I talked to a comrade in library IT who would be in that guy's cohort if they were in the same building, and that was the consensus we reached: he was afraid of me taking his job when all I wanted to do was learn from him, and he kept the gate to library IT all over the region.  So I never got into IT, or tech generally. I got good at writing and music and art and learned to blend creative and technical skills into something coherent. You are looking at the result.  The AI winter always comes. I make a point of understanding these tools so I don't get roped into the popular fear, and so I don't miss the real threat: capitalist assholes who want to replace labor with machines who don't care what happens to the people they displace because they're machines with no actual intelligence.","theyre, replace, winter, dont, linux, survive, chatgpt, threat, hey, taking, library, machines, tech, learned, ai",2024-05-31 13:30:34+00:00
35,AI winter,,https://en.wikipedia.org/wiki/AI_winter,"Period of reduced funding and interest in AI researchIn the history of artificial intelligence, an AI winter is a period of reduced funding and interest in artificial intelligence research. Roger Schank and Marvin Minsky—two leading AI researchers who experienced the ""winter"" of the 1970s—warned the business community that enthusiasm for AI had spiraled out of control in the 1980s and that disappointment would certainly follow. [6]Just like all AI booms that have been followed by desperate AI winters, the media tended to exaggerate the significance of these developments. ""[25] The result, Moravec claims, is that some of the staff at DARPA had lost patience with AI research. In 2006, Nick Bostrom explained that ""a lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.","Period of reduced funding and interest in AI research  In the history of artificial intelligence, an AI winter is a period of reduced funding and interest in artificial intelligence research.[1] The field has experienced several hype cycles, followed by disappointment and criticism, followed by funding cuts, followed by renewed interest years or even decades later.  The term first appeared in 1984 as the topic of a public debate at the annual meeting of AAAI (then called the ""American Association of Artificial Intelligence""). Roger Schank and Marvin Minsky—two leading AI researchers who experienced the ""winter"" of the 1970s—warned the business community that enthusiasm for AI had spiraled out of control in the 1980s and that disappointment would certainly follow. They described a chain reaction, similar to a ""nuclear winter"", that would begin with pessimism in the AI community, followed by pessimism in the press, followed by a severe cutback in funding, followed by the end of serious research. Three years later the billion-dollar AI industry began to collapse.  There were two major winters approximately 1974–1980 and 1987–2000,[3] and several smaller episodes, including the following:  Enthusiasm and optimism about AI has generally increased since its low point in the early 1990s. Beginning about 2012, interest in artificial intelligence (and especially the sub-field of machine learning) from the research and corporate communities led to a dramatic increase in funding and investment, leading to the current (as of 2024 ) AI boom.  Early episodes [ edit ]  Machine translation and the ALPAC report of 1966 [ edit ]  Natural language processing (NLP) research has its roots in the early 1930s and began its existence with the work on machine translation (MT).[4] However, significant advancements and applications began to emerge after the publication of Warren Weaver's influential memorandum in 1949.[5] The memorandum generated great excitement within the research community. In the following years, notable events unfolded: IBM embarked on the development of the first machine, MIT appointed its first full-time professor in machine translation, and several conferences dedicated to MT took place. The culmination came with the public demonstration of the IBM-Georgetown machine, which garnered widespread attention in respected newspapers in 1954.[6]  Just like all AI booms that have been followed by desperate AI winters, the media tended to exaggerate the significance of these developments. Headlines about the IBM-Georgetown experiment proclaimed phrases like ""The bilingual machine,"" ""Robot brain translates Russian into King's English,""[7] and ""Polyglot brainchild.""[8] However, the actual demonstration involved the translation of a curated set of only 49 Russian sentences into English, with the machine's vocabulary limited to just 250 words.[6] To put things into perspective, a 2006 study made by Paul Nation found that humans need a vocabulary of around 8,000 to 9,000-word families to comprehend written texts with 98% accuracy.[9]  During the Cold War, the US government was particularly interested in the automatic, instant translation of Russian documents and scientific reports. The government aggressively supported efforts at machine translation starting in 1954. Another factor that propelled the field of mechanical translation was the interest shown by the Central Intelligence Agency (CIA). During that period, the CIA firmly believed in the importance of developing machine translation capabilities and supported such initiatives. They also recognized that this program had implications that extended beyond the interests of the CIA and the intelligence community.[6]  At the outset, the researchers were optimistic. Noam Chomsky's new work in grammar was streamlining the translation process and there were ""many predictions of imminent 'breakthroughs'"".[10]  Briefing for US Vice President Gerald Ford in 1973 on the junction-grammar-based computer translation model  However, researchers had underestimated the profound difficulty of word-sense disambiguation. In order to translate a sentence, a machine needed to have some idea what the sentence was about, otherwise it made mistakes. An apocryphal[11] example is ""the spirit is willing but the flesh is weak."" Translated back and forth with Russian, it became ""the vodka is good but the meat is rotten."" Later researchers would call this the commonsense knowledge problem.  By 1964, the National Research Council had become concerned about the lack of progress and formed the Automatic Language Processing Advisory Committee (ALPAC) to look into the problem. They concluded, in a famous 1966 report, that machine translation was more expensive, less accurate and slower than human translation. After spending some 20 million dollars, the NRC ended all support. Careers were destroyed and research ended.[10]  Machine translation shared the same path with NLP from the rule-based approaches through the statistical approaches up to the neural network approaches, which have in 2023 culminated in large language models.  The failure of single-layer neural networks in 1969 [ edit ]  Simple networks or circuits of connected units, including Walter Pitts and Warren McCulloch's neural network for logic and Marvin Minsky's SNARC system, have failed to deliver the promised results and were abandoned in the late 1950s. Following the success of programs such as the Logic Theorist and the General Problem Solver,[13] algorithms for manipulating symbols seemed more promising at the time as means to achieve logical reasoning viewed at the time as the essence of intelligence, either natural or artificial.  Interest in perceptrons, invented by Frank Rosenblatt, was kept alive only by the sheer force of his personality.[14] He optimistically predicted that the perceptron ""may eventually be able to learn, make decisions, and translate languages"".[15] Mainstream research into perceptrons ended partially because the 1969 book Perceptrons by Marvin Minsky and Seymour Papert emphasized the limits of what perceptrons could do. While it was already known that multilayered perceptrons are not subject to the criticism, nobody in the 1960s knew how to train a multilayered perceptron. Backpropagation was still years away.[17]  Major funding for projects neural network approaches was difficult to find in the 1970s and early 1980s.[18] Important theoretical work continued despite the lack of funding. The ""winter"" of neural network approach came to an end in the middle 1980s, when the work of John Hopfield, David Rumelhart and others revived large scale interest.[19] Rosenblatt did not live to see this, however, as he died in a boating accident shortly after Perceptrons was published.[15]  The setbacks of 1974 [ edit ]  The Lighthill report [ edit ]  In 1973, professor Sir James Lighthill was asked by the UK Parliament to evaluate the state of AI research in the United Kingdom. His report, now called the Lighthill report, criticized the utter failure of AI to achieve its ""grandiose objectives"". He concluded that nothing being done in AI could not be done in other sciences. He specifically mentioned the problem of ""combinatorial explosion"" or ""intractability"", which implied that many of AI's most successful algorithms would grind to a halt on real world problems and were only suitable for solving ""toy"" versions.[20]  The report was contested in a debate broadcast in the BBC ""Controversy"" series in 1973. The debate ""The general purpose robot is a mirage"" from the Royal Institution was Lighthill versus the team of Donald Michie, John McCarthy and Richard Gregory.[21] McCarthy later wrote that ""the combinatorial explosion problem has been recognized in AI from the beginning"".[22]  The report led to the complete dismantling of AI research in the UK.[20] AI research continued in only a few universities (Edinburgh, Essex and Sussex). Research would not revive on a large scale until 1983, when Alvey (a research project of the British Government) began to fund AI again from a war chest of £350 million in response to the Japanese Fifth Generation Project (see below). Alvey had a number of UK-only requirements which did not sit well internationally, especially with US partners, and lost Phase 2 funding.  DARPA's early 1970s funding cuts [ edit ]  During the 1960s, the Defense Advanced Research Projects Agency (then known as ""ARPA"", now known as ""DARPA"") provided millions of dollars for AI research with few strings attached. J. C. R. Licklider, the founding director of DARPA's computing division, believed in ""funding people, not projects""[23] and he and several successors allowed AI's leaders (such as Marvin Minsky, John McCarthy, Herbert A. Simon or Allen Newell) to spend it almost any way they liked.  This attitude changed after the passage of Mansfield Amendment in 1969, which required DARPA to fund ""mission-oriented direct research, rather than basic undirected research"".[24] Pure undirected research of the kind that had gone on in the 1960s would no longer be funded by DARPA. Researchers now had to show that their work would soon produce some useful military technology. AI research proposals were held to a very high standard. The situation was not helped when the Lighthill report and DARPA's own study (the American Study Group) suggested that most AI research was unlikely to produce anything truly useful in the foreseeable future. DARPA's money was directed at specific projects with identifiable goals, such as autonomous tanks and battle management systems. By 1974, funding for AI projects was hard to find.[24]  AI researcher Hans Moravec blamed the crisis on the unrealistic predictions of his colleagues: ""Many researchers were caught up in a web of increasing exaggeration. Their initial promises to DARPA had been much too optimistic. Of course, what they delivered stopped considerably short of that. But they felt they couldn't in their next proposal promise less than in the first one, so they promised more.""[25] The result, Moravec claims, is that some of the staff at DARPA had lost patience with AI research. ""It was literally phrased at DARPA that 'some of these people were going to be taught a lesson [by] having their two-million-dollar-a-year contracts cut to almost nothing!'"" Moravec told Daniel Crevier.[26]  While the autonomous tank project was a failure, the battle management system (the Dynamic Analysis and Replanning Tool) proved to be enormously successful, saving billions in the first Gulf War, repaying all of DARPAs investment in AI[27] and justifying DARPA's pragmatic policy.[28]  The SUR debacle [ edit ]  As described in:[29]  In 1971, the Defense Advanced Research Projects Agency (DARPA) began an ambitious five-year experiment in speech understanding. The goals of the project were to provide recognition of utterances from a limited vocabulary in near-real time. Three organizations finally demonstrated systems at the conclusion of the project in 1976. These were Carnegie-Mellon University (CMU), who actually demonstrated two system [HEARSAY-II and HARPY]; Bolt, Beranek and Newman (BBN); and System Development Corporation with Stanford Research Institute (SDC/SRI)  The system that came closest to satisfying the original project goals was the CMU HARPY system. The relatively high performance of the HARPY system was largely achieved through 'hard-wiring' information about possible utterances into the system's knowledge base. Although HARPY made some interesting contributions, its dependence on extensive pre-knowledge limited the applicability of the approach to other signal-understanding tasks.  DARPA was deeply disappointed with researchers working on the Speech Understanding Research program at Carnegie Mellon University. DARPA had hoped for, and felt it had been promised, a system that could respond to voice commands from a pilot. The SUR team had developed a system which could recognize spoken English, but only if the words were spoken in a particular order. DARPA felt it had been duped and, in 1974, they cancelled a three million dollar a year contract.[30]  Many years later, several successful commercial speech recognition systems would use the technology developed by the Carnegie Mellon team (such as hidden Markov models) and the market for speech recognition systems would reach $4 billion by 2001.[31]  For a description of Hearsay-II see Hearsay-II, The Hearsay-II Speech Understanding System: Integrating Knowledge to Resolve Uncertainty and A Retrospective View of the Hearsay-II Architecture which appear in Blackboard Systems[32]  Reddy gives a review of progress in speech understanding at the end of the DARPA project in a 1976 article in Proceedings of the IEEE.[33]  Contrary view [ edit ]  Thomas Haigh argues that activity in the domain of AI did not slow down, even as funding from DoD was being redirected, mostly in the wake of congressional legislation meant to separate military and academic activities.[34] That indeed professional interest was growing throughout the 70s. Using the membership count of ACM's SIGART, the Special Interest Group on Artificial Intelligence, as a proxy for interest in the subject, the author writes:[34]  (...) I located two data sources, neither of which supports the idea of a broadly based AI winter during the 1970s. One is membership of ACM's SIGART, the major venue for sharing news and research abstracts during the 1970s. When the Lighthill report was published in 1973 the fast-growing group had 1,241 members, approximately twice the level in 1969. The next five years are conventionally thought of as the darkest part of the first AI winter. Was the AI community shrinking? No! By mid-1978 SIGART membership had almost tripled, to 3,500. Not only was the group growing faster than ever, it was increasing proportionally faster than ACM as a whole which had begun to plateau (expanding by less than 50% over the entire period from 1969 to 1978). One in every 11 ACM members was in SIGART.  The setbacks of the late 1980s and early 1990s [ edit ]  The collapse of the LISP machine market [ edit ]  In the 1980s, a form of AI program called an ""expert system"" was adopted by corporations around the world. The first commercial expert system was XCON, developed at Carnegie Mellon for Digital Equipment Corporation, and it was an enormous success: it was estimated to have saved the company 40 million dollars over just six years of operation. Corporations around the world began to develop and deploy expert systems and by 1985 they were spending over a billion dollars on AI, most of it to in-house AI departments. An industry grew up to support them, including software companies like Teknowledge and Intellicorp (KEE), and hardware companies like Symbolics and LISP Machines Inc. who built specialized computers, called LISP machines, that were optimized to process the programming language LISP, the preferred language for AI research in the USA.[35][36]  In 1987, three years after Minsky and Schank's prediction, the market for specialized LISP-based AI hardware collapsed. Workstations by companies like Sun Microsystems offered a powerful alternative to LISP machines and companies like Lucid offered a LISP environment for this new class of workstations. The performance of these general workstations became an increasingly difficult challenge for LISP Machines. Companies like Lucid and Franz LISP offered increasingly powerful versions of LISP that were portable to all UNIX systems. For example, benchmarks were published showing workstations maintaining a performance advantage over LISP machines.[37] Later desktop computers built by Apple and IBM would also offer a simpler and more popular architecture to run LISP applications on. By 1987, some of them had become as powerful as the more expensive LISP machines. The desktop computers had rule-based engines such as CLIPS available.[38] These alternatives left consumers with no reason to buy an expensive machine specialized for running LISP. An entire industry worth half a billion dollars was replaced in a single year.[39]  By the early 1990s, most commercial LISP companies had failed, including Symbolics, LISP Machines Inc., Lucid Inc., etc. Other companies, like Texas Instruments and Xerox, abandoned the field. A small number of customer companies (that is, companies using systems written in LISP and developed on LISP machine platforms) continued to maintain systems. In some cases, this maintenance involved the assumption of the resulting support work.[40]  Slowdown in deployment of expert systems [ edit ]  By the early 1990s, the earliest successful expert systems, such as XCON, proved too expensive to maintain. They were difficult to update, they could not learn, they were ""brittle"" (i.e., they could make grotesque mistakes when given unusual inputs), and they fell prey to problems (such as the qualification problem) that had been identified years earlier in research in nonmonotonic logic. Expert systems proved useful, but only in a few special contexts.[41][42] Another problem dealt with the computational hardness of truth maintenance efforts for general knowledge. KEE used an assumption-based approach supporting multiple-world scenarios that was difficult to understand and apply.  The few remaining expert system shell companies were eventually forced to downsize and search for new markets and software paradigms, like case-based reasoning or universal database access. The maturation of Common Lisp saved many systems such as ICAD which found application in knowledge-based engineering. Other systems, such as Intellicorp's KEE, moved from LISP to a C++ (variant) on the PC and helped establish object-oriented technology (including providing major support for the development of UML (see UML Partners).  The end of the Fifth Generation project [ edit ]  In 1981, the Japanese Ministry of International Trade and Industry set aside $850 million for the Fifth Generation computer project. Their objectives were to write programs and build machines that could carry on conversations, translate languages, interpret pictures, and reason like human beings. By 1991, the impressive list of goals penned in 1981 had not been met. According to HP Newquist in The Brain Makers, ""On June 1, 1992, The Fifth Generation Project ended not with a successful roar, but with a whimper.""[40] As with other AI projects, expectations had run much higher than what was actually possible.[43][44]  Strategic Computing Initiative cutbacks [ edit ]  In 1983, in response to the fifth generation project, DARPA again began to fund AI research through the Strategic Computing Initiative. As originally proposed the project would begin with practical, achievable goals, which even included artificial general intelligence as long-term objective. The program was under the direction of the Information Processing Technology Office (IPTO) and was also directed at supercomputing and microelectronics. By 1985 it had spent $100 million and 92 projects were underway at 60 institutions, half in industry, half in universities and government labs. AI research was well-funded by the SCI.[45]  Jack Schwarz, who ascended to the leadership of IPTO in 1987, dismissed expert systems as ""clever programming"" and cut funding to AI ""deeply and brutally"", ""eviscerating"" SCI. Schwarz felt that DARPA should focus its funding only on those technologies which showed the most promise, in his words, DARPA should ""surf"", rather than ""dog paddle"", and he felt strongly AI was not ""the next wave"". Insiders in the program cited problems in communication, organization and integration. A few projects survived the funding cuts, including pilot's assistant and an autonomous land vehicle (which were never delivered) and the DART battle management system, which (as noted above) was successful.[46]  AI winter of the 1990's and early 2000's [ edit ]  A survey of reports from the early 2000's suggests that AI's reputation was still poor:  Alex Castro, quoted in The Economist , 7 June 2007: ""[Investors] were put off by the term 'voice recognition' which, like 'artificial intelligence', is associated with systems that have all too often failed to live up to their promises."" [ 47 ]  , 7 June 2007: ""[Investors] were put off by the term 'voice recognition' which, like 'artificial intelligence', is associated with systems that have all too often failed to live up to their promises."" Patty Tascarella in Pittsburgh Business Times , 2006: ""Some believe the word 'robotics' actually carries a stigma that hurts a company's chances at funding."" [ 48 ]  , 2006: ""Some believe the word 'robotics' actually carries a stigma that hurts a company's chances at funding."" John Markoff in the New York Times, 2005: ""At its low point, some computer scientists and software engineers avoided the term artificial intelligence for fear of being viewed as wild-eyed dreamers."" [ 49 ]  Many researchers in AI in the mid 2000's deliberately called their work by other names, such as informatics, machine learning, analytics, knowledge-based systems, business rules management, cognitive systems, intelligent systems, intelligent agents or computational intelligence, to indicate that their work emphasizes particular tools or is directed at a particular sub-problem. Although this may be partly because they consider their field to be fundamentally different from AI, it is also true that the new names help to procure funding by avoiding the stigma of false promises attached to the name ""artificial intelligence"".[49][50]  In the late 1990's and early 21st century, AI technology became widely used as elements of larger systems,[51] but the field is rarely credited for these successes. In 2006, Nick Bostrom explained that ""a lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.""[53] Rodney Brooks stated around the same time that ""there's this stupid myth out there that AI has failed, but AI is around you every second of the day.""  Current AI spring (2022–present) [ edit ]  AI has reached the highest levels of interest and funding in its history in the past few years by every possible measure, including: publications, patent applications,[56] total investment ($50 billion in 2022), and job openings (800,000 U.S. job openings in 2022). The successes of the current ""AI spring"" or ""AI boom"" are advances in language translation (in particular, Google Translate), image recognition (spurred by the ImageNet training database) as commercialized by Google Image Search, and in game-playing systems such as AlphaZero (chess champion) and AlphaGo (go champion), and Watson (Jeopardy champion). A turning point was in 2012 when AlexNet (a deep learning network) won the ImageNet Large Scale Visual Recognition Challenge with half as many errors as the second place winner.  The 2022 release of OpenAI's AI chatbot ChatGPT which as of January 2023 has over 100 million users,[60] has reinvigorated the discussion about artificial intelligence and its effects on the world.[61][62]  See also [ edit ]  Notes [ edit ]  References [ edit ]","research, translation, lisp, winter, funding, system, systems, intelligence, machine, ai, edit",
36,AI winter is well on its way,,https://blog.piekniewski.info/2018/05/28/ai-winter-is-well-on-its-way/,"Deep learning (does not) scaleOne of the key slogans repeated about deep learning is that it scales almost effortlessly. Initially it was thought that end-to-end deep learning could somehow solve this problem, a premise particularly heavily promoted by Nvidia. In such situation it is easy to learn spurious relations, as exemplified by adversarial examples in deep learning. For those who missed it, he has excellent blog posts/papers ""Deep learning: A critical appraisal"" and ""In defense of skepticism about deep learning"", where he very meticulously deconstructs the deep learning hype. I respect Gary a lot, he behaves like a real scientist should, while most so called ""deep learning stars"" just behave like cheap celebrities.","Deep learning has been at the forefront of the so called AI revolution for quite a few years now, and many people had believed that it is the silver bullet that will take us to the world of wonders of technological singularity (general AI). Many bets were made in 2014, 2015 and 2016 when still new boundaries were pushed, such as the Alpha Go etc. Companies such as Tesla were announcing through the mouths of their CEO's that fully self driving car was very close, to the point that Tesla even started selling that option to customers [to be enabled by future software update].  We have now mid 2018 and things have changed. Not on the surface yet, NIPS conference is still oversold, the corporate PR still has AI all over its press releases, Elon Musk still keeps promising self driving cars and Google CEO keeps repeating Andrew Ng's slogan that AI is bigger than electricity. But this narrative begins to crack. And as I predicted in my older post, the place where the cracks are most visible is autonomous driving - an actual application of the technology in the real world.  The dust settled on deep learning  When the ImageNet was effectively solved (note this does not mean that vision is solved), many prominent researchers in the field (including even typically quiet Geoff Hinton) were actively giving press interviews, publicizing stuff on social media (e.g. Yann Lecun, Andrew Ng, Fei-Fei Li to name a few). The general tone was that we are in front of a gigantic revolution and from now on things can only accelerate. Well years have passed and the twitter feeds of those people became less active, as exemplified by Andrew Ng below:  2013 - 0.413 tweets per day  2014 - 0.605 tweets per day  2015 - 0.320 tweets per day  2016 - 0.802 tweets per day  2017 - 0.668 tweets per day  2018 - 0.263 tweets per day (until 24 May)  Perhaps this is because Andrew's outrageous claims are now put to more scrutiny by the community as indicated in a tweet below:  Visibly the sentiment has quite considerably declined, there are much fewer tweets praising deep learning as the ultimate algorithm, the papers are becoming less ""revolutionary"" and much more ""evolutionary"". Deepmind hasn't shown anything breathtaking since their Alpha Go zero [and even that wasn't that exciting, given the obscene amount of compute necessary and applicability to games only - see Moravec's paradox]. OpenAI was rather quiet, with their last media outburst being the Dota 2 playing agent [which I suppose was meant to create as much buzz as Alpha Go, but fizzled out rather quickly]. In fact articles began showing up that even Google in fact does not know what to do with Deepmind, as their results are apparently not as practical as originally expected... As for the prominent researchers, they've been generally touring around meeting with government officials in Canada or France to secure their future grants, Yann Lecun even stepped down (rather symbolically) from the Head of Research to Chief AI scientist at Facebook. This gradual shift from rich, big corporations to government sponsored institutes suggests to me that the interest in this kind of research within these corporations (I think of Google and Facebook) is actually slowly winding down. Again these are all early signs, nothing spoken out loud, just the body language.  Deep learning (does not) scale  One of the key slogans repeated about deep learning is that it scales almost effortlessly. We had the AlexNet in 2012 which had ~60M parameters, we probably now have models with at least 1000x that number right? Well probably we do, the question however is - are these things 1000x as capable? Or even 100x as capable? A study by openAI comes in handy:  So in terms of applications for vision we see that VGG and Resnets saturated somewhat around one order of magnitude of compute resources applied (in terms of number of parameters it is actually less). Xception is a variation of google inception architecture and actually only slightly outperforms inception on ImageNet, arguably actually slightly outperforms everyone else, because essentially AlexNet solved ImageNet. So at 100 times more compute than AlexNet we pretty much saturated architectures in terms of vision, or image classification to be precise. Neural machine translation is a big effort by all the big web search players and no wonder it takes all the compute it can take (and yet google translate still sucks, though has gotten arguably better). The latest three points on that graph, interestingly show reinforcement learning related projects, applied to games by Deepmind and OpenAI. Particularly AlphaGo Zero and slightly more general AlphaZero take ridiculous amount of compute, but are not applicable in the real world applications because much of that compute is needed to simulate and generate the data these data hungry models need. OK, so we can now train AlexNet in minutes rather than days, but can we train a 1000x bigger AlexNet in days and get qualitatively better results? Apparently not...  So in fact, this graph which was meant to show how well deep learning scales, indicates the exact opposite. We can't just scale up AlexNet and get respectively better results - we have to fiddle with specific architectures, and effectively additional compute does not buy much without order of magnitude more data samples, which are in practice only available in simulated game environments.  Self driving crashes  By far the biggest blow into deep learning fame is the domain of self driving vehicles (something I anticipated for a long time, see e.g. this post from 2016). Initially it was thought that end-to-end deep learning could somehow solve this problem, a premise particularly heavily promoted by Nvidia. I don't think there is a single person on Earth who still believes that, though I could be wrong. Looking at last year California DMV disengagement reports Nvidia car could not drive literally ten miles without a disengagement. In a separate post I discuss the general state of that development and comparison to human driver safety, which (spoiler alert) is not looking good. Since 2016 there were also several Tesla AutoPilot incidents [1, 2, 3] a few of which were fatal [1, 2]. Arguably Tesla Autopilot should not be confused with self driving (it is), but at least at the core it relies on the same kind of technology. As of today, aside from occasional spectacular errors [1], it still cannot stop at an intersection, recognize a traffic light, or even navigate through a roundabout. That is in May 2018, several months after the promised coast to coast Tesla autonomous drive (did not happen although the rumor is they've tried but could not get it to work without ~30 disengagements). Several months ago (in February 2018) Elon Musk repeated in a conference call when asked about the coast to coast drive that:  ""We could have done the coast-to-coast drive, but it would have required too much specialized code to effectively game it or make it somewhat brittle and that it would work for one particular route, but not the general solution. So I think we would be able to repeat it, but if it’s just not any other route, which is not really a true solution. (…)  I am pretty excited about how much progress we’re making on the neural net front. And it’s a little – it’s also one of those things that’s kind of exponential where the progress doesn’t seem – it doesn’t seem like much progress, it doesn’t seem like much progress, and suddenly wow. It will feel like well this is a lame driver, lame driver. Like okay, that’s a pretty good driver. Like ‘Holy Cow!’ this driver’s good. It’ll be like that,”  Well, looking at the graph above (from OpenAI) I seem not be seeing that exponential progress. Neither is it visible in miles before disengagement for pretty much every big player in this field. In essence the above statement should be interpreted: ""We currently don't have the technology that could safely drive us coast to coast, though we could have faked it if we really wanted to (maybe...). We deeply hope that some sort of exponential jump in capabilities of neural networks will soon happen and save us from disgrace and massive lawsuits"".  But by far the biggest pin punching through the AI bubble was the accident in which Uber self driving car killed a pedestrian in Arizona. From the preliminary report by the NTSB we can read some astonishing statements:  Aside from general system design failure apparent in this report, it is striking that the system spent long seconds trying to decide what exactly it sees in front (whether that be a pedestrian, bike, vehicle or whatever else) rather than making the only logical decision in these circumstances, which was to make sure not to hit it. There are several reasons for it: first, people will often verbalize their decisions post factum. So a human will typically say: ""I saw a cyclist therefore I veered to the left to avoid him"". Huge amount of psychophysical literature will suggest a quite different explanation: a human saw something which was very quickly interpreted as a obstacle by fast perceptual loops of his nervous systems and he performed a rapid action to avoid it, long seconds later realizing what happened and providing a verbal explanation"". There are tons of decisions we make every day that are not verbalized, and driving includes many of them. Verbalization is costly and takes time and reality often does not provide that time. These mechanisms have evolved for a billion years to keep us safe, and driving context (although modern) makes use of many such reflexes. And since these reflexes have not evolved specifically for driving, they may induce mistakes. A knee jerk reaction to a wasp buzzing in a car may have caused many crashes and deaths. But our general understanding of 3d space, speed, ability to predict the behavior of agents, behavior of physical objects traversing through our path are the primitive skills, that were just as useful 100 million years ago as they are today and they've been honed really sharp by evolution.  But because most of these things are not easily verbalizable, they are hard to measure, and consequently we don't optimize our machine learning systems on these aspects at all [see my earlier post for benchmark proposals that would address some of these capabilities]. Now this would speak in favor of Nvidia end-to-end approach - learn image->action mapping, skipping any verbalization, and in some ways this is the right way to do it but... the problem is that the input space is incredibly high dimensional, while the action space is very low dimensional. Hence the ""amount"" of ""label"" (readout) is extremely small compared to the amount of information coming in. In such situation it is easy to learn spurious relations, as exemplified by adversarial examples in deep learning. A different paradigm is needed and I postulate prediction of the entire perceptual input along with the action as a first step to make a system able to extract the semantics of the world, rather than spurious correlations [read more about my first proposed architecture called Predictive Vision Model].  In fact if there is anything at all we learned from the outburst of deep learning, is that (10k+ dimensional) image space has plenty enough spurious patterns in it, that they actually generalize across many images and make the impression like our classifiers actually understand what they are seeing. Nothing could be further from the truth, as admitted even by the top researchers who are heavily invested in this field. In fact many top researchers should not be too outraged by my observations, Yann Lecun warned about overexcitement and AI winter for a while, even Geoffrey Hinton - the father of the current outburst of backpropagation - admitted in an interview that this likely is all a dead end, and we need to start over. At this point though, the hype is so strong that nobody will listen, even to the founding fathers of the field.  Gary Marcus and his quest against the hype  I should mention that more top tier people are recognizing the hubris and have the courage to openly call it. One of the most active in that space is Gary Marcus. Although I don't think I agree with everything that Gary proposes in terms of AI, we certainly agree that it is not yet as powerful as painted by the deep learning hype-propaganda. In fact it is not even any close. For those who missed it, he has excellent blog posts/papers ""Deep learning: A critical appraisal"" and ""In defense of skepticism about deep learning"", where he very meticulously deconstructs the deep learning hype. I respect Gary a lot, he behaves like a real scientist should, while most so called ""deep learning stars"" just behave like cheap celebrities.  Conclusion  Predicting the A.I. winter is like predicting a stock market crash - impossible to tell precisely when it happens, but almost certain that it will at some point. Much like before a stock market crash, there are signs of the impending collapse, but the narrative is so strong that it is very easy to ignore them, even if they are in plain sight. In my opinion there are such signs of a huge decline in deep learning (and probably in AI in general as this term has been abused 'ad nauseam' by corporate propaganda) already visible. Visible in plain sight, yet hidden from the majority by an increasingly intense narrative. How ""deep"" will this winter be? I have no idea. What will come next? I have no idea. But I'm fairly positive it is coming, perhaps sooner rather than later.  If you found an error, highlight it and press Shift + Enter or click here to inform us.  Related  Comments  comments","driving, tweets, winter, general, coast, compute, way, alexnet, fact, learning, ai, deep",2018-05-28 00:00:00
37,AI winter is well on its way,,https://blog.piekniewski.info/2018/05/28/ai-winter-is-well-on-its-way/,"Deep learning (does not) scaleOne of the key slogans repeated about deep learning is that it scales almost effortlessly. Initially it was thought that end-to-end deep learning could somehow solve this problem, a premise particularly heavily promoted by Nvidia. In such situation it is easy to learn spurious relations, as exemplified by adversarial examples in deep learning. For those who missed it, he has excellent blog posts/papers ""Deep learning: A critical appraisal"" and ""In defense of skepticism about deep learning"", where he very meticulously deconstructs the deep learning hype. I respect Gary a lot, he behaves like a real scientist should, while most so called ""deep learning stars"" just behave like cheap celebrities.","Deep learning has been at the forefront of the so called AI revolution for quite a few years now, and many people had believed that it is the silver bullet that will take us to the world of wonders of technological singularity (general AI). Many bets were made in 2014, 2015 and 2016 when still new boundaries were pushed, such as the Alpha Go etc. Companies such as Tesla were announcing through the mouths of their CEO's that fully self driving car was very close, to the point that Tesla even started selling that option to customers [to be enabled by future software update].  We have now mid 2018 and things have changed. Not on the surface yet, NIPS conference is still oversold, the corporate PR still has AI all over its press releases, Elon Musk still keeps promising self driving cars and Google CEO keeps repeating Andrew Ng's slogan that AI is bigger than electricity. But this narrative begins to crack. And as I predicted in my older post, the place where the cracks are most visible is autonomous driving - an actual application of the technology in the real world.  The dust settled on deep learning  When the ImageNet was effectively solved (note this does not mean that vision is solved), many prominent researchers in the field (including even typically quiet Geoff Hinton) were actively giving press interviews, publicizing stuff on social media (e.g. Yann Lecun, Andrew Ng, Fei-Fei Li to name a few). The general tone was that we are in front of a gigantic revolution and from now on things can only accelerate. Well years have passed and the twitter feeds of those people became less active, as exemplified by Andrew Ng below:  2013 - 0.413 tweets per day  2014 - 0.605 tweets per day  2015 - 0.320 tweets per day  2016 - 0.802 tweets per day  2017 - 0.668 tweets per day  2018 - 0.263 tweets per day (until 24 May)  Perhaps this is because Andrew's outrageous claims are now put to more scrutiny by the community as indicated in a tweet below:  Visibly the sentiment has quite considerably declined, there are much fewer tweets praising deep learning as the ultimate algorithm, the papers are becoming less ""revolutionary"" and much more ""evolutionary"". Deepmind hasn't shown anything breathtaking since their Alpha Go zero [and even that wasn't that exciting, given the obscene amount of compute necessary and applicability to games only - see Moravec's paradox]. OpenAI was rather quiet, with their last media outburst being the Dota 2 playing agent [which I suppose was meant to create as much buzz as Alpha Go, but fizzled out rather quickly]. In fact articles began showing up that even Google in fact does not know what to do with Deepmind, as their results are apparently not as practical as originally expected... As for the prominent researchers, they've been generally touring around meeting with government officials in Canada or France to secure their future grants, Yann Lecun even stepped down (rather symbolically) from the Head of Research to Chief AI scientist at Facebook. This gradual shift from rich, big corporations to government sponsored institutes suggests to me that the interest in this kind of research within these corporations (I think of Google and Facebook) is actually slowly winding down. Again these are all early signs, nothing spoken out loud, just the body language.  Deep learning (does not) scale  One of the key slogans repeated about deep learning is that it scales almost effortlessly. We had the AlexNet in 2012 which had ~60M parameters, we probably now have models with at least 1000x that number right? Well probably we do, the question however is - are these things 1000x as capable? Or even 100x as capable? A study by openAI comes in handy:  So in terms of applications for vision we see that VGG and Resnets saturated somewhat around one order of magnitude of compute resources applied (in terms of number of parameters it is actually less). Xception is a variation of google inception architecture and actually only slightly outperforms inception on ImageNet, arguably actually slightly outperforms everyone else, because essentially AlexNet solved ImageNet. So at 100 times more compute than AlexNet we pretty much saturated architectures in terms of vision, or image classification to be precise. Neural machine translation is a big effort by all the big web search players and no wonder it takes all the compute it can take (and yet google translate still sucks, though has gotten arguably better). The latest three points on that graph, interestingly show reinforcement learning related projects, applied to games by Deepmind and OpenAI. Particularly AlphaGo Zero and slightly more general AlphaZero take ridiculous amount of compute, but are not applicable in the real world applications because much of that compute is needed to simulate and generate the data these data hungry models need. OK, so we can now train AlexNet in minutes rather than days, but can we train a 1000x bigger AlexNet in days and get qualitatively better results? Apparently not...  So in fact, this graph which was meant to show how well deep learning scales, indicates the exact opposite. We can't just scale up AlexNet and get respectively better results - we have to fiddle with specific architectures, and effectively additional compute does not buy much without order of magnitude more data samples, which are in practice only available in simulated game environments.  Self driving crashes  By far the biggest blow into deep learning fame is the domain of self driving vehicles (something I anticipated for a long time, see e.g. this post from 2016). Initially it was thought that end-to-end deep learning could somehow solve this problem, a premise particularly heavily promoted by Nvidia. I don't think there is a single person on Earth who still believes that, though I could be wrong. Looking at last year California DMV disengagement reports Nvidia car could not drive literally ten miles without a disengagement. In a separate post I discuss the general state of that development and comparison to human driver safety, which (spoiler alert) is not looking good. Since 2016 there were also several Tesla AutoPilot incidents [1, 2, 3] a few of which were fatal [1, 2]. Arguably Tesla Autopilot should not be confused with self driving (it is), but at least at the core it relies on the same kind of technology. As of today, aside from occasional spectacular errors [1], it still cannot stop at an intersection, recognize a traffic light, or even navigate through a roundabout. That is in May 2018, several months after the promised coast to coast Tesla autonomous drive (did not happen although the rumor is they've tried but could not get it to work without ~30 disengagements). Several months ago (in February 2018) Elon Musk repeated in a conference call when asked about the coast to coast drive that:  ""We could have done the coast-to-coast drive, but it would have required too much specialized code to effectively game it or make it somewhat brittle and that it would work for one particular route, but not the general solution. So I think we would be able to repeat it, but if it’s just not any other route, which is not really a true solution. (…)  I am pretty excited about how much progress we’re making on the neural net front. And it’s a little – it’s also one of those things that’s kind of exponential where the progress doesn’t seem – it doesn’t seem like much progress, it doesn’t seem like much progress, and suddenly wow. It will feel like well this is a lame driver, lame driver. Like okay, that’s a pretty good driver. Like ‘Holy Cow!’ this driver’s good. It’ll be like that,”  Well, looking at the graph above (from OpenAI) I seem not be seeing that exponential progress. Neither is it visible in miles before disengagement for pretty much every big player in this field. In essence the above statement should be interpreted: ""We currently don't have the technology that could safely drive us coast to coast, though we could have faked it if we really wanted to (maybe...). We deeply hope that some sort of exponential jump in capabilities of neural networks will soon happen and save us from disgrace and massive lawsuits"".  But by far the biggest pin punching through the AI bubble was the accident in which Uber self driving car killed a pedestrian in Arizona. From the preliminary report by the NTSB we can read some astonishing statements:  Aside from general system design failure apparent in this report, it is striking that the system spent long seconds trying to decide what exactly it sees in front (whether that be a pedestrian, bike, vehicle or whatever else) rather than making the only logical decision in these circumstances, which was to make sure not to hit it. There are several reasons for it: first, people will often verbalize their decisions post factum. So a human will typically say: ""I saw a cyclist therefore I veered to the left to avoid him"". Huge amount of psychophysical literature will suggest a quite different explanation: a human saw something which was very quickly interpreted as a obstacle by fast perceptual loops of his nervous systems and he performed a rapid action to avoid it, long seconds later realizing what happened and providing a verbal explanation"". There are tons of decisions we make every day that are not verbalized, and driving includes many of them. Verbalization is costly and takes time and reality often does not provide that time. These mechanisms have evolved for a billion years to keep us safe, and driving context (although modern) makes use of many such reflexes. And since these reflexes have not evolved specifically for driving, they may induce mistakes. A knee jerk reaction to a wasp buzzing in a car may have caused many crashes and deaths. But our general understanding of 3d space, speed, ability to predict the behavior of agents, behavior of physical objects traversing through our path are the primitive skills, that were just as useful 100 million years ago as they are today and they've been honed really sharp by evolution.  But because most of these things are not easily verbalizable, they are hard to measure, and consequently we don't optimize our machine learning systems on these aspects at all [see my earlier post for benchmark proposals that would address some of these capabilities]. Now this would speak in favor of Nvidia end-to-end approach - learn image->action mapping, skipping any verbalization, and in some ways this is the right way to do it but... the problem is that the input space is incredibly high dimensional, while the action space is very low dimensional. Hence the ""amount"" of ""label"" (readout) is extremely small compared to the amount of information coming in. In such situation it is easy to learn spurious relations, as exemplified by adversarial examples in deep learning. A different paradigm is needed and I postulate prediction of the entire perceptual input along with the action as a first step to make a system able to extract the semantics of the world, rather than spurious correlations [read more about my first proposed architecture called Predictive Vision Model].  In fact if there is anything at all we learned from the outburst of deep learning, is that (10k+ dimensional) image space has plenty enough spurious patterns in it, that they actually generalize across many images and make the impression like our classifiers actually understand what they are seeing. Nothing could be further from the truth, as admitted even by the top researchers who are heavily invested in this field. In fact many top researchers should not be too outraged by my observations, Yann Lecun warned about overexcitement and AI winter for a while, even Geoffrey Hinton - the father of the current outburst of backpropagation - admitted in an interview that this likely is all a dead end, and we need to start over. At this point though, the hype is so strong that nobody will listen, even to the founding fathers of the field.  Gary Marcus and his quest against the hype  I should mention that more top tier people are recognizing the hubris and have the courage to openly call it. One of the most active in that space is Gary Marcus. Although I don't think I agree with everything that Gary proposes in terms of AI, we certainly agree that it is not yet as powerful as painted by the deep learning hype-propaganda. In fact it is not even any close. For those who missed it, he has excellent blog posts/papers ""Deep learning: A critical appraisal"" and ""In defense of skepticism about deep learning"", where he very meticulously deconstructs the deep learning hype. I respect Gary a lot, he behaves like a real scientist should, while most so called ""deep learning stars"" just behave like cheap celebrities.  Conclusion  Predicting the A.I. winter is like predicting a stock market crash - impossible to tell precisely when it happens, but almost certain that it will at some point. Much like before a stock market crash, there are signs of the impending collapse, but the narrative is so strong that it is very easy to ignore them, even if they are in plain sight. In my opinion there are such signs of a huge decline in deep learning (and probably in AI in general as this term has been abused 'ad nauseam' by corporate propaganda) already visible. Visible in plain sight, yet hidden from the majority by an increasingly intense narrative. How ""deep"" will this winter be? I have no idea. What will come next? I have no idea. But I'm fairly positive it is coming, perhaps sooner rather than later.  If you found an error, highlight it and press Shift + Enter or click here to inform us.  Related  Comments  comments","driving, tweets, winter, general, coast, compute, way, alexnet, fact, learning, ai, deep",2018-05-28 00:00:00
38,AI Experts Discuss The Potential For An AI Winter Beyond 2020,"Read More, Nikita Johnson, Guest Blog",https://blog.re-work.co/ai-experts-discuss-the-possibility-of-another-ai-winter/,"The term AI Winter, first appeared in 1984 having been discussed at the American Association of Artificial Intelligence. This might sound like we have already passed the AI hype and we are entering the AI winter. Andriy Burkov, Director of Data Science, GartnerNo, in a foreseeable future, there will be no AI winter. Kavita Ganesan, Founder, Opinosis AnalyticsFrom a business and practical application perspective, I don’t think we’ll be seeing another AI winter. So why would there be an AI winter?","The term AI Winter, first appeared in 1984 having been discussed at the American Association of Artificial Intelligence. This discussion then saw a rise in pessimism and a reduction of funding. Many minds that had survived the first 'winter', prior to it being cited as such, suggested that an increase in enthusiasm for AI, perhaps without the technical capabilities to match, has seen a sharp rise and later collapse. Having hit extreme lows in the early 1990s, the enthusiasm for AI began to rise again, and as they say, the rest is now history with it now so ubiquitous in society.  That said, we have seen this year that anything is certainly possible, therefore, we thought we would ask our community of AI expert friends what they thought on the topic, asking - 'Do you think we will see another AI Winter? If so, why?  There is certainly differing of opinion, or at least a difference in perspective, making for a good range of answers.  Jane Wang, Senior Research Scientist, DeepMind  I don't think we'll see one in the same sense that we saw AI winters before, simply because AI technology is already so ubiquitous and has been demonstrated to work well in so many areas. This is in the realm of narrow AI, such as algorithms that can perform a narrow task incredibly well, like image classification/generation or machine translation. And I don't think there's any danger of deep neural networks going away anytime soon, as time and time again we've seen the power of combining vast amounts of data with very powerful machines to train these huge models. General AI, or AGI, is still not too well-defined, so I think it's been hyped up a bit with people's preconceptions from, say, movies and popular culture. I think there might be a bit of disillusionment after a while when it becomes obvious that general artificial intelligence won't necessarily look like that.  Alireza Fathi, Senior Research Scientist, Google Research  This is something that I continuously think about. It is very hard to predict the future, but here are a few thoughts. I think some of the projects that have attracted a lot of attention and funding like self-driving cars and AR/VR haven’t panned out the way we thought they would. Few years ago everyone was thinking self-driving cars are right around the corner, now people have become more skeptical and predict that they wouldn’t be launching in the next few years. On the AR/VR front, Magic Leap, a very well funded unicorn AR/VR startup, has laid off a lot of its employees. Google has reduced its focus on AR/VR. Also in general you don’t really see any AR/VR product by any company that has reached more than a few thousands of customers. In recent years, big companies like Google, Facebook, Apple, Amazon felt there was not enough AI talent for them to hire. Therefore, they started AI residency or other similar programs. Today it is much harder for a graduating AI student to find a job at some of these companies. This might sound like we have already passed the AI hype and we are entering the AI winter. However, I think on the positive side, now you see AI almost in every major product: Shopping, Search, Social Networks, Cloud, Gaming, etc. and I think it will continue to be more and more like that. The bottom line is that I think there is a huge need for AI expertise, but at the same time, I think we have enough experts right now and gradually we might have more than enough. This might affect how special AI experts feel today in comparison to experts in other fields.  Andriy Burkov, Director of Data Science, Gartner  No, in a foreseeable future, there will be no AI winter. Neural networks are such a great and versatile tool! They can solve almost any learning problem worth solving, we just need the right data and the right person. Never in history, was AI as impactful (for both business and humanity) as it is today.  Abhishek Gupta, Founder, Montreal AI Ethics Institute  I wouldn't say that we will see another AI Winter because some of the disillusionment that occurred in the past was primarily because of very large gaps between expectations and reality and after having gone through that a couple of times, people have realized the importance of having tempered expectations. In addition, the technical limitations that the field faced in the past haven't surfaced as of yet because of both software and hardware innovations that have helped to continuously inch ahead of the potential ceilings that we might hit which can trigger a new winter. I think we might have an ""AI Fall"" potentially where things like the strong belief in the imminent and immediate arrival of self-driving vehicles has experienced a slowdown because of some of the really tough technical challenges that came to light as the research world collided with the real world. Yet, these challenges are not insurmountable. Continued investments from both research and industry will help to overcome them, except that this time around we have civil society playing a more active role to ensure that the deployment of these systems is ethical, safe, and inclusive.  Jeff Clune, Research Team Leader, OpenAI  Interesting question, I think the AI is here to stay because it is useful and people can now take advantage of it. In that sense, there won't be a winter, but there will be times when the hype and expectations (of AI), outstrip the current capabilities, and so the hype will fade, while in the background it's continuing to be used at an ever increasing pace.  Aditya Grover, Research Scientist, Facebook AI Research  I am optimistic that we can steer clear of another AI winter by embracing the appropriate checks and balances at our disposal today. The previous winters were largely due to AI technologies that did not live up to their oversold promise. These risks exist today as well, especially with the hype around deep learning. However, the circumstances today are more robust to uncontrolled and unjustified hype from the winters of the 20th century in three major ways. First, the internet has laid the backbone for seamless access and exchange of information across a variety of disciplines. AI is no exception and in particular, sites such as arXiv and Github have helped spur a culture of openness in both research and development. Second, almost all major technology companies have made significant investments in fundamental and applied AI creating jobs for researchers and engineers and accelerating AI progress.  Notably, these companies run large AI labs that can effectively combine the research independence of academia with the engineering scale of industry; these labs publish papers in top-tier, peer-reviewed journals and conferences, release large-scale datasets and high-quality code, and help develop the software and hardware platforms critical for advancing AI. Third and last, the increasing diversity of individuals and organizations involved in developing, deploying, and auditing AI systems have helped highlight significant limitations of current AI technologies, such as the gender and racial biases in language models and facial recognition software. These efforts play a key role in mitigating the potential harm of AI systems and offering a more balanced perspective on their strengths and weaknesses.  The overall effect of such a 21st century ecosystem is that it incentivizes measurable, accountable, and holistic progress in AI, unlike the past winters.  Kavita Ganesan, Founder, Opinosis Analytics  From a business and practical application perspective, I don’t think we’ll be seeing another AI winter. Even though we’re currently in a pandemic, a Gartner survey shows us that 66% of organizations increased or didn’t change their AI investments since the onset of COVID-19. This is an indicator that companies are seeing AI as an important investment and not just a fad. The need for reduced human contact and social distancing will further propel the adoption of AI. For example, AI-powered robots are helping with medical pre-checks to limit contact with patients with COVID-19. The same robots are also helping COVID-19 patients connect with family and friends. There are also hospitals that are using AI to detect covid-induced pneumonia in patients due to a shortage of staff and long wait times.  We have machine learning algorithms that are mature and ready to solve practical problems. We have sufficient computing power to train algorithms to tackle many complex problems. Finally, although there’s a shortage of AI talent, more and more people are slowly becoming competent in AI. So why would there be an AI winter?  Further Reading:","research, today, winter, having, discuss, think, hype, potential, companies, 2020, arvr, right, experts, ai",2020-10-22 13:42:00+00:00
39,There won’t be an AI winter this time,Caleb Kaiser,https://medium.com/cortex-labs/there-wont-be-an-ai-winter-this-time-5a48c9d6c687,"There won’t be an AI winter this timeMachine learning isn’t a “Skynet or bust” proposition Caleb Kaiser · Follow 3 min read · Apr 3, 2020 -- Listen ShareDisclaimer: My observations are influenced by the fact that I work on Cortex, an open source machine learning deployment platform. Every few weeks, a new article predicting an imminent AI winter gets circulated. Production machine learning isn’t limited to tech giants, either. Machine learning isn’t a bet anymoreThe reason that hype cycles were able to crash AI investment in previous decades was that AI, and by extension machine learning, were essentially bets. Journalists who predict the Singularity will be here by Christmas might be wildly wrong, but they won’t be causing an AI winter.","There won’t be an AI winter this time  Machine learning isn’t a “Skynet or bust” proposition Caleb Kaiser · Follow 3 min read · Apr 3, 2020 -- Listen Share  Disclaimer: My observations are influenced by the fact that I work on Cortex, an open source machine learning deployment platform.  Every few weeks, a new article predicting an imminent AI winter gets circulated. The arguments generally follow the same lines:  The power of deep learning has been oversold to the public.  We are farther away from artificial general intelligence than reported.  Previous AI winters were caused by a similar hype bubble.  And while all of the above is true to an extent, the arguments miss something important: An AI winter similar to the one witnessed in the 80s, in which machine learning research and funding slowed to a crawl, simply isn’t possible anymore.  It’s true—we aren’t close to AGI or autonomous cars  For the last couple years, Starsky Robotics has been covered in major outlets as one of a few tech companies that would bring fully autonomous vehicles to the market. They had working demos, plenty of funding, and a talented team.  In March, Starsky Robotics shut down. In his post mortem (which you should read in its entirety), founder Stefan Seltz-Axmacher laid out the core reason very plainly: “supervised machine learning doesn’t live up to the hype.”  The hype he’s referring to is, of course, the endless promises from founders, journalists, and enthusiasts that technology like AGI and fully-autonomous vehicles are just months away. As Seltz-Axmacher reports, “Instead, the consensus (among researchers) has become that we’re at least 10 years away from self-driving cars.”  In the past, AI winters have been caused by similar cycles of exciting research leading to over-promises leading to disappointed investors and engineers giving up—but this time is different.  This time, even if the most grandiose promises of deep learning have not panned out, something else has happened:  Machine learning has become profitable.  Production machine learning is everywhere  Look at the most popular apps in the world:  Netflix, YouTube, Facebook, Amazon, Instagram, Spotify, and TikTok all rely heavily on ML-powered recommendation engines.  Snapchat, Instagram, and TikTok all use computer vision models to help users create, edit, and categorize visualize content.  Gmail and Messenger both use NLP to enhance messaging for users—filtering spam, suggesting text, categorizing messages, etc.  Google Maps, Uber, and Lyft rely on machine learning to calculate accurate ETA predictions.  That includes every single one of the most popular iOS apps.  These are the flagship products of the most valuable companies in the world—the same companies which, not coincidentally, are behind the majority of machine learning R&D. If you think any of these companies are going to stop investing in machine learning simply because they can’t build Skynet, you’re missing the point.  Production machine learning isn’t limited to tech giants, either. There are many startups who have already brought a product to market built on top of applied machine learning:  Onfido uses machine learning to provide identity verification services to over 1,500 financial organizations worldwide.  Ezra uses computer vision to provide full-body cancer screenings, currently operating in three states and growing.  AI Dungeon operates a ML-powered text adventure game built on OpenAI’s GPT-2. They have over 1,000,000 players.  Within virtually every industry—medicine, agriculture, gaming, finance, security, etc.—there are companies who have successfully brought a machine learning product to market.  Machine learning isn’t a bet anymore  The reason that hype cycles were able to crash AI investment in previous decades was that AI, and by extension machine learning, were essentially bets.  Founders and researchers were speculating about a future in which machine learning might have commercial applications. When those bets didn’t pay off, the market collapsed.  Machine learning is no longer a speculative proposition, it is a widely-applied, commercially viable technology powering some of the most popular (and profitable) companies in the world. Google isn’t going to disband Google Brain or stop funding TensorFlow because Starsky Robotics and OpenAI—who represent some of the most ambitious technological projects in history—stumbled a bit.  Journalists who predict the Singularity will be here by Christmas might be wildly wrong, but they won’t be causing an AI winter.","popular, robotics, winter, similar, companies, starsky, isnt, wont, machine, learning, ai",2020-04-03 23:08:03.795000+00:00
40,AI winter,,https://en.wikipedia.org/wiki/AI_winter,"Period of reduced funding and interest in AI researchIn the history of artificial intelligence, an AI winter is a period of reduced funding and interest in artificial intelligence research. Roger Schank and Marvin Minsky—two leading AI researchers who experienced the ""winter"" of the 1970s—warned the business community that enthusiasm for AI had spiraled out of control in the 1980s and that disappointment would certainly follow. [6]Just like all AI booms that have been followed by desperate AI winters, the media tended to exaggerate the significance of these developments. ""[25] The result, Moravec claims, is that some of the staff at DARPA had lost patience with AI research. In 2006, Nick Bostrom explained that ""a lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.","Period of reduced funding and interest in AI research  In the history of artificial intelligence, an AI winter is a period of reduced funding and interest in artificial intelligence research.[1] The field has experienced several hype cycles, followed by disappointment and criticism, followed by funding cuts, followed by renewed interest years or even decades later.  The term first appeared in 1984 as the topic of a public debate at the annual meeting of AAAI (then called the ""American Association of Artificial Intelligence""). Roger Schank and Marvin Minsky—two leading AI researchers who experienced the ""winter"" of the 1970s—warned the business community that enthusiasm for AI had spiraled out of control in the 1980s and that disappointment would certainly follow. They described a chain reaction, similar to a ""nuclear winter"", that would begin with pessimism in the AI community, followed by pessimism in the press, followed by a severe cutback in funding, followed by the end of serious research. Three years later the billion-dollar AI industry began to collapse.  There were two major winters approximately 1974–1980 and 1987–2000,[3] and several smaller episodes, including the following:  Enthusiasm and optimism about AI has generally increased since its low point in the early 1990s. Beginning about 2012, interest in artificial intelligence (and especially the sub-field of machine learning) from the research and corporate communities led to a dramatic increase in funding and investment, leading to the current (as of 2024 ) AI boom.  Early episodes [ edit ]  Machine translation and the ALPAC report of 1966 [ edit ]  Natural language processing (NLP) research has its roots in the early 1930s and began its existence with the work on machine translation (MT).[4] However, significant advancements and applications began to emerge after the publication of Warren Weaver's influential memorandum in 1949.[5] The memorandum generated great excitement within the research community. In the following years, notable events unfolded: IBM embarked on the development of the first machine, MIT appointed its first full-time professor in machine translation, and several conferences dedicated to MT took place. The culmination came with the public demonstration of the IBM-Georgetown machine, which garnered widespread attention in respected newspapers in 1954.[6]  Just like all AI booms that have been followed by desperate AI winters, the media tended to exaggerate the significance of these developments. Headlines about the IBM-Georgetown experiment proclaimed phrases like ""The bilingual machine,"" ""Robot brain translates Russian into King's English,""[7] and ""Polyglot brainchild.""[8] However, the actual demonstration involved the translation of a curated set of only 49 Russian sentences into English, with the machine's vocabulary limited to just 250 words.[6] To put things into perspective, a 2006 study made by Paul Nation found that humans need a vocabulary of around 8,000 to 9,000-word families to comprehend written texts with 98% accuracy.[9]  During the Cold War, the US government was particularly interested in the automatic, instant translation of Russian documents and scientific reports. The government aggressively supported efforts at machine translation starting in 1954. Another factor that propelled the field of mechanical translation was the interest shown by the Central Intelligence Agency (CIA). During that period, the CIA firmly believed in the importance of developing machine translation capabilities and supported such initiatives. They also recognized that this program had implications that extended beyond the interests of the CIA and the intelligence community.[6]  At the outset, the researchers were optimistic. Noam Chomsky's new work in grammar was streamlining the translation process and there were ""many predictions of imminent 'breakthroughs'"".[10]  Briefing for US Vice President Gerald Ford in 1973 on the junction-grammar-based computer translation model  However, researchers had underestimated the profound difficulty of word-sense disambiguation. In order to translate a sentence, a machine needed to have some idea what the sentence was about, otherwise it made mistakes. An apocryphal[11] example is ""the spirit is willing but the flesh is weak."" Translated back and forth with Russian, it became ""the vodka is good but the meat is rotten."" Later researchers would call this the commonsense knowledge problem.  By 1964, the National Research Council had become concerned about the lack of progress and formed the Automatic Language Processing Advisory Committee (ALPAC) to look into the problem. They concluded, in a famous 1966 report, that machine translation was more expensive, less accurate and slower than human translation. After spending some 20 million dollars, the NRC ended all support. Careers were destroyed and research ended.[10]  Machine translation shared the same path with NLP from the rule-based approaches through the statistical approaches up to the neural network approaches, which have in 2023 culminated in large language models.  The failure of single-layer neural networks in 1969 [ edit ]  Simple networks or circuits of connected units, including Walter Pitts and Warren McCulloch's neural network for logic and Marvin Minsky's SNARC system, have failed to deliver the promised results and were abandoned in the late 1950s. Following the success of programs such as the Logic Theorist and the General Problem Solver,[13] algorithms for manipulating symbols seemed more promising at the time as means to achieve logical reasoning viewed at the time as the essence of intelligence, either natural or artificial.  Interest in perceptrons, invented by Frank Rosenblatt, was kept alive only by the sheer force of his personality.[14] He optimistically predicted that the perceptron ""may eventually be able to learn, make decisions, and translate languages"".[15] Mainstream research into perceptrons ended partially because the 1969 book Perceptrons by Marvin Minsky and Seymour Papert emphasized the limits of what perceptrons could do. While it was already known that multilayered perceptrons are not subject to the criticism, nobody in the 1960s knew how to train a multilayered perceptron. Backpropagation was still years away.[17]  Major funding for projects neural network approaches was difficult to find in the 1970s and early 1980s.[18] Important theoretical work continued despite the lack of funding. The ""winter"" of neural network approach came to an end in the middle 1980s, when the work of John Hopfield, David Rumelhart and others revived large scale interest.[19] Rosenblatt did not live to see this, however, as he died in a boating accident shortly after Perceptrons was published.[15]  The setbacks of 1974 [ edit ]  The Lighthill report [ edit ]  In 1973, professor Sir James Lighthill was asked by the UK Parliament to evaluate the state of AI research in the United Kingdom. His report, now called the Lighthill report, criticized the utter failure of AI to achieve its ""grandiose objectives"". He concluded that nothing being done in AI could not be done in other sciences. He specifically mentioned the problem of ""combinatorial explosion"" or ""intractability"", which implied that many of AI's most successful algorithms would grind to a halt on real world problems and were only suitable for solving ""toy"" versions.[20]  The report was contested in a debate broadcast in the BBC ""Controversy"" series in 1973. The debate ""The general purpose robot is a mirage"" from the Royal Institution was Lighthill versus the team of Donald Michie, John McCarthy and Richard Gregory.[21] McCarthy later wrote that ""the combinatorial explosion problem has been recognized in AI from the beginning"".[22]  The report led to the complete dismantling of AI research in the UK.[20] AI research continued in only a few universities (Edinburgh, Essex and Sussex). Research would not revive on a large scale until 1983, when Alvey (a research project of the British Government) began to fund AI again from a war chest of £350 million in response to the Japanese Fifth Generation Project (see below). Alvey had a number of UK-only requirements which did not sit well internationally, especially with US partners, and lost Phase 2 funding.  DARPA's early 1970s funding cuts [ edit ]  During the 1960s, the Defense Advanced Research Projects Agency (then known as ""ARPA"", now known as ""DARPA"") provided millions of dollars for AI research with few strings attached. J. C. R. Licklider, the founding director of DARPA's computing division, believed in ""funding people, not projects""[23] and he and several successors allowed AI's leaders (such as Marvin Minsky, John McCarthy, Herbert A. Simon or Allen Newell) to spend it almost any way they liked.  This attitude changed after the passage of Mansfield Amendment in 1969, which required DARPA to fund ""mission-oriented direct research, rather than basic undirected research"".[24] Pure undirected research of the kind that had gone on in the 1960s would no longer be funded by DARPA. Researchers now had to show that their work would soon produce some useful military technology. AI research proposals were held to a very high standard. The situation was not helped when the Lighthill report and DARPA's own study (the American Study Group) suggested that most AI research was unlikely to produce anything truly useful in the foreseeable future. DARPA's money was directed at specific projects with identifiable goals, such as autonomous tanks and battle management systems. By 1974, funding for AI projects was hard to find.[24]  AI researcher Hans Moravec blamed the crisis on the unrealistic predictions of his colleagues: ""Many researchers were caught up in a web of increasing exaggeration. Their initial promises to DARPA had been much too optimistic. Of course, what they delivered stopped considerably short of that. But they felt they couldn't in their next proposal promise less than in the first one, so they promised more.""[25] The result, Moravec claims, is that some of the staff at DARPA had lost patience with AI research. ""It was literally phrased at DARPA that 'some of these people were going to be taught a lesson [by] having their two-million-dollar-a-year contracts cut to almost nothing!'"" Moravec told Daniel Crevier.[26]  While the autonomous tank project was a failure, the battle management system (the Dynamic Analysis and Replanning Tool) proved to be enormously successful, saving billions in the first Gulf War, repaying all of DARPAs investment in AI[27] and justifying DARPA's pragmatic policy.[28]  The SUR debacle [ edit ]  As described in:[29]  In 1971, the Defense Advanced Research Projects Agency (DARPA) began an ambitious five-year experiment in speech understanding. The goals of the project were to provide recognition of utterances from a limited vocabulary in near-real time. Three organizations finally demonstrated systems at the conclusion of the project in 1976. These were Carnegie-Mellon University (CMU), who actually demonstrated two system [HEARSAY-II and HARPY]; Bolt, Beranek and Newman (BBN); and System Development Corporation with Stanford Research Institute (SDC/SRI)  The system that came closest to satisfying the original project goals was the CMU HARPY system. The relatively high performance of the HARPY system was largely achieved through 'hard-wiring' information about possible utterances into the system's knowledge base. Although HARPY made some interesting contributions, its dependence on extensive pre-knowledge limited the applicability of the approach to other signal-understanding tasks.  DARPA was deeply disappointed with researchers working on the Speech Understanding Research program at Carnegie Mellon University. DARPA had hoped for, and felt it had been promised, a system that could respond to voice commands from a pilot. The SUR team had developed a system which could recognize spoken English, but only if the words were spoken in a particular order. DARPA felt it had been duped and, in 1974, they cancelled a three million dollar a year contract.[30]  Many years later, several successful commercial speech recognition systems would use the technology developed by the Carnegie Mellon team (such as hidden Markov models) and the market for speech recognition systems would reach $4 billion by 2001.[31]  For a description of Hearsay-II see Hearsay-II, The Hearsay-II Speech Understanding System: Integrating Knowledge to Resolve Uncertainty and A Retrospective View of the Hearsay-II Architecture which appear in Blackboard Systems[32]  Reddy gives a review of progress in speech understanding at the end of the DARPA project in a 1976 article in Proceedings of the IEEE.[33]  Contrary view [ edit ]  Thomas Haigh argues that activity in the domain of AI did not slow down, even as funding from DoD was being redirected, mostly in the wake of congressional legislation meant to separate military and academic activities.[34] That indeed professional interest was growing throughout the 70s. Using the membership count of ACM's SIGART, the Special Interest Group on Artificial Intelligence, as a proxy for interest in the subject, the author writes:[34]  (...) I located two data sources, neither of which supports the idea of a broadly based AI winter during the 1970s. One is membership of ACM's SIGART, the major venue for sharing news and research abstracts during the 1970s. When the Lighthill report was published in 1973 the fast-growing group had 1,241 members, approximately twice the level in 1969. The next five years are conventionally thought of as the darkest part of the first AI winter. Was the AI community shrinking? No! By mid-1978 SIGART membership had almost tripled, to 3,500. Not only was the group growing faster than ever, it was increasing proportionally faster than ACM as a whole which had begun to plateau (expanding by less than 50% over the entire period from 1969 to 1978). One in every 11 ACM members was in SIGART.  The setbacks of the late 1980s and early 1990s [ edit ]  The collapse of the LISP machine market [ edit ]  In the 1980s, a form of AI program called an ""expert system"" was adopted by corporations around the world. The first commercial expert system was XCON, developed at Carnegie Mellon for Digital Equipment Corporation, and it was an enormous success: it was estimated to have saved the company 40 million dollars over just six years of operation. Corporations around the world began to develop and deploy expert systems and by 1985 they were spending over a billion dollars on AI, most of it to in-house AI departments. An industry grew up to support them, including software companies like Teknowledge and Intellicorp (KEE), and hardware companies like Symbolics and LISP Machines Inc. who built specialized computers, called LISP machines, that were optimized to process the programming language LISP, the preferred language for AI research in the USA.[35][36]  In 1987, three years after Minsky and Schank's prediction, the market for specialized LISP-based AI hardware collapsed. Workstations by companies like Sun Microsystems offered a powerful alternative to LISP machines and companies like Lucid offered a LISP environment for this new class of workstations. The performance of these general workstations became an increasingly difficult challenge for LISP Machines. Companies like Lucid and Franz LISP offered increasingly powerful versions of LISP that were portable to all UNIX systems. For example, benchmarks were published showing workstations maintaining a performance advantage over LISP machines.[37] Later desktop computers built by Apple and IBM would also offer a simpler and more popular architecture to run LISP applications on. By 1987, some of them had become as powerful as the more expensive LISP machines. The desktop computers had rule-based engines such as CLIPS available.[38] These alternatives left consumers with no reason to buy an expensive machine specialized for running LISP. An entire industry worth half a billion dollars was replaced in a single year.[39]  By the early 1990s, most commercial LISP companies had failed, including Symbolics, LISP Machines Inc., Lucid Inc., etc. Other companies, like Texas Instruments and Xerox, abandoned the field. A small number of customer companies (that is, companies using systems written in LISP and developed on LISP machine platforms) continued to maintain systems. In some cases, this maintenance involved the assumption of the resulting support work.[40]  Slowdown in deployment of expert systems [ edit ]  By the early 1990s, the earliest successful expert systems, such as XCON, proved too expensive to maintain. They were difficult to update, they could not learn, they were ""brittle"" (i.e., they could make grotesque mistakes when given unusual inputs), and they fell prey to problems (such as the qualification problem) that had been identified years earlier in research in nonmonotonic logic. Expert systems proved useful, but only in a few special contexts.[41][42] Another problem dealt with the computational hardness of truth maintenance efforts for general knowledge. KEE used an assumption-based approach supporting multiple-world scenarios that was difficult to understand and apply.  The few remaining expert system shell companies were eventually forced to downsize and search for new markets and software paradigms, like case-based reasoning or universal database access. The maturation of Common Lisp saved many systems such as ICAD which found application in knowledge-based engineering. Other systems, such as Intellicorp's KEE, moved from LISP to a C++ (variant) on the PC and helped establish object-oriented technology (including providing major support for the development of UML (see UML Partners).  The end of the Fifth Generation project [ edit ]  In 1981, the Japanese Ministry of International Trade and Industry set aside $850 million for the Fifth Generation computer project. Their objectives were to write programs and build machines that could carry on conversations, translate languages, interpret pictures, and reason like human beings. By 1991, the impressive list of goals penned in 1981 had not been met. According to HP Newquist in The Brain Makers, ""On June 1, 1992, The Fifth Generation Project ended not with a successful roar, but with a whimper.""[40] As with other AI projects, expectations had run much higher than what was actually possible.[43][44]  Strategic Computing Initiative cutbacks [ edit ]  In 1983, in response to the fifth generation project, DARPA again began to fund AI research through the Strategic Computing Initiative. As originally proposed the project would begin with practical, achievable goals, which even included artificial general intelligence as long-term objective. The program was under the direction of the Information Processing Technology Office (IPTO) and was also directed at supercomputing and microelectronics. By 1985 it had spent $100 million and 92 projects were underway at 60 institutions, half in industry, half in universities and government labs. AI research was well-funded by the SCI.[45]  Jack Schwarz, who ascended to the leadership of IPTO in 1987, dismissed expert systems as ""clever programming"" and cut funding to AI ""deeply and brutally"", ""eviscerating"" SCI. Schwarz felt that DARPA should focus its funding only on those technologies which showed the most promise, in his words, DARPA should ""surf"", rather than ""dog paddle"", and he felt strongly AI was not ""the next wave"". Insiders in the program cited problems in communication, organization and integration. A few projects survived the funding cuts, including pilot's assistant and an autonomous land vehicle (which were never delivered) and the DART battle management system, which (as noted above) was successful.[46]  AI winter of the 1990's and early 2000's [ edit ]  A survey of reports from the early 2000's suggests that AI's reputation was still poor:  Alex Castro, quoted in The Economist , 7 June 2007: ""[Investors] were put off by the term 'voice recognition' which, like 'artificial intelligence', is associated with systems that have all too often failed to live up to their promises."" [ 47 ]  , 7 June 2007: ""[Investors] were put off by the term 'voice recognition' which, like 'artificial intelligence', is associated with systems that have all too often failed to live up to their promises."" Patty Tascarella in Pittsburgh Business Times , 2006: ""Some believe the word 'robotics' actually carries a stigma that hurts a company's chances at funding."" [ 48 ]  , 2006: ""Some believe the word 'robotics' actually carries a stigma that hurts a company's chances at funding."" John Markoff in the New York Times, 2005: ""At its low point, some computer scientists and software engineers avoided the term artificial intelligence for fear of being viewed as wild-eyed dreamers."" [ 49 ]  Many researchers in AI in the mid 2000's deliberately called their work by other names, such as informatics, machine learning, analytics, knowledge-based systems, business rules management, cognitive systems, intelligent systems, intelligent agents or computational intelligence, to indicate that their work emphasizes particular tools or is directed at a particular sub-problem. Although this may be partly because they consider their field to be fundamentally different from AI, it is also true that the new names help to procure funding by avoiding the stigma of false promises attached to the name ""artificial intelligence"".[49][50]  In the late 1990's and early 21st century, AI technology became widely used as elements of larger systems,[51] but the field is rarely credited for these successes. In 2006, Nick Bostrom explained that ""a lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.""[53] Rodney Brooks stated around the same time that ""there's this stupid myth out there that AI has failed, but AI is around you every second of the day.""  Current AI spring (2022–present) [ edit ]  AI has reached the highest levels of interest and funding in its history in the past few years by every possible measure, including: publications, patent applications,[56] total investment ($50 billion in 2022), and job openings (800,000 U.S. job openings in 2022). The successes of the current ""AI spring"" or ""AI boom"" are advances in language translation (in particular, Google Translate), image recognition (spurred by the ImageNet training database) as commercialized by Google Image Search, and in game-playing systems such as AlphaZero (chess champion) and AlphaGo (go champion), and Watson (Jeopardy champion). A turning point was in 2012 when AlexNet (a deep learning network) won the ImageNet Large Scale Visual Recognition Challenge with half as many errors as the second place winner.  The 2022 release of OpenAI's AI chatbot ChatGPT which as of January 2023 has over 100 million users,[60] has reinvigorated the discussion about artificial intelligence and its effects on the world.[61][62]  See also [ edit ]  Notes [ edit ]  References [ edit ]","research, translation, lisp, winter, funding, system, systems, intelligence, machine, ai, edit",
41,Is Another AI Winter Coming?,,https://hackernoon.com/is-another-ai-winter-coming-ac552669e58c,"Such failures would terminate AI research for decades. It is worth mentioning that these invaluable tools indeed came from AI research, but they were rebranded since they failed to live up to their grander purposes. By 2015, “AI” research commanded huge budgets of many Fortune 500 companies. If you want a more thorough explanation, check out Grant Sanderson’s amazing video series on neural networks here:An AI Renaissance? It is for these reasons I think another AI Winter is coming.","And, has deep learning already hit its limitations?  Many believed an algorithm would transcend humanity with cognitive awareness. Machines would discern and learn tasks without human intervention and replace workers in droves. They quite literally would be able to “think”. Many people even raised the question whether we could have robots for spouses.  But I am not talking about today. What if I told you this idea was widely marketed in the 1960’s, and AI pioneers Jerome Wiesner, Oliver Selfridge, and Claude Shannon insisted this could happen in their near future? If you find this surprising, watch this video and be amazed how familiar these sentiments are.  Fast forward to 1973, and the hype and exaggeration of AI backfired. The U.K. Parliament sent Sir James Lighthill to get a status report of A.I. research in the U.K. The report criticized the failure of artificial intelligence research to live up to its sensational claims. Interestingly, Lighthill also pointed out how specialized programs (or people) performed better than their “AI” counterparts, and had no prospects in real-world environments. Consequently, AI research funding was cancelled by the British government.  Across the pond, the United States Department of Defense was invested heavily in AI research, but then cancelled nearly all funding over the same frustrations: exaggeration of AI ability, high costs with no return, and dubious value in the real world.  In the 1980’s, Japan enthusiastically attempted a bold stab at “AI” with the Fifth Generation Project (EDIT: Toby Walsh himself corrected me in the comments. UK research did pick up again in the 1980’s with the Alvey Project in response to Japan). However, that ended up being a costly $850 million failure as well.  The First AI Winter  The end of the 1980’s brought forth an A.I. Winter, a dark period in computer science where “artificial intelligence” research burned organizations and governments with delivery failures and sunk costs. Such failures would terminate AI research for decades.  Oftentimes, these companies were driven by FOMO rather than practical use cases, worried that they would be left behind by their automated competitors.  By the time the 1990’s rolled around, “AI” became a dirty word and continued to be in the 2000’s. It was widely accepted that “AI just didn’t work”. Software companies who wrote seemingly intelligent programs would use terms like “search algorithms”, “business rule engines”, “constraint solvers”, and “operations research”. It is worth mentioning that these invaluable tools indeed came from AI research, but they were rebranded since they failed to live up to their grander purposes.  But around 2010, something started to change. There was rapidly growing interest in AI again and competitions in categorizing images caught the media’s eye. Silicon Valley was sitting on huge amounts of data, and for the first time there was enough to possibly make neural networks useful.  By 2015, “AI” research commanded huge budgets of many Fortune 500 companies. Oftentimes, these companies were driven by FOMO rather than practical use cases, worried that they would be left behind by their automated competitors. After all, having a neural network is nothing short of impressive! To the layperson, SkyNet capabilities must surely be next.  But is this really a step towards true AI? Or is history repeating itself, but this time emboldened by a handful of successful use cases?  What is AI Anyway?  For a long time, I have never liked the term “artificial intelligence”. It is vague and far-reaching, and defined more by marketing folks than scientists. Of course, marketing and buzzwords are arguably necessary to spur positive change. However, buzzword campaigns inevitably lead to confusion. My new ASUS smart phone has an “AI Ringtone” feature that dynamically adjusts the ring volume to be just loud enough over ambient noise. I guess something that literally could be programmed with a series of `if` conditions, or a simple linear function, is called “AI”. Alrighty then.  In light of that, it is probably no surprise the definition of “AI” is widely disputed. I like Geoffrey De Smet’s definition, which states AI solutions are for problems with a nondeterministic answer and/or an inevitable margin of error. This would include a wide array of tools from machine learning to probability and search algorithms.  It can also be said that the definition of AI evolves and only includes ground-breaking developments, while yesterday’s successes (like optical character recognition or language translators) are no longer considered “AI”. So “artificial intelligence” can be a relative term and hardly absolute.  In recent years, “AI” has often been associated with “neural networks” which is what this article will focus on. There are other “AI” solutions out there, from other machine learning models (Naive Bayes, Support Vector Machines, XGBoost) to search algorithms. However, neural networks are arguably the hottest and most hyped technology at the moment. If you want to learn more about neural networks, I posted my video below.  If you want a more thorough explanation, check out Grant Sanderson’s amazing video series on neural networks here:  An AI Renaissance?  The resurgence of AI hype after 2010 is simply due to a new class of tasks being mastered: categorization. More specifically, scientists have developed effective ways to categorize most types of data including images and natural language thanks to neural networks. Even self-driving cars are categorization tasks, where each image of the surrounding road translates into a set of discrete actions (gas, break, turn left, turn right, etc). To get a simplified idea of how this works, watch .  In my opinion, Natural language processing is more impressive than pure categorization though. It is easy to believe these algorithms are sentient, but if you study them carefully you can tell they are relying on language patterns rather than consciously-constructed thoughts. These can lead to some entertaining results, like these bots that will troll scammers for you:  Probably the most impressive feat of natural language processing is Google Duplex, which allows your Android phone to make phone calls on your behalf, specifically for appointments. However, you have to consider that Google trained, structured, and perhaps even hardcoded the “AI” just for that task. And sure, the fake caller sounds natural with pauses, “ahhs”, and “uhms”… but again, this was done through operations on speech patterns, not actual reasoning and thoughts.  This is all very impressive, and definitely has some useful applications. But we really need to temper our expectations and stop hyping “deep learning” capabilities. If we don’t, we may find ourselves in another AI Winter.  History Repeats Itself  Gary Marcus at NYU wrote an interesting article on the limitations of deep learning, and poses several sobering points (he also wrote an equally interesting follow-up after the article went viral). Rodney Brooks is putting timelines together and keeping track of his AI hype cycle predictions, and predicts we will see “ The Era of Deep Learning is Over” headlines in 2020.  The skeptics generally share a few key points. Neural networks are data-hungry and even today, data is finite. This is also why “game” AI examples you see on YouTube (like as well as ) often require days of constant losing gameplay until the neural network finds a pattern that allows it to win.  We really need to temper our expectations and stop hyping “deep learning” capabilities. If we don’t, we may find ourselves in another AI Winter.  Neural networks are “deep” in that they technically have several layers of nodes, not because it develops deep understanding about the problem. These layers also make the neural networks difficult to understand, even for its developer. Most importantly, neural networks are experiencing diminishing return when they venture out into other problem spaces, such as the Traveling Salesman Problem. And this makes sense. Why in the world would I solve the Traveling Salesman Problem with a neural network when a search algorithm will be much more straightforward, effective, scalable, and economical (as shown in the video below)?  Using search algorithms like simulated annealing for the Traveling Salesman Problem  Nor would I use deep learning to solve other everyday “AI” problems, like solving Sudokus or packing events into a schedule, which I discuss how to do in a separate article:      Sudokus and Schedules_Solving Scheduling Problems with Tree Search_towardsdatascience.com  Of course, there are folks looking to generalize more problem spaces into neural networks, and while that is interesting it rarely seems to outperform any specialized algorithms.  Luke Hewitt at MIT puts it best in this article:  It is a bad idea to intuit how broadly intelligent a machine must be, or have the capacity to be, based solely on a single task. The checkers-playing machines of the 1950s amazed researchers and many considered these a huge leap towards human-level reasoning, yet we now appreciate that achieving human or superhuman performance in this game is far easier than achieving human-level general intelligence. In fact, even the best humans can easily be defeated by a search algorithm with simple heuristics. Human or superhuman performance in one task is not necessarily a stepping-stone towards near-human performance across most tasks.  — Luke Hewitt  I think it is also worth pointing out that neural networks require vast amounts of hardware and energy to train. To me, that just does not feel sustainable. Of course, a neural network will predict much more efficiently than it trains. However I do think the ambitions people have for neural networks will demand constant training and therefore require exponential energy and costs. And sure, computers keep getting faster but can chip manufacturers struggle past the failure of Moore’s Law?  A final point to consider is the P versus NP problem. To describe this in the simplest terms possible, proving P = NP would mean we could calculate solutions to very difficult problems (like machine learning, cryptography, and optimization) just as quickly as we can verify them. Such a breakthrough would expand the capabilities of AI algorithms drastically and maybe transform our world beyond recognition (Fun fact: there’s a 2012 intellectual thriller movie called The Travelling Salesman which explores this idea).  Here is a great video that explains the P versus NP problem, and it is worth the 10 minutes to watch:  An explanation of P versus NP  Sadly after 50 years since the problem was formalized, more computer scientists are coming to believe that P does not equal NP. In my opinion, this is an enormous barrier to AI research that we may never overcome, as this means complexity will always limit what we can do.  And this makes sense. Why in the world would I solve the Traveling Salesman Problem with a neural network when a search algorithm will be much more effective, scalable, and economical?  It is for these reasons I think another AI Winter is coming. In 2018, a growing number of experts, articles, forum posts, and bloggers came forward calling out these limitations. I think this skepticism trend is going to intensify in 2019 and will go mainstream as soon as 2020. Companies are still sparing little expense in getting the best “deep learning” and “AI” talent, but I think it is a matter of time before many companies realize deep learning is not what they need. Even worse, if your company does not have Google’s research budget, the PhD talent, or massive data store it collected from users, you can quickly find your practical “deep learning” prospects very limited. This was best captured in this scene from the HBO show Silicon Valley (WARNING: language):  Each AI Winter is preceded with scientists exaggerating and hyping the potential of their creations. It is not enough to say their algorithm can do one task well. They want it to ambitiously adapt to any task, or at least give the impression it can. For instance, AlphaZero makes a better chess playing algorithm. Media’s reaction is “Oh my gosh, general AI is here. Everybody run for cover! The robots are coming!” Then the scientists do not bother correcting them and actually encourage it using clever choices of words. Tempering expectations does not help VC funding after all. But there could be other reasons why AI researchers anthropomorphize algorithms despite their robotic limitations, and it is more philosophical than scientific. I will save that for the end of the article.  So What’s Next?  Of course, not every company using “machine learning” or “AI” is actually using “deep learning.” A good data scientist may have been hired to build a neural network, but when she actually studies the problem she more appropriately builds a instead. For the companies that are successfully using image recognition and language processing, they will continue to do so happily. But I do think neural networks are not going to progress far from those problem spaces.  Tempering expectations does not help VC funding after all.  The AI Winters of the past were devastating in pushing the boundaries of computer science. It is worth pointing out that useful things came out of such research, like search algorithms which can effectively win at chess or minimize costs in transportation problems. Simply put, innovative algorithms emerged that often excelled at one particular task.  The point I am making is there are many proven solutions out there for many types of problems. To avoid getting put out in the cold by an AI Winter, the best thing you can do is be specific about the problem you are trying to solve and understand its nature. After that, find approaches that provide an intuitive path to a solution for that particular problem. If you want to categorize text messages, you probably want to use . If you are trying to optimize your transportation network, you likely should use Discrete Optimization. No matter the peer pressure, you are allowed to approach convoluted models with a healthy amount of skepticism, and question whether it is the right approach.  Hopefully this article made it abundantly clear deep learning is not the right approach for most problems. There is no free lunch. Do not hit the obstacle of seeking a generalized AI solution for all your problems, because you are not going to find one.  Are Our Thoughts Really Dot Products? Philosophy vs Science  One last point I want to throw in this article, and it is more philosophical than scientific. Is every thought and feeling we have simply a bunch of numbers being multiplied and added in linear algebra fashion? Are our brains, in fact, simply a neural network doing dot products all day? That sounds almost like a Pythagorean philosophy that reduces our consciousness to a matrix of numbers. Perhaps this is why so many scientists believe general artificial intelligence is possible, as being human is no different than being a computer. (I’m just pointing this out, not commenting whether this worldview is right or wrong).  No matter the peer pressure, you are allowed to approach convoluted models with a healthy amount of skepticism, and question whether it is the right approach.  If you do not buy into this Pythagorean philosophy, then the best you can strive for is have AI “simulate” actions that give the illusion it has sentiments and thoughts. A translation program does not understand Chinese. It “simulates” the illusion of understanding Chinese by finding probabilistic patterns. When your smartphone “recognizes” a picture of a dog, does it really recognize a dog? Or does it see a grid of numbers it saw before?  This article was originally published on Towards Data Science.  Related Articles:      Data Science in Tech - Hacker Noon_Data science is an interdisciplinary field that uses scientific methods, processes, algorithms and systems to extract…_hackernoon.com      Are Our Thoughts Really Dot Products?_How AI Research Revived Pythagoreanism and Confused Science with Philosophy_towardsdatascience.com      How It Feels to Learn Data Science in 2019_Seeing the (Random) Forest Through the (Decision) Trees_towardsdatascience.com      Data - Hacker Noon_Read writing about Data in Hacker Noon. how hackers start their afternoons._hackernoon.com","neural, research, coming, algorithms, winter, deep, problem, problems, search, learning, ai, networks",
42,AI winter - Addendum,,https://blog.piekniewski.info/2018/06/06/ai-winter-addendum/,"My previous post on AI winter went viral almost to the point of killing my Amazon instance (it got well north of 100k views). From this empirical evidence one thing is clear - whether the AI winter is close or not, it is a very sensitive and provocative subject. First off, many citations to my post were put in context that the AI hype is fading. It is only once there is nobody to buy this, which is long after seed financing had dried out, when the AI winter becomes official. The main line of defense against the AI winter is that this time around AI actually brings profit and there are real applications.","My previous post on AI winter went viral almost to the point of killing my Amazon instance (it got well north of 100k views). It triggered a serious tweet storms, lots of discussion on hackernews and reddit. From this empirical evidence one thing is clear - whether the AI winter is close or not, it is a very sensitive and provocative subject. Almost as if many people felt something under their skin...  Anyway, in this quick followup post, I'd like to respond to some of the points and explain some misunderstandings.  Hype is not fading, it is cracking.  First off, many citations to my post were put in context that the AI hype is fading. This was not my point at all. The hype is doing very well. Some of the major propagandists have gone quieter but much like I explained in the post, on the surface everything is still nice and colorful. You have to look below the propaganda to see the cracks. It would actually be great if the hype faded down but that is not how it works. When the stock market crashes, it is not like everybody slowly begin to admit that they overpaid for their stocks and quietly go home. It happens in sudden, violent attacks of panic, where everybody tries to sell off, while at the same time, same people pump the narrative so that they could find buyers [pump and dump]. Crisis is only announced once the market truly runs out of buyers and the pumpers run out of cash. So hype is a lagging indicator (often by quite a bit). I predict it will be like that with AI. Each fatality caused by a self driving car will cut the number of VC's likely to invest in AI by half. Same for every AI startup that quietly folds down. At the same time those who already heavily invested, will be pumping the propaganda while quietly trying to liquidate their assets. It is only once there is nobody to buy this, which is long after seed financing had dried out, when the AI winter becomes official.  Applications stupid!  The main line of defense against the AI winter is that this time around AI actually brings profit and there are real applications. OK. There are applications. Primarily for image search, speech recognition and perhaps surveillance (aka Google and Facebook etc.). There is the style transfer which will certainly make Photoshop great again. But these are all almost 3 years old now. I actually went to ""Applications of Deep Learning"" session on last ICML conference in Sydney. Let me just put it very mildly: this was a superbly underwhelming session.  Now regarding influence on winter, it actually does not matter how much money AI brings today. What matters is how much people invested, and hence how much return they expect in the future. If the reality does not match these expectations there will be a winter. Period. The amount of investment in AI in this cycle is enormous. And the focal point of that investment is in autonomous vehicles and by autonomous I don't mean remote controlled or with a safety driver - this stuff is only economical if they are truly autonomous. Coincidentally, this is the application which I think has the smallest chance of materializing.  But Waymo!  But Waymo what? That they are buying up to 60,000 vans over undefined period of time? So what? Uber ordered 20,000 Volvos late last year. I wonder how that deal is going. But Waymo tests AV's without safety drivers! Yes, in the most quiet and slow block in Phenix with perfect cellular reception such that they could constantly monitor these cars remotely. Oh and BTW, they have a speed limit of 25 mph... Anyway long story short: Waymo can deploy even a million LTE monitored, remote controlled cars. That proves nothing regarding autonomous car, because such deployment will happen at a massive loss. Obviously Google can pump money into them for as long as Google has the money, which will probably be for a while. Google AV project has been around for 10 years, I expect it to go for another 10 years. Unless they hit and kill somebody. At this point they are done. And that is why they are extremely cautious.  A few recent examples of Deep fail  After stirring a violent tweet storm with my post, a few very interesting papers came out and a few other had been brought to my attention:  These combined with good old gradient derived adversarial examples just exemplify how brittle these methods are. We are far away from robust perception and in my opinion we are stuck in a wrong paradigm and hence we are not even moving in the right direction.  Happy reading!  If you found an error, highlight it and press Shift + Enter or click here to inform us.  Related  Comments  comments","autonomous, google, quietly, winter, addendum, hype, post, actually, waymo, point, ai",2018-06-06 00:00:00
43,AI Winter Isn’t Coming,Will Knight,https://www.technologyreview.com/s/603062/ai-winter-isnt-coming/,"Andrew Ng, chief scientist at Baidu Research, and a major figure in the field of machine learning and AI, says improvements in computer processor design will keep performance advances and breakthroughs coming for the foreseeable future. The advances seen in recent years have come thanks to the development of powerful “deep learning” systems (see “10 Breakthrough Technologies 2013: Deep Learning”). This might not only increase the accuracy of existing deep learning tools, but also allow the technique to be leveraged in new areas, such as parsing and generating language. What’s more, Ng says, hardware advances will provide the fuel required to make emerging AI techniques feasible. The world’s leading AI experts convened in Barcelona this week for a prominent event called the Neural Information Processing Systems conference.","Andrew Ng, chief scientist at Baidu Research, and a major figure in the field of machine learning and AI, says improvements in computer processor design will keep performance advances and breakthroughs coming for the foreseeable future. “Multiple [hardware vendors] have been kind enough to share their roadmaps,” Ng says. “I feel very confident that they are credible and we will get more computational power and faster networks in the next several years.”  The field of AI has gone through phases of rapid progress and hype in the past, quickly followed by a cooling in investment and interest, often referred to as “AI winters.” The first chill occurred in the 1970s, as progress slowed and government funding dried up; another struck in the 1980s as the latest trends failed to have the expected commercial impact.  Then again, there’s perhaps been no boom to match the current one, propelled by rapid progress in training machines to do useful tasks. Artificial intelligence researchers are now offered huge wages to perform fundamental research, as companies build research teams on the assumption that commercially important breakthroughs will follow.  Andrew Ng, chief scientist at Baidu Research.  The advances seen in recent years have come thanks to the development of powerful “deep learning” systems (see “10 Breakthrough Technologies 2013: Deep Learning”). Starting a few years ago, researchers found that very large, or deep, neural networks could be trained, using labeled examples, to recognize all sorts of things with human-like accuracy. This has led to stunning advances in image and voice recognition and elsewhere.  Ng says these systems will only become more powerful. This might not only increase the accuracy of existing deep learning tools, but also allow the technique to be leveraged in new areas, such as parsing and generating language.  What’s more, Ng says, hardware advances will provide the fuel required to make emerging AI techniques feasible.  “There are multiple experiments I’d love to run if only we had a 10-x increase in performance,” Ng adds. For instance, he says, instead of having various different image-processing algorithms, greater computer power might make it possible to build a single algorithm capable of doing all sorts of image-related tasks.  The world’s leading AI experts convened in Barcelona this week for a prominent event called the Neural Information Processing Systems conference. The scale of the gathering, which has grown from several hundred people a few years ago to more than 6,000 this year, offers some sense of the huge interest there is in artificial intelligence.","research, coming, winter, scientist, progress, ai, advances, systems, researchers, learning, sorts, isnt, deep",
44,Just a light frost—or AI winter?,".Wp-Block-Co-Authors-Plus-Coauthors.Is-Layout-Flow, Class, Wp-Block-Co-Authors-Plus, Display Inline, .Wp-Block-Co-Authors-Plus-Avatar, Where Img, Height Auto Max-Width, Vertical-Align Bottom .Wp-Block-Co-Authors-Plus-Coauthors.Is-Layout-Flow .Wp-Block-Co-Authors-Plus-Avatar, Vertical-Align Middle .Wp-Block-Co-Authors-Plus-Avatar Is .Alignleft .Alignright, Display Table .Wp-Block-Co-Authors-Plus-Avatar.Aligncenter Display Table Margin-Inline Auto",https://mindmatters.ai/2019/12/just-a-light-frost-or-ai-winter/,"About a year ago, I wrote that mounting AI hype would likely give way to yet another AI winter. Now, according to the panelists at “the world’s leading academic AI conference” the temperature is already falling. Most recent advances in AI have come through a pair of related technologies: Deep Learning and Neural Networks. At the conference, researchers speculated that new techniques, perhaps more inspired by biology, will advance AI. Fan tries programming AI jazz, gets lots and lots of AI… Jazz is spontaneous, but spontaneous noise is not jazzandBoeing’s sidelined fuselage robots: What went wrong?","About a year ago, I wrote that mounting AI hype would likely give way to yet another AI winter. Now, according to the panelists at “the world’s leading academic AI conference” the temperature is already falling.  Most recent advances in AI have come through a pair of related technologies: Deep Learning and Neural Networks. The ideas beneath these, however, are more than 70 years old. Neural Network development, starting in 1944, predates the solid-state transistor (1947):  Warren McCulloch and Walter Pitts (1943) opened the subject by creating a computational model for neural networks…The first functional networks with many layers were published by Ivakhnenko and Lapa in 1965, as the Group Method of Data Handling. The basics of continuous backpropagation were derived in the context of control theory by Kelley in 1960 and by Bryson in 1961, using principles of dynamic programming. “Artificial Neural Networks” at Wikipedia  Sheer computing power, coupled with immense quantities of data (often gathered from the Internet), moved the ideas to the foreground. Tweaks and research, such as Reinforcement Learning, further improved the results. But, as I suggested last year, the hype was exceeding the actual promise.  At this year’s NeurIPS conference researchers admitted that the end is near:  “We’re kind of like the dog who caught the car,’ [Blaise] Aguera y Arcas [one of Google’s top researchers] said. Deep learning has rapidly knocked down some longstanding challenges in AI—but it doesn’t immediately seem well suited to many that remain.” Tom Simonite, “A Sobering Message About the Future at AI’s Biggest Party” at Wired  Later, Yoshua Bengio, director of Mila, an AI institute in Montreal and one of the “godfathers” of Deep Learning noted that  … the technique yields highly specialized results; a system trained to show superhuman performance at one videogame is incapable of playing any other. ‘We have machines that learn in a very narrow way,’ Bengio said. ‘They need much more data to learn a task than human examples of intelligence, and they still make stupid mistakes.’” Tom Simonite, “A Sobering Message About the Future at AI’s Biggest Party” at Wired  It wasn’t just researchers at a geeky conference who were forecasting gloom:  Discussion of the limitations of existing AI technology are growing too. Optimism from Google and others that self-driving taxi fleets could be deployed relatively quickly has been replaced by fuzzier and more restrained expectations. Facebook’s director of AI said recently that his company and others should not expect to keep making progress in AI just by making bigger deep learning systems with more computing power and data. ‘At some point we’re going to hit the wall,” he said. “In many ways we already have.” Tom Simonite, “A Sobering Message About the Future at AI’s Biggest Party” at Wired  The misguided belief that our human intelligence is the product of an undirected, accidental process has encouraged unfounded expectations that AI could somehow just continue to happen. With those expectations falling short, what choice remains?  At the conference, researchers speculated that new techniques, perhaps more inspired by biology, will advance AI. I remain doubtful.  It is too bad that, in the face of their own data, they fail to draw the obvious conclusion: Minds do not spring from accidents, no matter how much time is allowed. Only a mind can create a mind.  If we entirely dispense with that view, as the current failing expectations for AI show, we end up with far less, not far more.  If you enjoyed this piece, here are some more of Brendan Dixon’s recent reflections on overblown claims for AI, especially when they conflict with culture:  Pizza robots get the pink slip. The technology was sheer genius; the pizza lousy.  Fan tries programming AI jazz, gets lots and lots of AI… Jazz is spontaneous, but spontaneous noise is not jazz  and  Boeing’s sidelined fuselage robots: What went wrong? It’s not what we learn, it’s what we forget  And, more seriously: AI Winter Is Coming: Roughly every decade since the late 1960s has experienced a promising wave of AI that later crashed on real-world problems, leading to collapses in research funding.","neural, frostor, light, expectations, winter, tom, sobering, conference, data, researchers, learning, ai, deep",2019-12-17 18:06:01+00:00
45,The end of AI winter?,,https://web.archive.org/web/20091004004227/http://machineslikeus.com/the-end-of-AI-winter.html,"Ray Kurzweil agrees: ""Many observers still think that the AI winter was the end of the story and that nothing since come of the AI field. Fear of another winterConcerns are sometimes raised that a new AI winter could be triggered by any overly ambitious or unrealistic promise by prominent AI scientists. Far from the AI winter of the past decade, it is now a great time to be in AI."" Far from the AI winter of the past decade, it is now a great time to be in AI."" Ray Kurzweil in his book, The Singularity is Near , 2005: ""The AI Winter is long since over"", 2005: ""The AI Winter is long since over"" Heather Halvenstein in Computerworld , 2005: ""Researchers now are emerging from what has been called an 'AI winter'"", 2005: ""Researchers now are emerging from what has been called an 'AI winter'"" John Markoff in the New York Times, 2005: ""Now there is talk about an A.I.","Fri, 10/26/2007 - 15:14 - NLN  An AI Winter is a collapse in the perception of artificial intelligence research. The term was coined by analogy with the relentless spiral of a nuclear winter: a chain reaction of pessimism in the AI community, followed by pessimism in the press, followed by a severe cutback in funding, followed by the end of serious research. It first appeared in 1984 as the topic of a public debate at the annual meeting of AAAI (then called the ""American Association of Artificial Intelligence""). Two leading AI researchers, Roger Schank and Marvin Minsky, warned the business community that enthusiasm for AI had spiraled out of control and that disappointment would certainly follow. They were right. Just three years later, the billion-dollar AI industry began to collapse. An AI Winter is a collapse in the perception of artificial intelligence research. The term was coined by analogy with the relentless spiral of a nuclear winter: a chain reaction of pessimism in the AI community, followed by pessimism in the press, followed by a severe cutback in funding, followed by the end of serious research. It first appeared in 1984 as the topic of a public debate at the annual meeting of AAAI (then called the ""American Association of Artificial Intelligence""). Two leading AI researchers, Roger Schank and Marvin Minsky, warned the business community that enthusiasm for AI had spiraled out of control and that disappointment would certainly follow. They were right. Just three years later, the billion-dollar AI industry began to collapse.  The process of hype, disappointment and funding cuts are common in many advancing technologies (consider the dot-com bubble and the software crisis), but the problem has been particularly acute for AI. The pattern has occurred many times:  1966: the failure of machine translation,  1970: the abandonment of connectionism,  1971-75: DARPA's frustration with the Speech Understanding Research program at CMU,  1973: the end of AI research in England in response to the Lighthill Report,  1973-74: DARPA's cutbacks to academic AI research in general,  1987: the collapse of the LISP machine market,  1993: expert systems slowly reaching the bottom,  1990 or so: the quiet disappearance of the fifth-generation computer project's original goals and the generally bad reputation AI has had since.  The worst times for AI have been 1974-1980 and 1987 to the present. Sometimes one or the other of these periods (or some part of them) is referred to as the AI winter.  The historical episodes known as AI winters are collapses only in the perception of AI by government bureacrats and venture capitalists. Despite the rise and fall of AI's reputation, it has continued to develop new and successful technologies. AI researcher Rodney Brooks would complain in 2002 that ""there's this stupid myth out there that AI has failed, but AI is around you every second of the day."" Ray Kurzweil agrees: ""Many observers still think that the AI winter was the end of the story and that nothing since come of the AI field. Yet today many thousands of AI applications are deeply embedded in the infrastructure of every industry."" He adds unequivocally: ""the AI winter is long since over.""  Machine translation and the ALPAC report of 1966  During the Cold War, the US government was particularly interested in the automatic, instant translation of Russian documents and scientific reports. The government aggressively supported efforts at machine translation starting in 1954. At the outset, the researchers were optimistic. Noam Chomsky's new work in grammar was streamlining the translation process and there were ""many predictions of imminent 'breakthroughs'"".  However, researchers had underestimated the profound difficulty of disambiguation. In order to translate a sentence, a machine needed to have some idea what the sentence was about, otherwise it made ludicrous mistakes. A famous example was ""the spirit is willing but the flesh is weak."" Translated back and forth with Russian, it became ""the vodka is good but the meat is rotten."" Later researchers would call this the commonsense knowledge problem.  By 1964, National Research Council had become concerned about the lack progress and formed the Automatic Language Processing Advisory Committee (ALPAC) to look into the problem. They concluded, in a famous 1966 report, that machine translation was more expensive, less accurate and slower than human translation. After spending some 20 million dollars, the NRC ended all support. Careers were destroyed and research ended.  Machine translation is still an open research problem in the 21st century.  The abandonment of perceptrons in 1969  A perceptron is a form of neural network introduced in 1958 by Frank Rosenblatt, who had been a schoolmate of Marvin Minsky at the Bronx Technical High School. Like most AI researchers, he made optimistic claims about their power, predicting that ""perceptron may eventually be able to learn, make decisions, and translate languages."" An active research program into the paradigm was carried out throughout the 60s but came to a sudden halt with the publication of Minsky and Papert's 1969 book Perceptrons. They showed that there were severe limitations to what perceptrons could do and that Frank Rosenblatt's claims had been grossly exaggerated. Famously it was shown that the perceptron could not learn the XOR function. The effect of the book was devastating: virtually no research at all was done in connectionism for 10 years. This despite Stephen GrossBerg's papers in 1972 and 1973 describing networks capable of solving the XOR and other problems.  Eventually, the work of Hopfield and others would revive the field and thereafter it would become a vital and useful part of artificial intelligence. The specific problems brought up by Perceptrons were ultimately addressed using backpropagation and other modern machine learning techniques, developed by Paul Werbos in 1974 and championed by David Rumelhart in the early 80s.[13] Rosenblatt would not live to see this, as he died in a boating accident shortly after the book was published.  THE SETBACKS OF 1974  The S.U.R. debacle  DARPA was deeply disappointed with researchers working on the Speech Understanding Research program at CMU. DARPA had hoped, and felt it had been promised, to get a system that could respond to voice commands from a pilot. The SUR team had developed a system which could recognize spoken English, but only if the words were spoken in a particular order. DARPA felt it had been duped and cancelled a three million dollar a year grant.  Many years later, successful commercial speech recognition systems would use the technology developed by the CMU team (such as hidden Markov models) and the market for speech recognition systems would reach $4 billion by 2001.  The Lighthill report  Professor Sir James Lighthill was asked by the Parliament of the United Kingdom to evaluate the state of AI research in England. His report, now called the Lighthill report, criticized the utter failure of AI to achieve its ""grandiose objectives."" He concluded that nothing being done in AI couldn't be done in other sciences. The report led to the complete dismantling of AI research in that country.  Lighthill was shown to be fundamentally mistaken in a public debate broadcast by the BBC. The winners of the debate were a team composed of Donald Michie, Richard Gregory and John McCarthy.  DARPA's funding cuts  After the passage of Mansfield Amendment in 1969, DARPA had been under increasing pressure to fund ""mission-oriented direct research, rather than basic undirected research."" Researchers now had to show that their work would soon produce some useful military technology. The Lighthill report and DARPA's own study (the American Study Group) suggested that most AI research was unlikely to produce anything truly useful in the foreseeable future. As a result, AI research proposals were held to a very high standard. Pure undirected research of the kind that had gone on in the 60s would not be funded by DARPA.  By 1974, funding for AI projects was hard to find. AI researcher Hans Moravec believed that ""it was literally phrased at DARPA that 'some of these people were going to be taught a lesson [by] having their two-million-dollar-a-year contracts cut to almost nothing!'"" and he blamed the crisis on the unrealistic predictions of his colleagues: ""Many researchers were caught up in a web of increasing exaggeration. Their initial promises to DARPA had been much too optimistic. Of course, what they delivered stopped considerably short of that. But they felt they couldn't in their next proposal promise less than in the first one, so they promised more.""  THE SETBACKS OF THE LATE 80s AND EARLY 90s  The collapse of the LISP machine market in 1987  In the 1980s a form of AI program called an ""expert system"" was adopted by corporations around the world. The first commercial expert system was XCON, developed at CMU for Digital Equipment Corporation, and it was an enormous success: it was estimated to have saved the company 40 million dollars over just six years of operation. Corporations around the world began to develop and deploy expert systems and by 1985 they were spending over a billion dollars on AI, most of it to in-house AI departments. An industry grew up to support them, including software companies like Teknowledge and Intellicorp, and hardware companies like Symbolics and Lisp Machines Inc. who built specialized computers, called Lisp machines, that were optimized to process the programming language Lisp, the preferred language for AI.  In 1987, three years after Minsky and Schank's prediction, the market for specialized AI hardware collapsed. Desktop computers from Apple and IBM had been steadily gaining speed and power and in 1987 they became more powerful than the more expensive Lisp machines. There was no longer a good reason to buy them. An entire industry worth half a billion dollars was demolished overnight.  Commercially, many LISP machine companies failed, like Symbolics, LISP Machines Inc., Lucid Inc., etc. However, a number of customer companies (that is, companies using systems written in Lisp and developed on Lisp machine platforms) continued to maintain systems. In some cases, this maintenance involved the assumption of the resulting support work. The maturation of Common LISP saved many systems such as ICAD which found application in Knowledge-based engineering.  The fall of expert systems  Eventually the earliest successful expert systems, such as XCON, proved too expensive to maintain. They were difficult to update, they could not learn, they were ""brittle"" (i.e., they could made grotesque mistakes when given unusual inputs), and they fell prey to problems (such as the qualification problem) that had been identified years earlier. Expert systems proved useful, but only in a few special contexts.  The few remaining expert system shell companies were eventually forced to downsize and search for new markets and software paradigms, like case based reasoning or universal database access.  The fizzle of the fifth generation  In 1981, the Japanese Ministry of International Trade and Industry set aside $850 million dollars for the Fifth generation computer project. Their objectives were to write programs and build machines that could carry on conversations, translate languages, interpret pictures, and reason like human beings. By 1991, the impressive list of goals penned in 1981 had not been met. Indeed, some of them had not been met in 2001. As with other AI projects, expectations had run much higher than what was actually possible.  THE STATE OF AI TODAY  The winter that wouldn't end  A survey of recent reports suggests that AI's reputation is still less than pristine:  Alex Castro in The Economist, 2007: ""[Investors] were put off by the term 'voice recognition' which, like 'artificial intelligence', is associated with systems that have all too often failed to live up to their promises.""  Patty Tascarella in Pittsburgh Business Times, 2006: ""Some believe the word 'robotics' actually carries a stigma that hurts a company's chances at funding""  John Markoff in the New York Times, 2005: ""At its low point, some computer scientists and software engineers avoided the term artificial intelligence for fear of being viewed as wild-eyed dreamers.""  AI behind the scenes  Algorithms originally developed by AI researchers began to appear as parts of larger systems. AI had solved a lot of very difficult problems and their solutions proved to be useful throughout the technology industry, such as machine translation, data mining, industrial robotics, logistics, speech recognition, banking software, medical diagnosis and Google's search engine, to name a few.  The field of AI receives little or no credit for these successes. Now no longer considered a part of AI, each has been reduced to the status of just another item in the tool chest of computer science. Nick Rostrom explains ""A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore."" This is called the ""AI effect"" and is expressed most succinctly by Tesler's Theorem: ""AI is whatever hasn't been done yet.""  In fact, many researchers in AI today deliberately call their work by other names, such as informatics, knowledge-based systems or computational intelligence. In part, this may be because they considered their field to be fundamentally different from AI, but also the new names help to procure funding.  Fear of another winter  Concerns are sometimes raised that a new AI winter could be triggered by any overly ambitious or unrealistic promise by prominent AI scientists. For example, some researchers feared that the widely publicised promises in the early 1990s that Cog would show the intelligence of a human two-year-old might lead to an AI winter. In fact, the Cog project and the success of Deep Blue seems to have led to an increase of interest in strong AI in that decade from both government and industry.  Hope of another spring  There are also constant reports that another AI spring is imminent:  Jim Hendler and Devika Subramanian in AAAI Newsletter , 1999: ""Spring is here! Far from the AI winter of the past decade, it is now a great time to be in AI.""  , 1999: ""Spring is here! Far from the AI winter of the past decade, it is now a great time to be in AI."" Ray Kurzweil in his book, The Singularity is Near , 2005: ""The AI Winter is long since over""  , 2005: ""The AI Winter is long since over"" Heather Halvenstein in Computerworld , 2005: ""Researchers now are emerging from what has been called an 'AI winter'""  , 2005: ""Researchers now are emerging from what has been called an 'AI winter'"" John Markoff in the New York Times, 2005: ""Now there is talk about an A.I. spring among researchers""  AI Now  Technologies developed by AI researchers have achieved commercial success in a number of domains, for example Fuzzy Logic controllers have been developed for automatic gearboxes in automobiles (the 2006 Audi TT, VW Toureg [1] and VW Caravell feature the DSP transmission which utilizes Fuzzy logic, a number of Škoda variants (Škoda Fabia) also currently include a Fuzzy Logic based controller). Camera sensors widely utilize Fuzzy Logic to enable focus (ironically).  Heuristic Search and Data Analytics are both technologies that have developed from the Evolutionary Computing and Machine Learning subdivision of the AI research community. Again, these techniques have been applied to a wide range of real world problems with considerable commercial success.  In the case of Heuristic Search iLog has developed a large number of applications including deriving job shop schedules for many manufacturing installations. Many telecommunications companies also make use of this technology in the management of their workforces, for example BT has deployed heuristic search in a scheduling application that provides the work schedules of 20000 engineers.  Data Analytics technology utilizing algorithms for the automated formation of classifiers that were developed in the supervised machine learning community in the 1990's (for example, TDIDT, Support Vector Machines, Neural Nets, IBL) are now used pervasively by companies for marketing survey targeting and discovery of trends and features in data-sets.  Another way to judge the state of AI research is to look at the research programs that are currently being funded by the major funding agencies in the developed world.  Two bodies are currently supporting research in AI. DARPA in the USA supports a Grand Challenge Program which has developed fully automated road vehicles that can successfully navigate real world terrain in a fully autonomous fashion.  DARPA has also supported programs on the Semantic Web with a great deal of emphasis on intelligent management of content and automated understanding. However James Hendler who was the manager of the DARPA program at the time has expressed some disappointment with the outcome of the programe.  As of 2007 DARPA is soliciting AI research proposals under a number of programs including ""Cognitive Technology Threat Warning System (CT2WS),"" ""SN07-43 Human Assisted Neural Devices,"" ""AUTONOMOUS REAL-TIME GROUND UBIQUITOUS SURVEILLANCE- IMAGING SYSTEM (ARGUS-IS),"" and ""URBAN REASONING AND GEOSPATIAL EXPLOITATION TECHNOLOGY (URGENT)""  The EU-FP7 programme is a civilian funding program that is used to provide support to researchers in the European Union. Currently it funds AI research under the Cognitive Systems, Interaction and Robotics Programme (€193m), the Digital Libraries and Content Programme (€203m) and the FET programme (€185m)[39]","research, lisp, winter, end, called, developed, systems, intelligence, machine, researchers, ai",
46,"CAPTCHA is Dead, But the AI Winter Lives On","Erik Sofge, Posted On Oct, Pm Edt",http://www.popsci.com/blog-network/zero-moment/captcha-dead-ai-winter-lives,"The precursor to IBM’s Watson was referred to as an “electronic brain,” decades before the AI winter rolled in. If Vicarious proved anything, it’s that the AI winter is real, and still raging, stronger than ever. Wikipedia’s AI Winter entry summarizes the subsequent comedown extremely well, identifying two distinct periods—1974 to 1980 and 1987 to 1993—as well as numerous isolated events, during which investment in and federal backing of AI research fizzled. That the AI winter will stretch on, with no relief in sight, until some discrete system comes along that thinks like a full-fledged person? That is what artificial intelligence, or something close to it, looks and sounds like.","The 701 Electronic Data Processing Machine entered service in 1954, performing cost-benefit analysis. The precursor to IBM’s Watson was referred to as an “electronic brain,” decades before the AI winter rolled in. IBM  If you developed an artificial intelligence that could see as a human does, how would you announce its presence to the world?  Would you wait until it could recognize objects in a home, identifying a chair as a piece of movable furniture, to be potentially nudged back under the dining room table, and not simply another laser-mapped obstacle to be veered around? Or demonstrate its powers of perception while studying X-ray films or MRI results, sifting through complex visual data to arrive at possible diagnoses?  Or would you set that machine intelligence loose on a spam-filtering protocol, and proudly proclaim that one of the most widespread online security features has been roundly defeated?  If you’re Vicarious, a San Francisco-based startup hoping to eventually monetize its artificial intelligence (AI) work, you go for the latter—the company announced yesterday that its system could beat CAPTCHA more than 90 percent of the time, recognizing letters and numbers that are visually contorted in the hopes of preventing the automated creation of bogus user accounts, and the carpet-bombing of message boards and online comment sections with generous offers of penis enlargement.  And yet, why? Does Vicarious’ unnamed AI feel an intraspecies pity for the poor spambots battering themselves against CAPTCHA’s walls? Or does Vicarious simply think that the best way to get the internet’s attention is by playing the rogue, showing how its AI can swan right past a protocol that other systems have to shoulder their way through? CAPTCHA (short for, brace yourself, Completely Automated Public Turing Test to Tell Computers and Humans Apart) is already defeated by degrees on a regular basis, and updated to stay ahead of rampant spammer innovation. Check out Vicarious, though, striding right up to the register with its illicit beer, flashing the best fake ID in town.  Here’s how Vicarious co-founder D. Scott Phoenix explained the CAPTCHA stunt, in yesterday’s press release:  Instead of supercomputing through thousands of stored examples of a given letter in order to pick it out from an intentionally cluttered and distorted sequence, Vicarious’ system simply needs a little tutoring. As Phoenix told Popular Science, 10 examples of each letter is all the algorithms require.  In the wake of this stunt—and it’s clearly a stunt, designed to attract that plugged-in portion of the populace that knows or cares about CAPTCHA—Vicarious will slip back into obscurity. Thankfully, the company didn’t release its code-breaking algorithms. But, less fortunately, it stipulated in a media-oriented FAQ that it will be a few years before any additional news is released.  If Vicarious proved anything, it’s that the AI winter is real, and still raging, stronger than ever.  * * *  In some circles, “AI” and “winter” are fighting words. The term dates back to the 80’s, but the situation it describes is older: the freeze in funding and interest that followed the limitless promises of AI researchers in the late ‘60s. Those were heady days, when computer scientists believed that full, human-like intelligence would be recreated at any moment. Computers were beginning to show their almost inconceivable powers of data processing, and what was the brain, and its passenger, the intellect, but a biological engine for churning and sorting data?  Wikipedia’s AI Winter entry summarizes the subsequent comedown extremely well, identifying two distinct periods—1974 to 1980 and 1987 to 1993—as well as numerous isolated events, during which investment in and federal backing of AI research fizzled. In a general sense, though, some believe that this literal winter of discontent is a perpetual state. The Turing test, proposed in 1950 as a measure of a program’s ability to mimic human thought, by tricking a judge into believing he or she is interacting with a person, has yet to be “passed.” While AI is supposedly everywhere, in self-driving robotic toys and stock-picking algorithms and cover-diving video game characters, the term has been endlessly reinterpreted and defanged, pressed into service as a buzz word for nearly any automated process.  Those with skin in the game, researchers-turned-entrepreneurs like author and speaker Ray Kurzweil and Rethink Robotics founder Rodney Brooks, have long held that the winter is over. Since the basic tenets and functionality of AI are ubiquitous, with automation sunk almost invisibly into the search engines and automotive safety systems and other technologies that define life in the developed world, any talk of a stubborn winter, they argue, is a failure of imagination.  What’s more distressing, though? That the AI winter will stretch on, with no relief in sight, until some discrete system comes along that thinks like a full-fledged person? Or that the winter is over, because AI is all around us, and it’s far more boring than its original architects could have ever imagined?  The announcement from Vicarious is a textbook example of managed expectations in AI research. It can do one thing—but only one—about as well as a human being. So its breakthrough has to be painstakingly framed, within the context of a field that’s more hobbled than a lay reader might assume. Sorting through a string of mildly obscured letters, a task worthy of a grade schooler, is a big deal . . . because AI has problems doing simple things. Not surprisingly, the bright, limitless future of Vicarious’ system is deferred.  But before we watch Vicarious ride off into the cruel sunset of the AI winter, consider the shot that the company took at the most exciting artificial intelligence project in years, and the closest to a thinking machine in any current lab. Again, from the company’s press materials:  Forget AI Winter—those, right there, are fighting words. Though Watson was also unveiled with an unabashed PR stunt—its quiz show evisceration of two human Jeopardy champions in 2011—the system has since graduated to deployments with world-tilting implications. Less interesting is IBM pushing Watson as a call-center rep, using its natural language processing chops and rapid access to a teraflop of internal data and millions more online files and records to interact with customers. But Watson is also playing Sherlock, diving into medical records, imaging results, millions of pages of journals, and other data to provide possible diagnoses and treatment options to doctors. The system is being deployed in this capacity at multiple medical centers, including the MD Andersen Cancer Center at the University of Texas, as part of its “moon shot” program to cure leukemia outright. Watson will serve as a diagnostic assistant, grinding through scattered research, patient, laboratory and other databases to find meaningful connections, and even draw conclusions regarding treatment and candidates for clinical trials.  That is what artificial intelligence, or something close to it, looks and sounds like. Though a single high-performing system—one that’s in the wild, not simply teased via press release, or chatting up doctoral candidates in a lab—isn’t sufficient proof that the AI winter has receded. But Watson could be the nervy little groundhog we’ve been waiting for.","simply, winter, artificial, watson, lives, human, system, vicarious, intelligence, data, captcha, dead, ai",2013-10-30 18:24:09
47,"Wolfram Alpha, Semantic Web, and back to the pre AI winter future",,http://scottlocklin.wordpress.com/2009/05/20/wolfram-alpha-semantic-web-and-back-to-the-pre-ai-winter-future/,"Basically, Wolfram took the Mathematica engine, and added a cheap natural language interface to it. The natural language interface? Well, it is an extremely cheap natural language interface. There are of course niche applications of semantic web enabling technologies; it could be very useful for internal databases. Most of the “AI” technologies before 1982 were forgotten and abandoned.","The latest nerd buzz has been about Steven Wolfram’s entry into the search engine business. I’ll draw the conclusion before giving you the meat in case you want the executive summary: it kind of sucks.  This isn’t a big surprise. Most things suck. As my friend Philip says, simple solutions are often better. I remember when we worked together, he was fond of pointing this out in more specific cases -for example, “you have to get up pretty early in the morning to beat linear regression!”  So, what is it? Basically, Wolfram took the Mathematica engine, and added a cheap natural language interface to it. Online Mathematica is pretty helpful, as it’s still one of the most powerful computer algebra systems in the world. The natural language interface? Well, it is an extremely cheap natural language interface. Not even up to the very mediocre standards of Ask Jeeves search engine, which was the first popular natural language engine hooked up to the web. As far as I can tell from fiddling with it, Wolfram added the mathematical equivalent of ^X doctor in emacs. This is a type of code with a long and hoary history of niftiness, but fundamental uselessness. It’s also a type of code with an old history of being used in Mathematica type systems: for example, the last commercial version of Macsyma (I think released in 1998 or so) had a very advanced natural language interface. These sorts of things are easy to write in functional programming. They’re sort of what Lisp and ML type languages were invented for in the first place. If you want some classic examples of how this works, you can look at the source code for ^X doctor in emacs (in /usr/share/emacs/lisp/play/ ) or go look at Peter Norvig’s book, Paradigms in Artificial Intelligence. The relevant programs are in this handy link. Have a look at Eliza and Mycin. They’re both what used to be called “expert system shells.” What they really are, are interpreters where it is easy to update the rules, or write rules for itself.  Expert system shells were hot shit in the pre-AI winter days. If you read the “Journal of Advances in Computers” (my lodestone for this sort of historical context: I read the whole damn thing in the LBNL library while avoiding writing my dissertation), you can see all the excitement from the time when people first write such things, dating from around 1957 when they invented the first one, the “General Problem Solver.” Most of early AI research up until the AI winter (early 1980s) consisted in riffs on this basic theme. One of the great inventions which came out of this sort of thing was, in fact, the computer algebra system (CAS), of which Wolfram is the foremost vendor at present. While CAS do a lot more than the primitive expert system shells, it is effectively a subset of the old “General problem solver.”  So, it’s no surprise that Wolfram was able to cobble together a natural language system that understands very simple mathematical commands in a sort of pidgin English. I guess the real “breakthrough” of Wolfram Alpha is that it uses data found on the web. What would be really impressive if he had something like an augmented transition network (ATN to AI nerds) to parse data he found online and place it in context. Briefly, an ATN was a pre-AI winter technique used to parse grammars which work like the English language. The place you’re most likely to have heard of it is in Gödel, Escher, Bach by Hofstadter, wherein he makes the now hilarious claim that ATN’s will eventually become powerful enough to form a sort of Sentient AI. This is hilarious because ATNs are useless on languages which are unlike English in sentence structure. So, if you could build a Sentient Computer Program (an idea which itself seems hopelessly funny now) using only ATNs as Hofstadter thought we might one day, it would imply that people who speak a declinated language like Latin or Russian are not sentient. Putting aside the ethnic jokes this makes possible, there are all kinds of other parsing problems which humans easily solve which ATN’s haven’t got the remotest chance with. One example is parsing HTML. I mean, we don’t parse HTML directly unless we’re HTML nerds, but our browsers easily turn it into stuff we can read and make sense of. ATNs can’t help us do this, as the language structure of HTML doesn’t map to ATNs any better than Arabic does. I’m guessing, since Wolfram is a smart guy, he must have something like an ATN for some kinds of data based HTML. If he can get it to work properly, this would be an important breakthrough. It obviously doesn’t work right yet, and if he does have something like an ATN to help parse information found in HTML, it probably requires lots of human intervention.  “Semantic Web” is the sort of “next big thing” for solving this problem from the other end. The idea of “semantic web” is to solve the problem by phrasing web data in ways which computers (rather than people armed with browsers) can understand more easily. I have always been confused by the idea of “semantic web.” The problem with getting everyone in the world to adopt your standard is one of motivating them. HTML is a world wide standard because it solves lots of problems. Semantic web only solves the problems of search engine engineers; it doesn’t solve any content creator problems, so i can’t see why any of them would take the trouble to use it. The only types of content creators who would want to use something like this are basically advertisers, who are pretty much useless to search engines. In fact, advertisers steal money from search engines if they appear in an unsponsored search! I mean, that’s how Google makes money! There are of course niche applications of semantic web enabling technologies; it could be very useful for internal databases. But I suspect simple html tags and ordinary search engines will work just as well for internal databases. So, to solve the problem of how to make computers able to think about information on the web, you need the right kind of parse engine for natural language HTML processing.  Does Wolfram have this “HTML parsing special sauce?” Evidently not yet. There are forms where you can submit data to the thing wikipedia style -this is probably how most data gets loaded. Maybe he never will grow special HTML parsing sauce. The problem is actually much harder than teaching computers to read and understand books in natural languages, which they are still largely incapable of. Context is hard. Still, it’s a valiant effort, and a pleasant throw back to a set of largely forgotten technologies. Why were these technologies forgotten in the first place? Mostly: K&R invented C, and Intel invented useful commodity microprocessors. There are tons of useful things you can do with C and a commodity microprocessor. These old AI techniques are not among them. They required much higher level computer languages, and the hardware to support such things. This form of AI also made a sort of unfortunate detour into technologies like Prolog, which made it really easy to ask computers for solutions to NP hard problems, without realizing that you’re asking the computer something impossible for it to solve. Finally, there was a serious AI software bubble which popped in the 80s. There were many AI startups which promised big business the world. They failed in that economic apocalypse because they were largely unable to deliver on their promises. PC style machines and the C programming language made real improvements in business productivity that all the Lisp-AI propellor heads were unable to match with the tools they were using at the time. As such, much “AI” research since the 1980s has looked a lot like signal processing and statistics; fields which map much better into procedural C and limited memory microprocessor machines. Most of the “AI” technologies before 1982 were forgotten and abandoned.  I used to think I could code up an expert system shell for something useful at work. I like forgotten technologies, and I like Lisp. The last time I had this thought, I was plagued by support questions by people I worked with, and considered writing an expert system shell to answer their questions. Why didn’t I follow through with is? Well, it’s back to Philip’s saying. The simple solution is generally hard to beat with a technologically advanced one. I put together a searchable wiki for support questions instead. Sure, it would have saved me seconds a day if I had all that content loaded into an expert system shell, but it probably would have taken me months to build the tailored expert system shell and make it work. And it might not have worked, whereas the wiki worked and was useful immediately. So, you have to give Wolfram some credit for reviving some neat technologies. Minus points for not hiring Philip as a consultant beforehand.  Fun Wolfram Alpha Easter Eggs which show its “Eliza” intestines:  http://mashable.com/2009/05/17/wolfram-easter-eggs/  http://mashable.com/2009/05/17/better-wolfram-easter-eggs/  Fun observations (to be updated as I make more of them):","search, winter, sort, future, pre, ai, alpha, system, technologies, semantic, html, language, natural, wolfram, web",2009-05-20 00:00:00
48,History of the AI Winter,Startup Kit,https://slimsaas.com/blog/the-ai-winter/,"History of the AI WinterBy Alex on 8/28/2024The Birth of AI and Early Optimism (1950s-1960s)In 1950, Alan Turing published his seminal paper “Computing Machinery and Intelligence,” introducing the Turing Test and laying the groundwork for AI research. The perceptron controversy contributed to the shift in AI research away from neural networks and towards symbolic AI approaches. Brief Resurgence (1980s)The 1980s saw a renewed interest in AI, particularly in the form of expert systems and national AI initiatives. Second AI Winter (Late 1980s-Early 1990s)However, this resurgence was short-lived as the limitations of expert systems and other AI technologies became evident. Quiet Progress (1990s-2000s)Despite reduced hype and funding, AI research continued during this period, with a focus on more practical applications and new approaches.","History of the AI Winter  By Alex on 8/28/2024  The Birth of AI and Early Optimism (1950s-1960s)  In 1950, Alan Turing published his seminal paper “Computing Machinery and Intelligence,” introducing the Turing Test and laying the groundwork for AI research.  The Dartmouth Conference in 1956, organized by John McCarthy, Marvin Minsky, Nathaniel Rochester, and Claude Shannon, is widely considered the official birth of AI as a field of study.  The Perceptron  In 1957, Frank Rosenblatt introduced the perceptron, one of the first artificial neural networks, which played a crucial role in early AI research:  The perceptron was designed to model the human neuron and could learn to recognize simple patterns.  Rosenblatt’s 1962 book “Principles of Neurodynamics” outlined the potential of perceptrons for pattern recognition and learning.  The U.S. government, through the Office of Naval Research, funded significant research into perceptrons.  Media hype surrounded the perceptron, with the New York Times reporting in 1958 that it would be “the embryo of an electronic computer that [the Navy] expects will be able to walk, talk, see, write, reproduce itself and be conscious of its existence.”  Reality Check and the First AI Winter (Late 1960s-1970s)  The Perceptron Controversy  In 1969, Marvin Minsky and Seymour Papert published “Perceptrons,” a mathematical analysis of the perceptron’s limitations:  The book demonstrated that single-layer perceptrons could not learn certain simple functions, such as the XOR function.  This revelation led to a significant decrease in funding and interest in neural network research.  The perceptron controversy contributed to the shift in AI research away from neural networks and towards symbolic AI approaches.  Other Factors Contributing to the Winter  The complexity of natural language understanding proved far greater than initially estimated.  Hardware limitations constrained the scope of AI applications.  The ALPAC report (1966) criticized progress in machine translation, leading to funding cuts.  The Lighthill ReUK’s port (1973) in the UK is critical of AI’s progress, leading to further cuts.  DARPA reduced its funding for undirected, exploratory research in AI.  During this period, many AI researchers shifted focus to more specific, limited-domain problems.  Brief Resurgence (1980s)  The 1980s saw a renewed interest in AI, particularly in the form of expert systems and national AI initiatives.  Japan launched the ambitious Fifth Generation Computer Project, aiming to create intelligent computers and sparking international competition.  Companies invested heavily in expert systems for various industries, leading to commercial applications of AI.  The AI industry grew to billions of dollars, with specialized AI hardware and software being developed.  Second AI Winter (Late 1980s-Early 1990s)  However, this resurgence was short-lived as the limitations of expert systems and other AI technologies became evident.  Expert systems proved costly to maintain and difficult to scale to more complex problems.  The Fifth Generation Project failed to meet its ambitious goals.  The collapse of the Lisp machine market contributed to disillusionment in the field.  Funding for AI research once again declined sharply.  Quiet Progress (1990s-2000s)  Despite reduced hype and funding, AI research continued during this period, with a focus on more practical applications and new approaches.  Machine learning techniques, particularly neural networks, saw renewed interest.  Probabilistic reasoning and statistical methods gained prominence in AI research.  AI found success in specific domains, such as data mining and logistics.  AI Renaissance (2010s-Present)  The field of AI has experienced a dramatic resurgence in recent years, driven by advances in computing power, the availability of big data, and breakthroughs in machine learning algorithms.  Significant progress in deep learning and neural networks has led to practical applications in areas like computer vision and natural language processing.  Innovations in GPU technology have enabled faster and more efficient training of deep learning models.  This inflection point in technology is an exciting time. If you’ve been thinking about it starting a startup, before you burn out writing all the basic authentication, stripe integrations, email integrations, etc, check out the slimsaas kit first. We want to provide the foundation so that you can focus on the fun stuff.","neural, research, winter, history, progress, funding, systems, perceptrons, learning, perceptron, ai, networks",2024-08-28 00:00:00+00:00
49,The Risk of a New AI Winter,Clive Thompson,https://clivethompson.medium.com/the-risk-of-a-new-ai-winter-332ffb4767f0,"The Risk of a New AI WinterHistory has not been kind to AI pioneers who overpromise and underdeliver Clive Thompson · Follow 8 min read · Feb 22, 2023 -- 35 SharePhoto by Rodion Kutsaiev on UnsplashIs winter coming? I speak of an “AI winter”, of course. During an AI boom, computer scientists and firms invent new techniques that seem exciting and powerful. Then an “AI winter” begins. Customers stop paying top dollar for AI products; investors close their purses.","The Risk of a New AI Winter  History has not been kind to AI pioneers who overpromise and underdeliver Clive Thompson · Follow 8 min read · Feb 22, 2023 -- 35 Share  Photo by Rodion Kutsaiev on Unsplash  Is winter coming?  I speak of an “AI winter”, of course.  They’re part of the boom-bust cycle that we’ve seen in AI history. During an AI boom, computer scientists and firms invent new techniques that seem exciting and powerful. Tech firms use them to build products that promise to make everyone’s lives easier (or more productive) and investors unleash a geyser of funding. Everyone — including starry-eyed journalists — begins overpromising, gushing about the artificial smarts that will be invented. Humanlike! Godlike! Omniscient!  That level of hype can’t be maintained, though — and at some point the industry starts underdelivering. The AI turns out to be surprisingly fail-ridden. Companies and people that try using it to solve everyday problems discover it’s prone to errors, often quite mundane ones.  Then an “AI winter” begins. Customers stop paying top dollar for AI products; investors close their purses. Journalists begin more critically appraising the landscape. And because everyone feels burned (or embarrassed), things slide into an overly negative cycle: Even the computer scientists and inventors with legitimately interesting new paths for AI can’t easily get funding to…","cycle, winter, scientists, products, investors, risk, funding, computer, firms, journalists, ai",2023-02-22 17:20:07.628000+00:00
50,History of the first AI Winter,Sebastian Schuchmann,https://medium.com/@schuchmannsebastian/history-of-the-first-ai-winter-6f8c2186f80b,"AI has a long history. This chapter only covers events relevant to the periods of AI winters without being too exhaustive in hope to extract knowledge that can be applied today. Events leading to the first AI WinterTo aid understanding the phenomenon of AI Winters, the events leading up to them are examined. Beginnings of the AI Field in the 1950sMany early ideas about thinking machines appeared in the late 1940s to ’50s by people like Turing or Von Neumann. Turing tried to frame the questions of “Can machines think?” differently and created the imitation game, now famously called the Turing Test.","AI has a long history. One can argue it even started long before the term was first coined; mostly in stories and later in actual mechanical devices called automata. This chapter only covers events relevant to the periods of AI winters without being too exhaustive in hope to extract knowledge that can be applied today.  Events leading to the first AI Winter  To aid understanding the phenomenon of AI Winters, the events leading up to them are examined.  Beginnings of the AI Field in the 1950s  Many early ideas about thinking machines appeared in the late 1940s to ’50s by people like Turing or Von Neumann. Turing tried to frame the questions of “Can machines think?” differently and created the imitation game, now famously called the Turing Test.  In 1955, Arthur Samuel wrote a program that could play checkers very well. A year later, it even appeared on television. It used a combination of a tree search with heuristics and learned weights. Samuel handcrafted the heuristics inspired by a book from checkers experts. He used a learning algorithm he called “temporal-difference learning” where the weights are adjusted using the “error” between the score initially calculated and the score after the search was completed.","samuel, winter, winters, turing, history, called, score, search, weights, machines, used, ai",2020-05-16 21:25:59.465000+00:00
51,AI winter,,https://en.wikipedia.org/wiki/AI_winter,"Period of reduced funding and interest in AI researchIn the history of artificial intelligence, an AI winter is a period of reduced funding and interest in artificial intelligence research. Roger Schank and Marvin Minsky—two leading AI researchers who experienced the ""winter"" of the 1970s—warned the business community that enthusiasm for AI had spiraled out of control in the 1980s and that disappointment would certainly follow. [6]Just like all AI booms that have been followed by desperate AI winters, the media tended to exaggerate the significance of these developments. ""[25] The result, Moravec claims, is that some of the staff at DARPA had lost patience with AI research. In 2006, Nick Bostrom explained that ""a lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.","Period of reduced funding and interest in AI research  In the history of artificial intelligence, an AI winter is a period of reduced funding and interest in artificial intelligence research.[1] The field has experienced several hype cycles, followed by disappointment and criticism, followed by funding cuts, followed by renewed interest years or even decades later.  The term first appeared in 1984 as the topic of a public debate at the annual meeting of AAAI (then called the ""American Association of Artificial Intelligence""). Roger Schank and Marvin Minsky—two leading AI researchers who experienced the ""winter"" of the 1970s—warned the business community that enthusiasm for AI had spiraled out of control in the 1980s and that disappointment would certainly follow. They described a chain reaction, similar to a ""nuclear winter"", that would begin with pessimism in the AI community, followed by pessimism in the press, followed by a severe cutback in funding, followed by the end of serious research. Three years later the billion-dollar AI industry began to collapse.  There were two major winters approximately 1974–1980 and 1987–2000,[3] and several smaller episodes, including the following:  Enthusiasm and optimism about AI has generally increased since its low point in the early 1990s. Beginning about 2012, interest in artificial intelligence (and especially the sub-field of machine learning) from the research and corporate communities led to a dramatic increase in funding and investment, leading to the current (as of 2024 ) AI boom.  Early episodes [ edit ]  Machine translation and the ALPAC report of 1966 [ edit ]  Natural language processing (NLP) research has its roots in the early 1930s and began its existence with the work on machine translation (MT).[4] However, significant advancements and applications began to emerge after the publication of Warren Weaver's influential memorandum in 1949.[5] The memorandum generated great excitement within the research community. In the following years, notable events unfolded: IBM embarked on the development of the first machine, MIT appointed its first full-time professor in machine translation, and several conferences dedicated to MT took place. The culmination came with the public demonstration of the IBM-Georgetown machine, which garnered widespread attention in respected newspapers in 1954.[6]  Just like all AI booms that have been followed by desperate AI winters, the media tended to exaggerate the significance of these developments. Headlines about the IBM-Georgetown experiment proclaimed phrases like ""The bilingual machine,"" ""Robot brain translates Russian into King's English,""[7] and ""Polyglot brainchild.""[8] However, the actual demonstration involved the translation of a curated set of only 49 Russian sentences into English, with the machine's vocabulary limited to just 250 words.[6] To put things into perspective, a 2006 study made by Paul Nation found that humans need a vocabulary of around 8,000 to 9,000-word families to comprehend written texts with 98% accuracy.[9]  During the Cold War, the US government was particularly interested in the automatic, instant translation of Russian documents and scientific reports. The government aggressively supported efforts at machine translation starting in 1954. Another factor that propelled the field of mechanical translation was the interest shown by the Central Intelligence Agency (CIA). During that period, the CIA firmly believed in the importance of developing machine translation capabilities and supported such initiatives. They also recognized that this program had implications that extended beyond the interests of the CIA and the intelligence community.[6]  At the outset, the researchers were optimistic. Noam Chomsky's new work in grammar was streamlining the translation process and there were ""many predictions of imminent 'breakthroughs'"".[10]  Briefing for US Vice President Gerald Ford in 1973 on the junction-grammar-based computer translation model  However, researchers had underestimated the profound difficulty of word-sense disambiguation. In order to translate a sentence, a machine needed to have some idea what the sentence was about, otherwise it made mistakes. An apocryphal[11] example is ""the spirit is willing but the flesh is weak."" Translated back and forth with Russian, it became ""the vodka is good but the meat is rotten."" Later researchers would call this the commonsense knowledge problem.  By 1964, the National Research Council had become concerned about the lack of progress and formed the Automatic Language Processing Advisory Committee (ALPAC) to look into the problem. They concluded, in a famous 1966 report, that machine translation was more expensive, less accurate and slower than human translation. After spending some 20 million dollars, the NRC ended all support. Careers were destroyed and research ended.[10]  Machine translation shared the same path with NLP from the rule-based approaches through the statistical approaches up to the neural network approaches, which have in 2023 culminated in large language models.  The failure of single-layer neural networks in 1969 [ edit ]  Simple networks or circuits of connected units, including Walter Pitts and Warren McCulloch's neural network for logic and Marvin Minsky's SNARC system, have failed to deliver the promised results and were abandoned in the late 1950s. Following the success of programs such as the Logic Theorist and the General Problem Solver,[13] algorithms for manipulating symbols seemed more promising at the time as means to achieve logical reasoning viewed at the time as the essence of intelligence, either natural or artificial.  Interest in perceptrons, invented by Frank Rosenblatt, was kept alive only by the sheer force of his personality.[14] He optimistically predicted that the perceptron ""may eventually be able to learn, make decisions, and translate languages"".[15] Mainstream research into perceptrons ended partially because the 1969 book Perceptrons by Marvin Minsky and Seymour Papert emphasized the limits of what perceptrons could do. While it was already known that multilayered perceptrons are not subject to the criticism, nobody in the 1960s knew how to train a multilayered perceptron. Backpropagation was still years away.[17]  Major funding for projects neural network approaches was difficult to find in the 1970s and early 1980s.[18] Important theoretical work continued despite the lack of funding. The ""winter"" of neural network approach came to an end in the middle 1980s, when the work of John Hopfield, David Rumelhart and others revived large scale interest.[19] Rosenblatt did not live to see this, however, as he died in a boating accident shortly after Perceptrons was published.[15]  The setbacks of 1974 [ edit ]  The Lighthill report [ edit ]  In 1973, professor Sir James Lighthill was asked by the UK Parliament to evaluate the state of AI research in the United Kingdom. His report, now called the Lighthill report, criticized the utter failure of AI to achieve its ""grandiose objectives"". He concluded that nothing being done in AI could not be done in other sciences. He specifically mentioned the problem of ""combinatorial explosion"" or ""intractability"", which implied that many of AI's most successful algorithms would grind to a halt on real world problems and were only suitable for solving ""toy"" versions.[20]  The report was contested in a debate broadcast in the BBC ""Controversy"" series in 1973. The debate ""The general purpose robot is a mirage"" from the Royal Institution was Lighthill versus the team of Donald Michie, John McCarthy and Richard Gregory.[21] McCarthy later wrote that ""the combinatorial explosion problem has been recognized in AI from the beginning"".[22]  The report led to the complete dismantling of AI research in the UK.[20] AI research continued in only a few universities (Edinburgh, Essex and Sussex). Research would not revive on a large scale until 1983, when Alvey (a research project of the British Government) began to fund AI again from a war chest of £350 million in response to the Japanese Fifth Generation Project (see below). Alvey had a number of UK-only requirements which did not sit well internationally, especially with US partners, and lost Phase 2 funding.  DARPA's early 1970s funding cuts [ edit ]  During the 1960s, the Defense Advanced Research Projects Agency (then known as ""ARPA"", now known as ""DARPA"") provided millions of dollars for AI research with few strings attached. J. C. R. Licklider, the founding director of DARPA's computing division, believed in ""funding people, not projects""[23] and he and several successors allowed AI's leaders (such as Marvin Minsky, John McCarthy, Herbert A. Simon or Allen Newell) to spend it almost any way they liked.  This attitude changed after the passage of Mansfield Amendment in 1969, which required DARPA to fund ""mission-oriented direct research, rather than basic undirected research"".[24] Pure undirected research of the kind that had gone on in the 1960s would no longer be funded by DARPA. Researchers now had to show that their work would soon produce some useful military technology. AI research proposals were held to a very high standard. The situation was not helped when the Lighthill report and DARPA's own study (the American Study Group) suggested that most AI research was unlikely to produce anything truly useful in the foreseeable future. DARPA's money was directed at specific projects with identifiable goals, such as autonomous tanks and battle management systems. By 1974, funding for AI projects was hard to find.[24]  AI researcher Hans Moravec blamed the crisis on the unrealistic predictions of his colleagues: ""Many researchers were caught up in a web of increasing exaggeration. Their initial promises to DARPA had been much too optimistic. Of course, what they delivered stopped considerably short of that. But they felt they couldn't in their next proposal promise less than in the first one, so they promised more.""[25] The result, Moravec claims, is that some of the staff at DARPA had lost patience with AI research. ""It was literally phrased at DARPA that 'some of these people were going to be taught a lesson [by] having their two-million-dollar-a-year contracts cut to almost nothing!'"" Moravec told Daniel Crevier.[26]  While the autonomous tank project was a failure, the battle management system (the Dynamic Analysis and Replanning Tool) proved to be enormously successful, saving billions in the first Gulf War, repaying all of DARPAs investment in AI[27] and justifying DARPA's pragmatic policy.[28]  The SUR debacle [ edit ]  As described in:[29]  In 1971, the Defense Advanced Research Projects Agency (DARPA) began an ambitious five-year experiment in speech understanding. The goals of the project were to provide recognition of utterances from a limited vocabulary in near-real time. Three organizations finally demonstrated systems at the conclusion of the project in 1976. These were Carnegie-Mellon University (CMU), who actually demonstrated two system [HEARSAY-II and HARPY]; Bolt, Beranek and Newman (BBN); and System Development Corporation with Stanford Research Institute (SDC/SRI)  The system that came closest to satisfying the original project goals was the CMU HARPY system. The relatively high performance of the HARPY system was largely achieved through 'hard-wiring' information about possible utterances into the system's knowledge base. Although HARPY made some interesting contributions, its dependence on extensive pre-knowledge limited the applicability of the approach to other signal-understanding tasks.  DARPA was deeply disappointed with researchers working on the Speech Understanding Research program at Carnegie Mellon University. DARPA had hoped for, and felt it had been promised, a system that could respond to voice commands from a pilot. The SUR team had developed a system which could recognize spoken English, but only if the words were spoken in a particular order. DARPA felt it had been duped and, in 1974, they cancelled a three million dollar a year contract.[30]  Many years later, several successful commercial speech recognition systems would use the technology developed by the Carnegie Mellon team (such as hidden Markov models) and the market for speech recognition systems would reach $4 billion by 2001.[31]  For a description of Hearsay-II see Hearsay-II, The Hearsay-II Speech Understanding System: Integrating Knowledge to Resolve Uncertainty and A Retrospective View of the Hearsay-II Architecture which appear in Blackboard Systems[32]  Reddy gives a review of progress in speech understanding at the end of the DARPA project in a 1976 article in Proceedings of the IEEE.[33]  Contrary view [ edit ]  Thomas Haigh argues that activity in the domain of AI did not slow down, even as funding from DoD was being redirected, mostly in the wake of congressional legislation meant to separate military and academic activities.[34] That indeed professional interest was growing throughout the 70s. Using the membership count of ACM's SIGART, the Special Interest Group on Artificial Intelligence, as a proxy for interest in the subject, the author writes:[34]  (...) I located two data sources, neither of which supports the idea of a broadly based AI winter during the 1970s. One is membership of ACM's SIGART, the major venue for sharing news and research abstracts during the 1970s. When the Lighthill report was published in 1973 the fast-growing group had 1,241 members, approximately twice the level in 1969. The next five years are conventionally thought of as the darkest part of the first AI winter. Was the AI community shrinking? No! By mid-1978 SIGART membership had almost tripled, to 3,500. Not only was the group growing faster than ever, it was increasing proportionally faster than ACM as a whole which had begun to plateau (expanding by less than 50% over the entire period from 1969 to 1978). One in every 11 ACM members was in SIGART.  The setbacks of the late 1980s and early 1990s [ edit ]  The collapse of the LISP machine market [ edit ]  In the 1980s, a form of AI program called an ""expert system"" was adopted by corporations around the world. The first commercial expert system was XCON, developed at Carnegie Mellon for Digital Equipment Corporation, and it was an enormous success: it was estimated to have saved the company 40 million dollars over just six years of operation. Corporations around the world began to develop and deploy expert systems and by 1985 they were spending over a billion dollars on AI, most of it to in-house AI departments. An industry grew up to support them, including software companies like Teknowledge and Intellicorp (KEE), and hardware companies like Symbolics and LISP Machines Inc. who built specialized computers, called LISP machines, that were optimized to process the programming language LISP, the preferred language for AI research in the USA.[35][36]  In 1987, three years after Minsky and Schank's prediction, the market for specialized LISP-based AI hardware collapsed. Workstations by companies like Sun Microsystems offered a powerful alternative to LISP machines and companies like Lucid offered a LISP environment for this new class of workstations. The performance of these general workstations became an increasingly difficult challenge for LISP Machines. Companies like Lucid and Franz LISP offered increasingly powerful versions of LISP that were portable to all UNIX systems. For example, benchmarks were published showing workstations maintaining a performance advantage over LISP machines.[37] Later desktop computers built by Apple and IBM would also offer a simpler and more popular architecture to run LISP applications on. By 1987, some of them had become as powerful as the more expensive LISP machines. The desktop computers had rule-based engines such as CLIPS available.[38] These alternatives left consumers with no reason to buy an expensive machine specialized for running LISP. An entire industry worth half a billion dollars was replaced in a single year.[39]  By the early 1990s, most commercial LISP companies had failed, including Symbolics, LISP Machines Inc., Lucid Inc., etc. Other companies, like Texas Instruments and Xerox, abandoned the field. A small number of customer companies (that is, companies using systems written in LISP and developed on LISP machine platforms) continued to maintain systems. In some cases, this maintenance involved the assumption of the resulting support work.[40]  Slowdown in deployment of expert systems [ edit ]  By the early 1990s, the earliest successful expert systems, such as XCON, proved too expensive to maintain. They were difficult to update, they could not learn, they were ""brittle"" (i.e., they could make grotesque mistakes when given unusual inputs), and they fell prey to problems (such as the qualification problem) that had been identified years earlier in research in nonmonotonic logic. Expert systems proved useful, but only in a few special contexts.[41][42] Another problem dealt with the computational hardness of truth maintenance efforts for general knowledge. KEE used an assumption-based approach supporting multiple-world scenarios that was difficult to understand and apply.  The few remaining expert system shell companies were eventually forced to downsize and search for new markets and software paradigms, like case-based reasoning or universal database access. The maturation of Common Lisp saved many systems such as ICAD which found application in knowledge-based engineering. Other systems, such as Intellicorp's KEE, moved from LISP to a C++ (variant) on the PC and helped establish object-oriented technology (including providing major support for the development of UML (see UML Partners).  The end of the Fifth Generation project [ edit ]  In 1981, the Japanese Ministry of International Trade and Industry set aside $850 million for the Fifth Generation computer project. Their objectives were to write programs and build machines that could carry on conversations, translate languages, interpret pictures, and reason like human beings. By 1991, the impressive list of goals penned in 1981 had not been met. According to HP Newquist in The Brain Makers, ""On June 1, 1992, The Fifth Generation Project ended not with a successful roar, but with a whimper.""[40] As with other AI projects, expectations had run much higher than what was actually possible.[43][44]  Strategic Computing Initiative cutbacks [ edit ]  In 1983, in response to the fifth generation project, DARPA again began to fund AI research through the Strategic Computing Initiative. As originally proposed the project would begin with practical, achievable goals, which even included artificial general intelligence as long-term objective. The program was under the direction of the Information Processing Technology Office (IPTO) and was also directed at supercomputing and microelectronics. By 1985 it had spent $100 million and 92 projects were underway at 60 institutions, half in industry, half in universities and government labs. AI research was well-funded by the SCI.[45]  Jack Schwarz, who ascended to the leadership of IPTO in 1987, dismissed expert systems as ""clever programming"" and cut funding to AI ""deeply and brutally"", ""eviscerating"" SCI. Schwarz felt that DARPA should focus its funding only on those technologies which showed the most promise, in his words, DARPA should ""surf"", rather than ""dog paddle"", and he felt strongly AI was not ""the next wave"". Insiders in the program cited problems in communication, organization and integration. A few projects survived the funding cuts, including pilot's assistant and an autonomous land vehicle (which were never delivered) and the DART battle management system, which (as noted above) was successful.[46]  AI winter of the 1990's and early 2000's [ edit ]  A survey of reports from the early 2000's suggests that AI's reputation was still poor:  Alex Castro, quoted in The Economist , 7 June 2007: ""[Investors] were put off by the term 'voice recognition' which, like 'artificial intelligence', is associated with systems that have all too often failed to live up to their promises."" [ 47 ]  , 7 June 2007: ""[Investors] were put off by the term 'voice recognition' which, like 'artificial intelligence', is associated with systems that have all too often failed to live up to their promises."" Patty Tascarella in Pittsburgh Business Times , 2006: ""Some believe the word 'robotics' actually carries a stigma that hurts a company's chances at funding."" [ 48 ]  , 2006: ""Some believe the word 'robotics' actually carries a stigma that hurts a company's chances at funding."" John Markoff in the New York Times, 2005: ""At its low point, some computer scientists and software engineers avoided the term artificial intelligence for fear of being viewed as wild-eyed dreamers."" [ 49 ]  Many researchers in AI in the mid 2000's deliberately called their work by other names, such as informatics, machine learning, analytics, knowledge-based systems, business rules management, cognitive systems, intelligent systems, intelligent agents or computational intelligence, to indicate that their work emphasizes particular tools or is directed at a particular sub-problem. Although this may be partly because they consider their field to be fundamentally different from AI, it is also true that the new names help to procure funding by avoiding the stigma of false promises attached to the name ""artificial intelligence"".[49][50]  In the late 1990's and early 21st century, AI technology became widely used as elements of larger systems,[51] but the field is rarely credited for these successes. In 2006, Nick Bostrom explained that ""a lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.""[53] Rodney Brooks stated around the same time that ""there's this stupid myth out there that AI has failed, but AI is around you every second of the day.""  Current AI spring (2022–present) [ edit ]  AI has reached the highest levels of interest and funding in its history in the past few years by every possible measure, including: publications, patent applications,[56] total investment ($50 billion in 2022), and job openings (800,000 U.S. job openings in 2022). The successes of the current ""AI spring"" or ""AI boom"" are advances in language translation (in particular, Google Translate), image recognition (spurred by the ImageNet training database) as commercialized by Google Image Search, and in game-playing systems such as AlphaZero (chess champion) and AlphaGo (go champion), and Watson (Jeopardy champion). A turning point was in 2012 when AlexNet (a deep learning network) won the ImageNet Large Scale Visual Recognition Challenge with half as many errors as the second place winner.  The 2022 release of OpenAI's AI chatbot ChatGPT which as of January 2023 has over 100 million users,[60] has reinvigorated the discussion about artificial intelligence and its effects on the world.[61][62]  See also [ edit ]  Notes [ edit ]  References [ edit ]","research, translation, lisp, winter, funding, system, systems, intelligence, machine, ai, edit",
52,AI Winter Is Coming,".Wp-Block-Co-Authors-Plus-Coauthors.Is-Layout-Flow, Class, Wp-Block-Co-Authors-Plus, Display Inline, .Wp-Block-Co-Authors-Plus-Avatar, Where Img, Height Auto Max-Width, Vertical-Align Bottom .Wp-Block-Co-Authors-Plus-Coauthors.Is-Layout-Flow .Wp-Block-Co-Authors-Plus-Avatar, Vertical-Align Middle .Wp-Block-Co-Authors-Plus-Avatar Is .Alignleft .Alignright, Display Table .Wp-Block-Co-Authors-Plus-Avatar.Aligncenter Display Table Margin-Inline Auto",https://mindmatters.ai/2018/11/is-a-bad-ai-winter-looming/,"And yet …But it’s those very self-driving cars that are causing scientists to sweat the possibility of another AI winter. Eleanor Cummins, “Another AI winter could usher in a dark period for artificial intelligence” at Popular ScienceCummins is not alone, nor is she the first to notice. What is an “AI Winter”? — We have adversaries: Assuming, for the moment, that we can ameliorate the previous problems, modern AI remains fragile for other reasons. ), it points up a sober reality: Fooling modern AI systems is not that hard, especially when you’re determined to do so.","Self-driving cars, robotic surgeons, and digital assistants seem to promise a world in which AI can do anything. And yet …  But it’s those very self-driving cars that are causing scientists to sweat the possibility of another AI winter. In 2015, Tesla founder Elon Musk said a fully-autonomous car would hit the roads in 2018. (He technically still has four months.) General Motors is betting on 2019. And Ford says buckle up for 2021. But these predictions look increasingly misguided. And, because they were made public, they may have serious consequences for the field. Couple the hype with the recent death of a pedestrian in Arizona, who was killed in March by an Uber in driverless mode, and things look increasingly frosty for applied AI. Eleanor Cummins, “Another AI winter could usher in a dark period for artificial intelligence” at Popular Science  Cummins is not alone, nor is she the first to notice. Even venture capitalists have been asking, is AI doomed to yet another period of darkness?  What is an “AI Winter”? Researchers coined the term in the mid-1980s, after observing that AI projects appeared to follow a boom-and-bust cycle, not that you hear much of that checkered history from boosters. Roughly every decade since the late 1960s has experienced a promising wave of AI that later crashed on real-world problems, leading to collapses in research funding.  So, is AI currently headed for another round of winter doldrums? Will it again retreat into relative hibernation as funding dries up and computer scientists take on other challenges? Always winter and never Christmas? I don’t believe so. Though AI has no hope of ever fulfilling the fever-dreams of Elon Musk or Ray Kurzweil, things really are different this time. But other dangers loom.  First, what caused previous AI winters? There was one straightforward reason: The technology did not work. Expert systems weren’t experts. Language translators failed to translate. Even Watson, after winning Jeopardy, failed to provide useful answers in the real-world context of medicine. When technology fails, winters come.  Nearly all of AI’s recent gains have been realized due to massive increases in data and computing power that enable old algorithms to suddenly become useful. For example, researchers first conceived neural networks—the core idea powering much machine learning and AI’s notable advances—in the late 1950s. The worries of an impending winter arise because we’re approaching the limits of what massive data combined with hordes of computers can do.  Though some among the current slate of so-called AI marvels are failures (I’m looking at you, Watson) and limits do exist, others are delivering as promised, in the real-world, with live data—and therefore real problems. As long as an idea works, people will pay, funding will remain, and winter can be held off. But, as spectacular new results come less often, our attention will drift, and we’ll no longer “see” the AI around us, chugging away through unimaginable amounts of data to make decisions affecting us all. And that’s the problem.  A couple of months ago, Amazon “fired” an AI tool it had created to screen resumés when they learned it showed bias against women. If they had read mathematician Cathy O’Neil’s book, Weapons of Math Destruction (available at Amazon, naturally), they might have been more wary. You see, she has raised the problem of data bias in AI, and computer algorithms more generally, for years. While Amazon may have caught their misogynistic resumé reader in time, data-backed AI is quickly expanding into business and elsewhere; those systems, too, likely have hidden, embedded biases.  Here are a few other problems to watch for with modern, data-driven AI:  — Data biases: This is Dr. O’Neil’s concern. The data used to train the AI system fails to include all the cases the system might encounter, leading to biased decisions of all kinds (such as, not granting loans or misinterpreting a medical condition).  — Plain ol’ mistakes: Even with sufficient, good data, AI remains an algorithm (of sorts). Algorithms can make mistakes. The problem with nearly all modern AI systems is that the “reasons” for a decision are opaque; that is, no one knows why the system chooses one thing over another. Mistakes, then, look just like right answers.  It happened, for example, when a hospital AI system informed doctors that patients with asthma would be at low risk for pneumonia complications if sent home. The problem is, asthma sufferers were at low risk for complications because they never were sent home; they were sent to intensive care instead. But the artificial intelligence system did not “know” that. It didn’t “know” anything.” Fortunately, no one took its advice.  — Failing off the edge: Last spring, Wei Huang’s Tesla drove head-first into a concrete highway barrier, causing the car to burst into flames, and killing Huang. Tesla, naturally, reminded everyone that their “autopilot” system was designed only to assist drivers. But what human would, under normal freeway conditions, suddenly, as if for no reason, decide to slam head-first into the lane dividers? Humans can make approximations. They can ease away from a decision. They can choose to go only partway. Computers, not so much. Because machines, at bottom, speak only zeros and ones, edges emerge in the system. Those edges can be fatal.  — We have adversaries: Assuming, for the moment, that we can ameliorate the previous problems, modern AI remains fragile for other reasons. Last year, researchers fooled Google’s image recognition system into misidentifying a turtle as a gun. While that may be a humorous episode (as long the system does not misidentify a gun as a turtle!), it points up a sober reality: Fooling modern AI systems is not that hard, especially when you’re determined to do so. For example, subtle changes to street signs could fool self-driving cars. Bad actors are well aware of these possibilities.  — Alternate realities: We tend to believe what we see. Which is why, if we can make up what we see, we can affect what we believe. The same AI that can help us spot weapons in airplane luggage can be used to create fake images or video, indistinguishable from the real thing. So-called “deep fakes” can be so convincing that “even people who personally know the subject in question—President Obama, in one example—couldn’t tell it was fake” (BBC, 2017).  So, what do we do? We should do what we do any time we have a technology that can both help and hurt. We should put boundaries in place to protect against errors. We should stop promoting and believing ridiculous hype. And we should never forget the absolute, stunning thing we call the human mind. Even if AI never experiences another winter.  Brendan Dixon is a Software Architect with experience designing, creating, and managing projects of all sizes. He first foray into Artificial Intelligence was in the 1980s when he built an Expert System to assist in the diagnosis of software problems at IBM. Though he’s spent the majority of his career on other types of software, he’s remained engaged and interested in the field.  Also by Brendan Dixon: The “Superintelligent AI” Myth The problem that even the skeptical Deep Learning researcher left out  and  There is no universal moral machine Brendan Dixon’s view of MIT’s Moral Machine is featured.","coming, winter, system, thing, problem, systems, data, tesla, problems, modern, ai",2018-11-29 17:47:37+00:00
53,Why We Are Not Heading Into An AI Winter,,https://hackernoon.com/why-we-are-not-heading-into-an-ai-winter-d7e46536f6ec,"For the past few years as AI has risen in stature, we have witnessed regular takedowns of the tech and posts about why this is just another temporary boom preceding another AI winter. Filip Piekniewski wrote a great piece back in May arguing that an AI winter was well on it’s way. So when I read about an AI winter, I’m very skeptical that people are looking at the right signals. I don’t think we are headed into another AI winter. Don’t underinvest in AI because of this AI winter scare.","This is republished from my newsletter at inside.com/ai.  For the past few years as AI has risen in stature, we have witnessed regular takedowns of the tech and posts about why this is just another temporary boom preceding another AI winter. Filip Piekniewski wrote a great piece back in May arguing that an AI winter was well on it’s way. Other takes on the subject include an analysis by Ron Schmelzer and a very recent piece in Popular Science. But I don’t buy it.  The biggest problem with AI is that it’s different tech, and the world has changed a lot since previous AI winters. But soooo many people in tech constantly use the frameworks of the last technology wave to evaluate the current one. Several of my own experiences have reinforced this. I remember keying in to the topic when Josh Kopelman from First Round Capital hosted a small dinner in Boston many years ago. Josh sold Half.com to Ebay so was known as an ecommerce guy. But that was 1999, and this dinner was probably 2011 or 2012. Josh said ecommerce entrepreneurs come to him because of his experience but, then he noted that the three biggest customer acquisition channels for most of them: Social, Mobile, SEO, were not even around when he built Half.com. He wasn’t sure his personal experiences were relevant to modern ecommerce companies. (side note — all the best investors I’ve seen have this kind of intellectual humility and self awareness).  My last company was a SaaS company started in December 2008. Many of our early investors did not understand SaaS metrics. I constantly had to fight the battle of being judged by the metrics of the packaged software industry rather than as a recurring revenue business. While the laggards came around, it was a lot of education on my part, and from the more progressive investors on my Board, to get everyone comfortable with MRR, CAC, and LTV.  AI is where SaaS was in 2009. Most people don’t understand that it requires different frameworks to evaluate it. So when I read about an AI winter, I’m very skeptical that people are looking at the right signals.  Consider this — most of the AI news that has driven hype cycles over the past 3 years is just research. Very little of it has been news about really great AI applied in products. That research news is stalling not because AI is stalling, but because AI is working it’s way from the research labs into more applications. The problems to solve there are real, and aren’t very sexy to write about. I see this at Talla. Our team recently worked to put a Bidaf Machine Comprehension model into the product. While it’s a model that can perform really well in isolation on a synthetic data set, getting it to work in the real world was messy. We had to solve a bunch of very pragmatic implementation problems, and some conceptual problems (the model always returns an answer, and we had to train it not to return something if the answer isn’t good or relevant). They weren’t sexy or newsworthy, but we believe we pushed practical implementations of Bidaf forward pretty far.  Enterprise adoption of AI is still in the very early days. Again, I see this first hand in Talla’s sales pipeline. Two years ago, no one had budget for an AI project. One year ago, a few really forward thinking companies had budget for prototypes and paid pilots. Now, those more progressive companies have budgets and projects and requirements all around AI. It’s still small but, the difference is noticeable. It will be 5–7 years before all enterprises are educated enough on AI to start buying and implementing it effectively.  Previous AI winters have happened because things never really made it out of the research lab into real adoption, outside of very targeted use cases. Now, while we definitely aren’t seeing the amazing we-are-close-to-GAI types of applications that the research sometimes suggests should be here, we are seeing real applications that have real value in the world.  The framework I use for thinking about it is actually a consumer adoption framework that I first learned about from BIjan Sabet of Spark Capital, when he spoke at a conference. He said too many investors who don’t invest in consumer companies regularly freak out when growth of new users stalls, but that, it happens all the time. Consumer companies go through waves of viral adoption vs deeper user engagement, and so, he said that the right way to think about it is as this oscillating wave. You get a bunch of new users, and then user growth slows, so you focus on getting the existing user base more engaged. This eventually leads to a new viral growth trend, etc.  I think a similar framework will drive AI adoption for the next decade. Companies will deploy the latest and greatest out of research labs, but it will be slow. They have to find (and cleanse) data sets that work. They have to solve pragmatic problems like whether inference should happen on the edge, or centralized in the cloud (which has both performance and privacy issues), how to shard large models in production systems, how to effectively QA models that have probabilistic outputs, and much more. These things take time. So I think what you will see in AI going forward is an oscillation between smarter algorithms that do cooler things, then what seems like stalled innovation but is really just the practical time period of everyone being more concerned with applying the new algorithms rather than inventing new ones. Once the new stuff gets deployed into production systems effectively, we will shift back to focusing on algorithm performance problems (rather than those due to implementation challenges) and need better algorithmic approaches.  I don’t think we are headed into another AI winter. I think we are just in a different part of the oscillation wave — laying the groundwork for the next research boom beyond DNNs.  On top of that, you have massive innovation in neuromorphic chips and tool sets to make deep learning more easily applied by non-experts. Both of these will take a few years to really take hold, but they will break the frameworks people currently use to think about what products and services can be built with AI. I’m particularly excited about the innovation that will come from breaking the x86 chip architecture framework we’ve relied on for so long.  In summary, I think this talk of an AI winter is wrong. It’s focused on a slow down in the research, but that’s a combination of over-hyped research news along with the slow arduous work of really getting these systems running in enterprises. Don’t underinvest in AI because of this AI winter scare. Now is the time to push forward and lay the groundwork for the next waves.","really, research, winter, dont, think, companies, adoption, real, heading, problems, ai",
54,Self-driving cars are headed toward an AI roadblock,Russell Brandom,https://www.theverge.com/2018/7/3/17530232/self-driving-ai-winter-full-autonomy-waymo-tesla-uber,"There’s growing concern among AI experts that it may be years, if not decades, before self-driving systems can reliably avoid accidents. That leaves Tesla and other autonomy companies with a scary question: Will self-driving cars keep getting better, like image search, voice recognition, and the other AI success stories? But nearly every car accident involves some sort of unforeseen circumstance, and without the power to generalize, self-driving cars will have to confront each of these scenarios as if for the first time. “I see all these micro-improvements as extraordinary features on the journey towards full autonomy.”Still, it’s not clear how long self-driving cars can stay in their current limbo. One study by the Rand Corporation estimated that self-driving cars would have to drive 275 million miles without a fatality to prove they were as safe as human drivers.","Part of / The Real-World AI Issue  If you believe the CEOs, a fully autonomous car could be only months away. In 2015, Elon Musk predicted a fully autonomous Tesla by 2018; so did Google. Delphi and MobileEye’s Level 4 system is currently slated for 2019, the same year Nutonomy plans to deploy thousands of driverless taxis on the streets of Singapore. GM will put a fully autonomous car into production in 2019, with no steering wheel or ability for drivers to intervene. There’s real money behind these predictions, bets made on the assumption that the software will be able to catch up to the hype.  On its face, full autonomy seems closer than ever. Waymo is already testing cars on limited-but-public roads in Arizona. Tesla and a host of other imitators already sell a limited form of Autopilot, counting on drivers to intervene if anything unexpected happens. There have been a few crashes, some deadly, but as long as the systems keep improving, the logic goes, we can’t be that far from not having to intervene at all.  But the dream of a fully autonomous car may be further than we realize. There’s growing concern among AI experts that it may be years, if not decades, before self-driving systems can reliably avoid accidents. As self-trained systems grapple with the chaos of the real world, experts like NYU’s Gary Marcus are bracing for a painful recalibration in expectations, a correction sometimes called “AI winter.” That delay could have disastrous consequences for companies banking on self-driving technology, putting full autonomy out of reach for an entire generation.  “Driverless cars are like a scientific experiment where we don’t know the answer”  It’s easy to see why car companies are optimistic about autonomy. Over the past ten years, deep learning — a method that uses layered machine-learning algorithms to extract structured information from massive data sets — has driven almost unthinkable progress in AI and the tech industry. It powers Google Search, the Facebook News Feed, conversational speech-to-text algorithms, and champion Go-playing systems. Outside the internet, we use deep learning to detect earthquakes, predict heart disease, and flag suspicious behavior on a camera feed, along with countless other innovations that would have been impossible otherwise.  But deep learning requires massive amounts of training data to work properly, incorporating nearly every scenario the algorithm will encounter. Systems like Google Images, for instance, are great at recognizing animals as long as they have training data to show them what each animal looks like. Marcus describes this kind of task as “interpolation,” taking a survey of all the images labeled “ocelot” and deciding whether the new picture belongs in the group.  Engineers can get creative in where the data comes from and how it’s structured, but it places a hard limit on how far a given algorithm can reach. The same algorithm can’t recognize an ocelot unless it’s seen thousands of pictures of an ocelot — even if it’s seen pictures of housecats and jaguars, and knows ocelots are somewhere in between. That process, called “generalization,” requires a different set of skills.  For a long time, researchers thought they could improve generalization skills with the right algorithms, but recent research has shown that conventional deep learning is even worse at generalizing than we thought. One study found that conventional deep learning systems have a hard time even generalizing across different frames of a video, labeling the same polar bear as a baboon, mongoose, or weasel depending on minor shifts in the background. With each classification based on hundreds of factors in aggregate, even small changes to pictures can completely change the system’s judgment, something other researchers have taken advantage of in adversarial data sets.  Marcus points to the chat bot craze as the most recent example of hype running up against the generalization problem. “We were promised chat bots in 2015,” he says, “but they’re not any good because it’s not just a matter of collecting data.” When you’re talking to a person online, you don’t just want them to rehash earlier conversations. You want them to respond to what you’re saying, drawing on broader conversational skills to produce a response that’s unique to you. Deep learning just couldn’t make that kind of chat bot. Once the initial hype faded, companies lost faith in their chat bot projects, and there are very few still in active development.  That leaves Tesla and other autonomy companies with a scary question: Will self-driving cars keep getting better, like image search, voice recognition, and the other AI success stories? Or will they run into the generalization problem like chat bots? Is autonomy an interpolation problem or a generalization problem? How unpredictable is driving, really?  It may be too early to know. “Driverless cars are like a scientific experiment where we don’t know the answer,” Marcus says. We’ve never been able to automate driving at this level before, so we don’t know what kind of task it is. To the extent that it’s about identifying familiar objects and following rules, existing technologies should be up to the task. But Marcus worries that driving well in accident-prone scenarios may be more complicated than the industry wants to admit. “To the extent that surprising new things happen, it’s not a good thing for deep learning.”  “Safety isn’t just about the quality of the AI technology”  The experimental data we have comes from public accident reports, each of which offers some unusual wrinkle. A fatal 2016 crash saw a Model S drive full speed into the rear portion of a white tractor trailer, confused by the high ride height of the trailer and bright reflection of the sun. In March, a self-driving Uber crash killed a woman pushing a bicycle, after she emerged from an unauthorized crosswalk. According to the NTSB report, Uber’s software misidentified the woman as an unknown object, then a vehicle, then finally as a bicycle, updating its projections each time. In a California crash, a Model X steered toward a barrier and sped up in the moments before impact, for reasons that remain unclear.  Each accident seems like an edge case, the kind of thing engineers couldn’t be expected to predict in advance. But nearly every car accident involves some sort of unforeseen circumstance, and without the power to generalize, self-driving cars will have to confront each of these scenarios as if for the first time. The result would be a string of fluke-y accidents that don’t get less common or less dangerous as time goes on. For skeptics, a turn through the manual disengagement reports shows that scenario already well under way, with progress already reaching a plateau.  Andrew Ng — a former Baidu executive, Drive.AI board member, and one of the industry’s most prominent boosters — argues the problem is less about building a perfect driving system than training bystanders to anticipate self-driving behavior. In other words, we can make roads safe for the cars instead of the other way around. As an example of an unpredictable case, I asked him whether he thought modern systems could handle a pedestrian on a pogo stick, even if they had never seen one before. “I think many AV teams could handle a pogo stick user in pedestrian crosswalk,” Ng told me. “Having said that, bouncing on a pogo stick in the middle of a highway would be really dangerous.”  “Rather than building AI to solve the pogo stick problem, we should partner with the government to ask people to be lawful and considerate,” he said. “Safety isn’t just about the quality of the AI technology.”  “This is not an easily isolated problem”  Deep learning isn’t the only AI technique, and companies are already exploring alternatives. Though techniques are closely guarded within the industry (just look at Waymo’s recent lawsuit against Uber), many companies have shifted to rule-based AI, an older technique that lets engineers hard-code specific behaviors or logic into an otherwise self-directed system. It doesn’t have the same capacity to write its own behaviors just by studying data, which is what makes deep learning so exciting, but it would let companies avoid some of the deep learning’s limitations. But with the basic tasks of perception still profoundly shaped by deep learning techniques, it’s hard to say how successfully engineers can quarantine potential errors.  Ann Miura-Ko, a venture capitalist who sits on the board of Lyft, says she thinks part of the problem is high expectations for autonomous cars themselves, classifying anything less than full autonomy as a failure. “To expect them to go from zero to level five is a mismatch in expectations more than a failure of technology,” Miura-Ko says. “I see all these micro-improvements as extraordinary features on the journey towards full autonomy.”  Still, it’s not clear how long self-driving cars can stay in their current limbo. Semi-autonomous products like Tesla’s Autopilot are smart enough to handle most situations, but require human intervention if anything too unpredictable happens. When something does go wrong, it’s hard to know whether the car or the driver is to blame. For some critics, that hybrid is arguably less safe than a human driver, even if the errors are hard to blame entirely on the machine. One study by the Rand Corporation estimated that self-driving cars would have to drive 275 million miles without a fatality to prove they were as safe as human drivers. The first death linked to Tesla’s Autopilot came roughly 130 million miles into the project, well short of the mark.  But with deep learning sitting at the heart of how cars perceive objects and decide to respond, improving the accident rate may be harder than it looks. “This is not an easily isolated problem,” says Duke professor Mary Cummings, pointing to an Uber crash that killed a pedestrian earlier this year. “The perception-decision cycle is often linked, as in the case of the pedestrian death. A decision was made to do nothing based on ambiguity in perception, and the emergency braking was turned off because it got too many false alarms from the sensor”  That crash ended with Uber pausing its self-driving efforts for the summer, an ominous sign for other companies planning rollouts. Across the industry, companies are racing for more data to solve the problem, assuming the company with the most miles will build the strongest system. But where companies see a data problem, Marcus sees something much harder to solve. “They’re just using the techniques that they have in the hopes that it will work,” Marcus says. “They’re leaning on the big data because that’s the crutch that they have, but there’s no proof that ever gets you to the level of precision that we need.”","headed, companies, marcus, problem, cars, roadblock, systems, data, selfdriving, learning, ai, deep",2018-07-03 00:00:00
55,AI winter - Addendum,,https://blog.piekniewski.info/2018/06/06/ai-winter-addendum/,"My previous post on AI winter went viral almost to the point of killing my Amazon instance (it got well north of 100k views). From this empirical evidence one thing is clear - whether the AI winter is close or not, it is a very sensitive and provocative subject. First off, many citations to my post were put in context that the AI hype is fading. It is only once there is nobody to buy this, which is long after seed financing had dried out, when the AI winter becomes official. The main line of defense against the AI winter is that this time around AI actually brings profit and there are real applications.","My previous post on AI winter went viral almost to the point of killing my Amazon instance (it got well north of 100k views). It triggered a serious tweet storms, lots of discussion on hackernews and reddit. From this empirical evidence one thing is clear - whether the AI winter is close or not, it is a very sensitive and provocative subject. Almost as if many people felt something under their skin...  Anyway, in this quick followup post, I'd like to respond to some of the points and explain some misunderstandings.  Hype is not fading, it is cracking.  First off, many citations to my post were put in context that the AI hype is fading. This was not my point at all. The hype is doing very well. Some of the major propagandists have gone quieter but much like I explained in the post, on the surface everything is still nice and colorful. You have to look below the propaganda to see the cracks. It would actually be great if the hype faded down but that is not how it works. When the stock market crashes, it is not like everybody slowly begin to admit that they overpaid for their stocks and quietly go home. It happens in sudden, violent attacks of panic, where everybody tries to sell off, while at the same time, same people pump the narrative so that they could find buyers [pump and dump]. Crisis is only announced once the market truly runs out of buyers and the pumpers run out of cash. So hype is a lagging indicator (often by quite a bit). I predict it will be like that with AI. Each fatality caused by a self driving car will cut the number of VC's likely to invest in AI by half. Same for every AI startup that quietly folds down. At the same time those who already heavily invested, will be pumping the propaganda while quietly trying to liquidate their assets. It is only once there is nobody to buy this, which is long after seed financing had dried out, when the AI winter becomes official.  Applications stupid!  The main line of defense against the AI winter is that this time around AI actually brings profit and there are real applications. OK. There are applications. Primarily for image search, speech recognition and perhaps surveillance (aka Google and Facebook etc.). There is the style transfer which will certainly make Photoshop great again. But these are all almost 3 years old now. I actually went to ""Applications of Deep Learning"" session on last ICML conference in Sydney. Let me just put it very mildly: this was a superbly underwhelming session.  Now regarding influence on winter, it actually does not matter how much money AI brings today. What matters is how much people invested, and hence how much return they expect in the future. If the reality does not match these expectations there will be a winter. Period. The amount of investment in AI in this cycle is enormous. And the focal point of that investment is in autonomous vehicles and by autonomous I don't mean remote controlled or with a safety driver - this stuff is only economical if they are truly autonomous. Coincidentally, this is the application which I think has the smallest chance of materializing.  But Waymo!  But Waymo what? That they are buying up to 60,000 vans over undefined period of time? So what? Uber ordered 20,000 Volvos late last year. I wonder how that deal is going. But Waymo tests AV's without safety drivers! Yes, in the most quiet and slow block in Phenix with perfect cellular reception such that they could constantly monitor these cars remotely. Oh and BTW, they have a speed limit of 25 mph... Anyway long story short: Waymo can deploy even a million LTE monitored, remote controlled cars. That proves nothing regarding autonomous car, because such deployment will happen at a massive loss. Obviously Google can pump money into them for as long as Google has the money, which will probably be for a while. Google AV project has been around for 10 years, I expect it to go for another 10 years. Unless they hit and kill somebody. At this point they are done. And that is why they are extremely cautious.  A few recent examples of Deep fail  After stirring a violent tweet storm with my post, a few very interesting papers came out and a few other had been brought to my attention:  These combined with good old gradient derived adversarial examples just exemplify how brittle these methods are. We are far away from robust perception and in my opinion we are stuck in a wrong paradigm and hence we are not even moving in the right direction.  Happy reading!  If you found an error, highlight it and press Shift + Enter or click here to inform us.  Related  Comments  comments","autonomous, google, quietly, winter, addendum, hype, post, actually, waymo, point, ai",2018-06-06 00:00:00
56,The AI Winter is Over. Here's Why.,"Ben Wilde, Principal Engineer - Store Platforms, Nevin Taylor, Advisor, Consultant, Subject Matter Expert, Conrad Christink, Software Architect At Systems, Steven Tshakatumba, Management Consulting",https://www.linkedin.com/pulse/ai-winter-over-heres-why-jason-roell,"Some have heard of the term “AI” used before and even have some experience with AI applications as far back as the 70s. Unfortunately, there was a problem with these types of AI expert systems. Another drawback to expert systems is that they are very costly to build. However, that is no longer the case, and the ""AI Winter"" may soon be over. Big DataBig Data this, Big Data that...","Unless you’re living under a rock, you’ve probably noticed Artificial Intelligence (AI) is popping up more and more in technology talks and business strategies. I’ve even noticed among my friends an increased interest in “cognifying” their applications.  It's easy to see why. Everyone is aware of the autonomous car revolution, and, if you are in the loop, you know it’s due largely to the advancements in AI, particularly ""Machine Learning,"" which is a strategy used to implement AI in software applications or robots running software. More on this later.  Let's first step back and discuss AI. What does it mean for a machine to possess artificial intelligence?  At its core, it’s a simple idea. Artificial Intelligence is the broader concept of machines carrying out tasks in a way that we consider “smart.”  And,  Machine Learning is a current application of AI based upon the idea that we should give machines access to data and let them learn for themselves.  Some have heard of the term “AI” used before and even have some experience with AI applications as far back as the 70s. Most people's familiarity with AI is thanks to gaming. It is common to play an AI in a video game when you don't have another person to play with. Other AI applications with which users are familiar include tools like spell checkers and other helpful systems that seem partially smart in helping humans complete a task using well-defined rules. However, many of these ""older AIs"" were developed and implemented in what we call an ""Expert System"". This means that to codify the intelligence into the program or software, we would need an expert, such as a linguistic expert when talking about spell check or a medical expert when talking about systems that help doctors diagnose patients. This type of AI system was very popular in the 80's and 90's.  Unfortunately, there was a problem with these types of AI expert systems. As anyone who has used an AI that was implemented with an expert system approach can attest, these systems constantly made mistakes when dealing with uncommon scenarios or in situations where even the expert was not well versed. The AI was useless in these situations, and fixing these systems called for reprograming them with new expert information. Another drawback to expert systems is that they are very costly to build. It requires finding for each particular domain an expert who can articulate to programmers the intricacies of their field and when and why any given decision should be made. These types of decisions are hard to codify when writing a deterministic algorithm (a deterministic algorithm is an algorithm which, given a particular input, will always produce the same output, with the underlying machine always passing through the same sequence of states).  For these reasons, artificial intelligence researchers needed to invent a better way to give ""smarts"" to a machine.  This is where Machine Learning (ML) comes into play. Many people are surprised to learn that ML is actually a relatively old topic when it comes to AI research. Researchers understood back in the 80s that expert systems were never going to create an AI that would be capable of driving our cars or beating the best humans in chess or Jeopardy. That is because the parameters of problems such as these are too varied, change over time, and have many different weights based on different states of the applications. Moreover, there are many attributes to a problem that cannot be directly observed and thus cannot be directly programmed into the application logic.  Machine Learning addresses this problem by developing a program capable of learning and decision making based upon accessible data, similar to human cognition, instead of programming the machine to perform a deterministic task. So, instead of making a deterministic decision, the program relies on probability and a probability threshold to decide if it ""knows"" or “doesn’t know"" something. This is how the human brain makes decisions in the complex world in which we live.  For example, if you see a flower that looks like a rose and you say, ""Yep that is a rose,"" what you are really saying is, ""Yep, based on my previous knowledge (data) on what I believe a rose to look like and what I've been told a rose looks like, I am 97 percent sure that this is a rose.""  On the other hand, if you saw an iris flower, and you haven't seen many before or don't have many representations in your memory as to what an iris looks like, you might say, ""I'm not sure what that flower is."" Your ""confidence interval"" is below the threshold of what you believe is acceptable for identifying the flower as an iris (let's call that 30 percent sure that it was an iris flower.)  Making decisions based on probability is what our brains do best. If we can model a computer program in the same conceptual way, then the implications for AI is potentially unlimited!  Okay, so the question you should ask now is, ""If we knew all this in the 70s and 80s then why is ML just now becoming popular?""  My answer to that is analogous to the scientific community just now verifying the existence of gravitational waves that Einstein proposed in theoretical physics so many years ago. We just didn't have the machinery or tools to validate or invalidate his theory, it was ahead of its time and its practical use had to wait.  Even though we've understood the mathematical model around machine learning for many years, the infrastructure, technology, and data needed to make machine learning a reality (as opposed to a theory) were not available in the 70s and 80s. However, that is no longer the case, and the ""AI Winter"" may soon be over.  Three basic ""raw materials"" are necessary to create a practical machine learning application. Let's talk about them.  1. Cheap Parallel Computation  Thinking is not a synchronous process. At its core, thinking is just pattern recognition. We are constantly seeing, feeling, hearing, tasting, or smelling patterns that activate different clusters of neurons. Millions of these ""pattern recognizing neurons"" communicate low-level patterns to the next neural network. This process continues until we reach the highest conceptual layers and most abstract patterns.  Each neuron can be thought of as an individual pattern recognition unit, and based on the input it gets from other neurons (recognized patterns) we can eventually make high-level decisions. We would like to model computer programs in a similar way. Modeling computer programs in a way that resembles the biological structure of the brain is called an ""artificial neural network."" How fitting!  Parallelization of this type of data processing is obviously vital to building a system that simulates human thought. That type of power was not available in the 80's and just recently became cheap enough for it to be practical in machine learning solutions.  Why now?  One word. GPUs.  Okay, maybe that's three words? Graphical Processing Units (GPUs) became popular because of their ability to process high graphics for video games, consoles, and then even cell phones. Graphics processing is inherently parallel, and these GPUs were architected in a way to take advantage of this type of computing. As GPUs became popular, they also became cheaper because companies competed against each to drive down prices of GPUs. It didn't take long to realize that GPUs might solve the computation problem that had been stumping researchers for decades. This could give them the necessary parallel computation that is required to build an artificial neural network. They were right, and this lowering of costs of GPUs enabled companies to buy massive amounts of them and use them in building machine learning platforms. This will greatly accelerate what we are able to build with highly parallelized neural networks and the amount of data we are able to process. Speaking of data...  2. Big Data  Big Data this, Big Data that...  I know everyone has heard all of the hype about big data. What does it really mean, though? Why is it here now? Where was it before? How big is ""big""??  Well, the truth of the matter is that when we talk about big data, we're really saying that we're capturing, processing, and generating more data every year and this data is growing exponentially.  The reason this is a big deal is that in order to train an artificial brain to learn for itself, you need a MASSIVE amount of data. The amount of visual data alone that a baby takes in and processes each year is more data than data centers had in the 80s. That's not even enough to train machines, though. That's because we don't want to wait a year to learn elementary vision! To train computers in artificial vision we need more data than some people can absorb in a lifetime. That has finally become possible because storing and recording so much data is cheap, fast, and everywhere and generated by everything!  The amount of data on our smartphone holds more data than most giant computers systems in the 80s. Data and memory to store that it has grown to epic proportions and has no indication of slowing down anytime soon. This data is crucial for the implementation of smart machines as it takes many instances of a problem to infer a probabilistically correct solution. Big data is the knowledge base from which these computers need to learn. All the knowledge to which an AI has access is the result of us collecting and feeding the AI more and more data and letting the machine learn from the underlying patterns in the data.  The power of AI comes when computers start recognizing patterns never seen to human practitioners. The machine will understand the data and recognize patterns in that data the same way our neurons begin to recognize certain patterns to problems that we have seen before. The advantage the machine has over us is the electronic signaling through circuitry, which is much faster than our biological chemical signaling over synapses in our brain. Without big data, our machines would have nothing from which to learn. The larger the data set, the smarter the AI will become and the quicker the machines will learn!  3. Better/Deep Algorithms  As I have alluded before, researchers invented artificial neural nets in the 1950s, but the problem was that even if they had the computing power, they still didn't have efficient algorithms to process these neural nets. There were just too many astronomically huge combinatorial relationships between a million—or a hundred million— neurons. Recently that has all changed. Breakthroughs in the algorithms involved in this process have lead to new types of artificial networks. Layered Networks.  For example, take the relatively simple task of recognizing that a face is a face. When a group of bits in a neural net is found to trigger a pattern—the image of an eye, for instance—that result (“It’s an eye!”) is moved up to another level in the neural net for further parsing. The next level might group two eyes together and pass that meaningful chunk on to another level of hierarchical structure that associates it with the pattern of a nose. It can take many millions of these nodes (each one producing a calculation feeding others around it), stacked up many levels high, to recognize a human face. In 2006, Geoff Hinton, then at the University of Toronto, made a key tweak to this method, which he dubbed “deep learning.” He was able to mathematically optimize results from each layer so that the learning accumulated faster as it proceeded up the stack of layers. Deep learning algorithms accelerated enormously a few years later when they were ported to GPUs. The code of deep learning alone is insufficient to generate complex logical thinking, but it is an essential component of all current AIs, including IBM’s Watson; DeepMind, Google’s search engine; and Facebook’s algorithms.  This perfect storm of cheap parallel computation, bigger data, and deeper algorithms generated the 60-years-in-the-making overnight success of AI. And this convergence suggests that as long as these technological trends continue—and there’s no reason to think they won’t—AI will keep improving.  Thanks for reading! I write these posts in my spare time and if you enjoyed it, the biggest complement to a blogger is to SHARE IT!  Also, you might be interested in subscribing to my blog at JasonRoell.com where I post about technology topics that I think are interesting for the general programmer or even technology enthusiast to know.  I would like to personally thank Kevin Kelly for the inspiration for this post. I highly recommend picking up his book ""The Inevitable"" in which he discusses these and many more processes in much more detail.  Sources:  The Inevitable: Understanding the 12 Technological Forces that will Shape our Future. -Kevin Kelly 2016  How to Create a Mind - Ray Kurzweil  http://www.forbes.com/sites/bernardmarr/2016/12/06/what-is-the-difference-between-artificial-intelligence-and-machine-learning/#7ed065c687cb","neural, big, winter, artificial, heres, way, expert, systems, data, machine, learning, ai",
57,AI Winter Isn’t Coming,Will Knight,https://www.technologyreview.com/s/603062/ai-winter-isnt-coming/?utm_campaign=internal&utm_medium=homepage&utm_source=top-stories_1&set=603054,"Andrew Ng, chief scientist at Baidu Research, and a major figure in the field of machine learning and AI, says improvements in computer processor design will keep performance advances and breakthroughs coming for the foreseeable future. The advances seen in recent years have come thanks to the development of powerful “deep learning” systems (see “10 Breakthrough Technologies 2013: Deep Learning”). This might not only increase the accuracy of existing deep learning tools, but also allow the technique to be leveraged in new areas, such as parsing and generating language. What’s more, Ng says, hardware advances will provide the fuel required to make emerging AI techniques feasible. The world’s leading AI experts convened in Barcelona this week for a prominent event called the Neural Information Processing Systems conference.","Andrew Ng, chief scientist at Baidu Research, and a major figure in the field of machine learning and AI, says improvements in computer processor design will keep performance advances and breakthroughs coming for the foreseeable future. “Multiple [hardware vendors] have been kind enough to share their roadmaps,” Ng says. “I feel very confident that they are credible and we will get more computational power and faster networks in the next several years.”  The field of AI has gone through phases of rapid progress and hype in the past, quickly followed by a cooling in investment and interest, often referred to as “AI winters.” The first chill occurred in the 1970s, as progress slowed and government funding dried up; another struck in the 1980s as the latest trends failed to have the expected commercial impact.  Then again, there’s perhaps been no boom to match the current one, propelled by rapid progress in training machines to do useful tasks. Artificial intelligence researchers are now offered huge wages to perform fundamental research, as companies build research teams on the assumption that commercially important breakthroughs will follow.  Andrew Ng, chief scientist at Baidu Research.  The advances seen in recent years have come thanks to the development of powerful “deep learning” systems (see “10 Breakthrough Technologies 2013: Deep Learning”). Starting a few years ago, researchers found that very large, or deep, neural networks could be trained, using labeled examples, to recognize all sorts of things with human-like accuracy. This has led to stunning advances in image and voice recognition and elsewhere.  Ng says these systems will only become more powerful. This might not only increase the accuracy of existing deep learning tools, but also allow the technique to be leveraged in new areas, such as parsing and generating language.  What’s more, Ng says, hardware advances will provide the fuel required to make emerging AI techniques feasible.  “There are multiple experiments I’d love to run if only we had a 10-x increase in performance,” Ng adds. For instance, he says, instead of having various different image-processing algorithms, greater computer power might make it possible to build a single algorithm capable of doing all sorts of image-related tasks.  The world’s leading AI experts convened in Barcelona this week for a prominent event called the Neural Information Processing Systems conference. The scale of the gathering, which has grown from several hundred people a few years ago to more than 6,000 this year, offers some sense of the huge interest there is in artificial intelligence.","research, coming, winter, scientist, progress, ai, advances, systems, researchers, learning, sorts, isnt, deep",
58,It's all fun and games until someone loses an AI,Katyanna Quach,http://www.theregister.co.uk/2016/09/28/kill_the_ai_hype_we_dont_want_another_ai_winter/,"Many speeches at AI conferences begin with AlphaGo, the Google-built AI that beat Lee Sedol - one of the highest ranked Go players in the world – to illustrate how far AI has progressed. “99 per cent of machine learning - or deep learning - is human work. Driverless cars have the potential to reduce traffic accidents and AI in healthcare may just help researchers tackle diseases. Machine learning is already applied in medicine to diagnose tumours. “The take-home message is that AI will not exterminate us; AI will empower us, ” said Etzioni.","Many speeches at AI conferences begin with AlphaGo, the Google-built AI that beat Lee Sedol - one of the highest ranked Go players in the world – to illustrate how far AI has progressed.  Most speakers briefly talk about how the computer programme works, and then go on to praise the intelligence of the machine. Oren Etzioni, CEO of the Allen Institute for Artificial Intelligence (AI2), however, gave the credit to the developers and not to AlphaGo.  The famous science fiction writer, Arthur C Clarke, once said: “Any sufficiently advanced technology is indistinguishable from magic.""  “But deep learning is not magic,” Etzioni said. “99 per cent of machine learning - or deep learning - is human work. The human team beat Lee Sedol.”  AlphaGo is actually very limited, Etzioni told The Register. “If I asked AlphaGo, can you play poker? It would say no. Well, can you at least talk about the position - you know analyse the game? And it would say no again. Okay, how did you feel about the game? If it could speak it’d say it didn’t understand the question.”  Just because AI can have superhuman intelligence in narrow tasks, it doesn’t mean that that can be transferred into a broader range of tasks.  “We tend to extrapolate outwards from narrow intelligence. But in fact, the field is moving incrementally - AlphaGo’s overnight success with algorithms was 30 years in the making,” Etzioni said.  “To paraphrase Churchill, deep learning is not the end. It's not the beginning of the end. It’s not even the end of the beginning - it’s still very early days.”  AI for the common good  Etzioni isn’t a complete party pooper, however. He says he does want to see the far-reaching achievements and fulfil the potential of AI, but believes researchers should approach the future realistically – lest they trigger a second ""AI winter"" – leading to devs losing their funding.  “One of the biggest crippling things of positive hype is its cyclical nature. Those of us that have been working in AI for more than 30 years. We’ve seen these high hopes, and then there’s these AI 'winters' - where people’s realise it was overhyped,” Etzioni told El Reg.  The field has experienced many hype cycles that lead to disappointment and funding cuts, before interest in AI is regenerated.  Another problem is that hype distracts people from the real issues. Etzioni agrees with Moshe Vardi, professor of computer science at Rice University, that researchers have a “moral imperative” to study AI in order to save lives.  A team of 65 people work at AI2 and believe in “AI for the common good”. Instead of focusing on when AI overlords will take over humanity, or the over ambitious timelines of achievement in projects, researchers should look at if and how AI can be used to solve society’s thorniest problems.  Driverless cars have the potential to reduce traffic accidents and AI in healthcare may just help researchers tackle diseases. Machine learning is already applied in medicine to diagnose tumours.  “The take-home message is that AI will not exterminate us; AI will empower us, ” said Etzioni.  The faster we realise that, the more we can focus on how AI can be a valuable tool for society. ®","fun, etzioni, say, end, games, hype, loses, intelligence, machine, researchers, learning, ai, deep",2016-09-28 00:00:00
59,14 notable AI startups from YC's Winter '22 batch,"Devin Coldewey, Frederic Lardinois, Brian Heater, Dominic-Madori Davis, Anthony Ha, Cody Corrall, Matt Rosoff, Connie Loizos, Writer, Lauren Forristal",https://techcrunch.com/2022/03/30/selected-ai-startups-from-ycs-winter-22-batch/,"Dozens of startups in Y Combinator’s Winter 2022 cohort do something that could be described as artificial intelligence. Here are 14 notable startups from the latest batch. Whitelab GenomicsFounded: 2019 in Paris, FranceBy: Julien Cottineau, David DelbourgoBuilding: An AI-powered platform for discovery and design of genomic therapies. Strong ComputeFounded: 2021 in Sydney, AustraliaBy: Ben SandBuilding: An ML training platform that focuses on ultra-high-performance and efficiency. It’s hard to pick winners at this stage, it will depend a lot on emerging video use cases.","Dozens of startups in Y Combinator’s Winter 2022 cohort do something that could be described as artificial intelligence. Though the term has lost much of its meaning, it’s still an important part of the tech landscape, and both using it and enabling it are fertile ground for new companies. Here are 14 notable startups from the latest batch.  Eventual  Founded: 2022 in San Francisco, CA  By: Jay Chia, Sammy Sidhu  Building: A turnkey data warehouse for images and video. Eventual ingests, organizes and processes imaging data all on one platform. Also handles queries, simple integration with cloud providers and smart scheduling to save costs on compute.  Quote: “Currently users jerry-rig one together with multiple vendors or hard to manage open-source frameworks. We are a one-stop shop.”  TC Quick Take™: Many companies require some kind of image analysis pipeline and making a one-stop shop is a valid approach to the market. Keeping customers that start to scale and squint at the costs could be a challenge, though.  Whitelab Genomics  Founded: 2019 in Paris, France  By: Julien Cottineau, David Delbourgo  Building: An AI-powered platform for discovery and design of genomic therapies. Data science and databasing for “scientific, technical, biological, genomic, and experimental” data sets. Plus suggestions for experimental design and workflows.  Quote: “We help our customers develop more genomic medicines, faster, in leaner ways and make those revolutionary drugs more accessible to patients in need.”  TC Quick Take™: AI drug discovery is hot, but few have created serious value. Still, it’s obviously promising and specializing in genomic therapy is a good idea. How they’ll do the woollier workflow and design stuff I don’t know.  AiSupervision  Founded: 2021 in Mannheim, Germany  By: Alex Conway, Sascha Lang  Building: An OS for production lines that records, digitizes and monitors factories with lots of workers. Includes real-time alerts, automated reports and photo-based quality control.  Quote: “We automate what the best supervisors would do if they watched everything that’s happening inside your factory.”  TC Quick Take™: If it’s anything like what Amazon is already doing, it’s a short ride to hell for the workers. But if they can make this humane and helpful, many factories would consider it as a way to integrate multiple tracking tools.  Powerhouse AI  Founded: 2021 in Singapore  By: Kushal Pillay and Ivo Verhaegh  Building: An automated inventory solution for warehouses. Lots of worker hours are used up doing this thankless task, so why not automate it? Powerhouse uses ordinary phones that count inventory using regular pictures taken by staff. It’s part of a broader effort to make warehouses smarter and more efficient.  Quote: “They just take a picture with their phone, and our software counts the boxes.”  TC Quick Take™: I thought I heard the presenter say 97% accuracy, but the site says 99%. I don’t do inventory these days but hopefully it can be trusted enough that it doesn’t require hand recounting.  Strong Compute  Founded: 2021 in Sydney, Australia  By: Ben Sand  Building: An ML training platform that focuses on ultra-high-performance and efficiency. By performing optimizations “around” a model (like pre-computing values, identifying bottlenecks and so on) they claim to be able to improve training times by orders of magnitude.  Quote: “The future of Cloud Computing, priced by performance not consumption.”  TC Quick Take™: Frederic looked into it a few weeks ago, and it seems like a nice upscale competitor that plays nicely with existing solutions.  Reality Defender  Founded: 2021 in New York  By: Gaurav Bharaj, Benjamin Colman and Ali Shahriyari  Building: An easy to use tool with which companies can scan media for deepfake content. Produces alerts, report cards and other ways to visualize and take action against fake content.  Quote: “Empower humanity to recognize the truth.”  TC Quick Take™: This will almost certainly be an arms race, but a tool for catching the low-hanging fruit will be wanted to prevent oceans of fake profiles, lightly disguised spam content and so on. Their methods may take ongoing refining though, which may not be cheap or easy.  KorrAI  Founded: 2020 in Canada  By: Rahul Anand and Rob McEwan  Building: A geospatial and satellite data platform for mining operations that pulls together multiple sources and offers a more efficient way to locate and organize mineral collection. Early cases have reduced lead times from a year to a few weeks.  Quote: “We’ll change mining more in the next five years than it has in the last 50.”  TC Quick Take™: We’ve seen a lot of movement in the mining area recently, probably a sign that this legacy industry is investing its considerable resources in modernization. It’s a promising space to get into right now.  Mutable AI  Founded: 2021 in Pittsburgh  By: Omar Shams  Building: A machine learning-powered polishing engine for Python code and Jupyter notebooks. These notebooks are often cobbled together as experiments with no plan for being production code — but then one gets adopted as a popular tool and needs to be cleaned up. Mutable AI does this automatically, autocompleting, refactoring and minimizing quickly and easily.  Quote: “Be kind to your teammates and future self.”  TC Quick Take™: Would it be useful? Probably… I’ve had to use hacked-together notebooks and wouldn’t want to do it full time. Is there a big market for it? Hard to say. The company seems born out of the founder’s own frustrations at the frontier of ML research, so maybe there are others like him looking for this.  Voize  Founded: 2020 in Berlin, Germany  By: Fabio and Marcel Schmidberger, Erik Ziegler  Building: Voice-powered filling out of forms for healthcare workers. These folks spend hours doing paperwork, and it’s a sea of fields that need to be tabbed through and typed in. Voize lets them speak directly into a smartphone and it creates structured records from that.  Quote: “The digital voice assistant for non-desk workers.”  TC Quick Take™: Speaking medical information out loud while walking around seems like a big no-no… I have a feeling there’s a reason it’s done the way it’s done. But streamlining medical recordkeeping is something that definitely needs to happen.  Mintlify  Founded: 2021 in San Francisco  By: Hahnbee Lee and Han Wang  Building: An automatic code documentation engine that reads your code and safely inserts comments that put it in context. Coders don’t always have time to do that and it makes code auditing and reuse difficult — so having a tool that does it for them (at least to a basic degree) could save time and effort.  Quote: “Everyone suffers from documentation debt.” Also, regarding co-founder Han Wang, “Knows over 100 cow jokes.” We’ll see about that.  TC Quick Take™: The proof is in the pudding for this one, but I bet a ton of companies would love to be able to say every line of code is explained. “Mintlify it before you send it to me next time!”  Speechly  Founded: 2016 in Helsinki, Finland  By: Hannes Heikinheimo and Otto Soderlund  Building: On-device speech recognition for apps that don’t want to have to call a cloud service. Doing any process locally is preferable, but until recently there was too much trade-off on the accuracy versus speed side. If Speechly can make quick and accurate speech recognition that easily fills in forms or summons relevant information, that’s one less online API call to make.  Quote: “Reduce cloud costs, enhance privacy and ensure zero-latency experiences by running Speechly directly on the end-users’ device.”  TC Quick Take™: Others are definitely pursuing this (including monsters like Google and Amazon), but having an independent and functional solution could be very valuable for app creators who want to keep their code as local as possible.  DynamoFL  Founded: 2021 in San Francisco  By: Christian Lau and Vaikkunth Mugunthan  Building: A plug and play federated learning platform for AI models in privacy-critical industries. Federated learning is an established technique and has already generated a lot of value, but it’s not the easiest thing to deploy. DynamoFL slips into training workflows that use private data and makes or keeps them private.  Quote: “In three minutes, we can plug our federated learning module into your existing ML workflow.”  TC Quick Take™: If it were easier to use federated learning, more niche industries in healthcare, finance and so on would probably be likely to deploy ML models. Whether they can keep the general ease of use that other ML tools have cultivated is a question mark.  Starling  Founded: 2019 in Houston, TX  By: Alex Arevalos, Drew Hendricks and Hannah McKenney  Building: AI-powered home urinalysis using an in-bathroom spectrometer. Using a usually lab-bound device for identifying and detecting substances, Starling aims to catch things like bladder infections and other problems early, or do home monitoring that would normally involve bringing a urine sample in.  Quote: “What information did you just flush down the toilet?”  TC Quick Take™: Depending on how easy it is to use, this could be a new standard tool for a lot of medical preventive stuff. I can see it being everywhere in two years.  Sieve Data  Founded: 2021 in Berkeley, CA  By: Abhinav Ayalur and Mokshith Voodarla  Building: A dead simple video analysis platform that scans incoming video for things like people, objects and situations and returns that metadata quickly. Powers search, storage, moderation — anything you need to understand video to do.  Quote: “You never have to train a model or manage a data set. Just push video and get results.”  TC Quick Take™: I just wrote up a company called Twelve Labs that does something like this and it seems like a very promising niche. It’s hard to pick winners at this stage, it will depend a lot on emerging video use cases.","lot, winter, 2021, code, batch, tool, 22, using, ml, ycs, quick, video, data, notable, platform, startups, ai, 14",2022-03-30 00:00:00
60,14 notable AI startups from YC's Winter '22 batch,"Devin Coldewey, Frederic Lardinois, Brian Heater, Dominic-Madori Davis, Anthony Ha, Cody Corrall, Matt Rosoff, Connie Loizos, Writer, Lauren Forristal",https://techcrunch.com/2022/03/30/selected-ai-startups-from-ycs-winter-22-batch/,"Dozens of startups in Y Combinator’s Winter 2022 cohort do something that could be described as artificial intelligence. Here are 14 notable startups from the latest batch. Whitelab GenomicsFounded: 2019 in Paris, FranceBy: Julien Cottineau, David DelbourgoBuilding: An AI-powered platform for discovery and design of genomic therapies. Strong ComputeFounded: 2021 in Sydney, AustraliaBy: Ben SandBuilding: An ML training platform that focuses on ultra-high-performance and efficiency. It’s hard to pick winners at this stage, it will depend a lot on emerging video use cases.","Dozens of startups in Y Combinator’s Winter 2022 cohort do something that could be described as artificial intelligence. Though the term has lost much of its meaning, it’s still an important part of the tech landscape, and both using it and enabling it are fertile ground for new companies. Here are 14 notable startups from the latest batch.  Eventual  Founded: 2022 in San Francisco, CA  By: Jay Chia, Sammy Sidhu  Building: A turnkey data warehouse for images and video. Eventual ingests, organizes and processes imaging data all on one platform. Also handles queries, simple integration with cloud providers and smart scheduling to save costs on compute.  Quote: “Currently users jerry-rig one together with multiple vendors or hard to manage open-source frameworks. We are a one-stop shop.”  TC Quick Take™: Many companies require some kind of image analysis pipeline and making a one-stop shop is a valid approach to the market. Keeping customers that start to scale and squint at the costs could be a challenge, though.  Whitelab Genomics  Founded: 2019 in Paris, France  By: Julien Cottineau, David Delbourgo  Building: An AI-powered platform for discovery and design of genomic therapies. Data science and databasing for “scientific, technical, biological, genomic, and experimental” data sets. Plus suggestions for experimental design and workflows.  Quote: “We help our customers develop more genomic medicines, faster, in leaner ways and make those revolutionary drugs more accessible to patients in need.”  TC Quick Take™: AI drug discovery is hot, but few have created serious value. Still, it’s obviously promising and specializing in genomic therapy is a good idea. How they’ll do the woollier workflow and design stuff I don’t know.  AiSupervision  Founded: 2021 in Mannheim, Germany  By: Alex Conway, Sascha Lang  Building: An OS for production lines that records, digitizes and monitors factories with lots of workers. Includes real-time alerts, automated reports and photo-based quality control.  Quote: “We automate what the best supervisors would do if they watched everything that’s happening inside your factory.”  TC Quick Take™: If it’s anything like what Amazon is already doing, it’s a short ride to hell for the workers. But if they can make this humane and helpful, many factories would consider it as a way to integrate multiple tracking tools.  Powerhouse AI  Founded: 2021 in Singapore  By: Kushal Pillay and Ivo Verhaegh  Building: An automated inventory solution for warehouses. Lots of worker hours are used up doing this thankless task, so why not automate it? Powerhouse uses ordinary phones that count inventory using regular pictures taken by staff. It’s part of a broader effort to make warehouses smarter and more efficient.  Quote: “They just take a picture with their phone, and our software counts the boxes.”  TC Quick Take™: I thought I heard the presenter say 97% accuracy, but the site says 99%. I don’t do inventory these days but hopefully it can be trusted enough that it doesn’t require hand recounting.  Strong Compute  Founded: 2021 in Sydney, Australia  By: Ben Sand  Building: An ML training platform that focuses on ultra-high-performance and efficiency. By performing optimizations “around” a model (like pre-computing values, identifying bottlenecks and so on) they claim to be able to improve training times by orders of magnitude.  Quote: “The future of Cloud Computing, priced by performance not consumption.”  TC Quick Take™: Frederic looked into it a few weeks ago, and it seems like a nice upscale competitor that plays nicely with existing solutions.  Reality Defender  Founded: 2021 in New York  By: Gaurav Bharaj, Benjamin Colman and Ali Shahriyari  Building: An easy to use tool with which companies can scan media for deepfake content. Produces alerts, report cards and other ways to visualize and take action against fake content.  Quote: “Empower humanity to recognize the truth.”  TC Quick Take™: This will almost certainly be an arms race, but a tool for catching the low-hanging fruit will be wanted to prevent oceans of fake profiles, lightly disguised spam content and so on. Their methods may take ongoing refining though, which may not be cheap or easy.  KorrAI  Founded: 2020 in Canada  By: Rahul Anand and Rob McEwan  Building: A geospatial and satellite data platform for mining operations that pulls together multiple sources and offers a more efficient way to locate and organize mineral collection. Early cases have reduced lead times from a year to a few weeks.  Quote: “We’ll change mining more in the next five years than it has in the last 50.”  TC Quick Take™: We’ve seen a lot of movement in the mining area recently, probably a sign that this legacy industry is investing its considerable resources in modernization. It’s a promising space to get into right now.  Mutable AI  Founded: 2021 in Pittsburgh  By: Omar Shams  Building: A machine learning-powered polishing engine for Python code and Jupyter notebooks. These notebooks are often cobbled together as experiments with no plan for being production code — but then one gets adopted as a popular tool and needs to be cleaned up. Mutable AI does this automatically, autocompleting, refactoring and minimizing quickly and easily.  Quote: “Be kind to your teammates and future self.”  TC Quick Take™: Would it be useful? Probably… I’ve had to use hacked-together notebooks and wouldn’t want to do it full time. Is there a big market for it? Hard to say. The company seems born out of the founder’s own frustrations at the frontier of ML research, so maybe there are others like him looking for this.  Voize  Founded: 2020 in Berlin, Germany  By: Fabio and Marcel Schmidberger, Erik Ziegler  Building: Voice-powered filling out of forms for healthcare workers. These folks spend hours doing paperwork, and it’s a sea of fields that need to be tabbed through and typed in. Voize lets them speak directly into a smartphone and it creates structured records from that.  Quote: “The digital voice assistant for non-desk workers.”  TC Quick Take™: Speaking medical information out loud while walking around seems like a big no-no… I have a feeling there’s a reason it’s done the way it’s done. But streamlining medical recordkeeping is something that definitely needs to happen.  Mintlify  Founded: 2021 in San Francisco  By: Hahnbee Lee and Han Wang  Building: An automatic code documentation engine that reads your code and safely inserts comments that put it in context. Coders don’t always have time to do that and it makes code auditing and reuse difficult — so having a tool that does it for them (at least to a basic degree) could save time and effort.  Quote: “Everyone suffers from documentation debt.” Also, regarding co-founder Han Wang, “Knows over 100 cow jokes.” We’ll see about that.  TC Quick Take™: The proof is in the pudding for this one, but I bet a ton of companies would love to be able to say every line of code is explained. “Mintlify it before you send it to me next time!”  Speechly  Founded: 2016 in Helsinki, Finland  By: Hannes Heikinheimo and Otto Soderlund  Building: On-device speech recognition for apps that don’t want to have to call a cloud service. Doing any process locally is preferable, but until recently there was too much trade-off on the accuracy versus speed side. If Speechly can make quick and accurate speech recognition that easily fills in forms or summons relevant information, that’s one less online API call to make.  Quote: “Reduce cloud costs, enhance privacy and ensure zero-latency experiences by running Speechly directly on the end-users’ device.”  TC Quick Take™: Others are definitely pursuing this (including monsters like Google and Amazon), but having an independent and functional solution could be very valuable for app creators who want to keep their code as local as possible.  DynamoFL  Founded: 2021 in San Francisco  By: Christian Lau and Vaikkunth Mugunthan  Building: A plug and play federated learning platform for AI models in privacy-critical industries. Federated learning is an established technique and has already generated a lot of value, but it’s not the easiest thing to deploy. DynamoFL slips into training workflows that use private data and makes or keeps them private.  Quote: “In three minutes, we can plug our federated learning module into your existing ML workflow.”  TC Quick Take™: If it were easier to use federated learning, more niche industries in healthcare, finance and so on would probably be likely to deploy ML models. Whether they can keep the general ease of use that other ML tools have cultivated is a question mark.  Starling  Founded: 2019 in Houston, TX  By: Alex Arevalos, Drew Hendricks and Hannah McKenney  Building: AI-powered home urinalysis using an in-bathroom spectrometer. Using a usually lab-bound device for identifying and detecting substances, Starling aims to catch things like bladder infections and other problems early, or do home monitoring that would normally involve bringing a urine sample in.  Quote: “What information did you just flush down the toilet?”  TC Quick Take™: Depending on how easy it is to use, this could be a new standard tool for a lot of medical preventive stuff. I can see it being everywhere in two years.  Sieve Data  Founded: 2021 in Berkeley, CA  By: Abhinav Ayalur and Mokshith Voodarla  Building: A dead simple video analysis platform that scans incoming video for things like people, objects and situations and returns that metadata quickly. Powers search, storage, moderation — anything you need to understand video to do.  Quote: “You never have to train a model or manage a data set. Just push video and get results.”  TC Quick Take™: I just wrote up a company called Twelve Labs that does something like this and it seems like a very promising niche. It’s hard to pick winners at this stage, it will depend a lot on emerging video use cases.","lot, winter, 2021, code, batch, tool, 22, using, ml, ycs, quick, video, data, notable, platform, startups, ai, 14",2022-03-30 00:00:00
61,,,https://pulselab.jhu.edu/wp-content/uploads/2017/01/SWE_Winter2017_p36-40-AI.Feature.JMD2_.AP-1.pdf,,,,
62,5 neat AI startups from Y Combinator’s Summer 2023 batch,"Kyle Wiggers, Dominic-Madori Davis, Connie Loizos, Cody Corrall, Matt Rosoff, Manish Singh, Senior Reporter, Anthony Ha, Maxwell Zeff, --C-Author-Card-Image-Size Align-Items Center Display Flex Gap Var",https://techcrunch.com/2023/09/06/5-neat-ai-startups-from-y-combinators-winter-2023-batch/,"And the voraciousness is manifesting in this summer’s Y Combinator cohort, which features over double (57 versus 28) the number of AI companies compared to the winter 2022 batch. AI infrastructure startupsSeveral startups in the Y Combinator cohort focus not on what AI can accomplish, but on the tools and infrastructure necessary to build AI from the ground up. Another intriguing Y Combinator startup tackling challenges in AI operations is Cerelyze, founded by ex-Peloton AI engineer Sarang Zambare. AI appsTransitioning away from the tooling subset of AI Y Combinator startups this year, we have Nowadays, which bills itself as the “AI co-pilot for corporate event planning.”Anna Sun and Amy Yan co-founded the company in early 2023. “FleetWorks helps freight operators focus on high-value work by automating routine calls and emails,” Singer wrote in a Y Combinator post.","It’s that time of year again: the week when startups in Y Combinator’s latest batch present their products for media — and investor — scrutiny. Over the next two days, roughly 217 companies will present in total, a tad smaller than last winter’s 235-firm cohort as VC enthusiasm suffered a slight slump.  In the first half of 2023, VCs backed close to 4,300 deals totaling $64.6 billion. That might sound like a lot. But the deal value represented a 49% decline from H1 2022 while the deal volume was a 35% dip year-over-year.  In a bright note, one segment — driven by equal parts hype and demand — is wildly outperforming the others: AI.  Nearly a fifth of total global venture funding from August to July came from the AI sector, according to CrunchBase. And the voraciousness is manifesting in this summer’s Y Combinator cohort, which features over double (57 versus 28) the number of AI companies compared to the winter 2022 batch.  To get a sense of which AI technologies are driving investments these days, I dove deep into the summer 2023 batch, rounding up the YC-backed AI startups that appeared to me to be the most differentiated — or hold the most promise.  AI infrastructure startups  Several startups in the Y Combinator cohort focus not on what AI can accomplish, but on the tools and infrastructure necessary to build AI from the ground up.  There’s Shadeform, for example, which provides a platform to enable customers to access and deploy AI training and inferencing workloads to any cloud provider. Founded by data engineers and distributed systems architects Ed Goode, Ronald Ding and Zachary Warren, Shadeform aims to ensure AI jobs run on time and at “optimal cost.”  As Goode notes in a blog post on the Y Combinator website, the explosion in demand for hardware to develop AI models, particularly GPUs, has resulted in a shortage of capacity. (Microsoft recently warned of service disruptions if it can’t get enough AI chips for its data centers.) Smaller providers are coming online, but they don’t always deliver the most predictable resources — making it difficult to scale across them.  Shadeform solves for this problem by letting customers launch AI jobs anywhere, across public cloud infrastructure. Leveraging the platform, companies can manage GPU instances on every provider from a single pane of glass, configuring “auto-reservations” when the machines they need are available or deploying into server clusters with a single click.  Another intriguing Y Combinator startup tackling challenges in AI operations is Cerelyze, founded by ex-Peloton AI engineer Sarang Zambare. Cerelyze is Zambare’s second YC go-around after leading the AI team at cashier-less retail startup Caper.  Cerelyze takes AI research papers — the kind typically found on open access archives like Arxiv.org — and translates the math contained within into functioning code. Why is that useful? Well, lots of papers describe AI techniques using formulas but don’t provide links to the code that was used to put them into practice. Developers are normally left having to reverse engineer the methods described in papers to build working models and apps from them.  Cerelyze seeks to automate implementation through a combination of AI models that understand language and code and PDF parsers “optimized for scientific content.” From a browser-based interface, users can upload a research paper, ask Cerelyze natural language questions about specific parts of the paper, generate or modify code and run the resulting code in the browser.  Now, Cerelyze can’t translate everything in a paper to code — at least not in its current state. Zambare acknowledges that the platform’s code translation only works for a “small subset of papers” right now and that Cerelyze can only extract and analyze equations and tables from papers, not figures. But I still think it’s a fascinating concept, and I’m hoping it’ll grow and improve with time — and the right investments.  AI dev tools  Still developer-focused but not an AI infrastructure startup per se, Sweep autonomously handles small dev tasks like high-level debugging and feature requests. The startup was launched this year by William Zeng and Kevin Lu, both veterans of the video-game-turned-social-network Roblox.  “As software engineers, we found ourselves switching from exciting technical challenges into mundane tasks like writing tests, documentation and refactors,” Zeng wrote on Y Combinator’s blog. “This was frustrating because we knew large language models [similar to OpenAI’s GPT-4] could handle this for us.”  Sweep can take a code error or GitHub issue and plan how to solve it, Zeng and Lu say — writing and pushing the code to GitHub via pull requests. It can also address comments made on the pull request either from code maintainers or owners — a bit like GitHub Copilot but more autonomous.  “Sweep started when we realized some software engineering tasks were so simple we could automate the entire change,” Zeng said. “Sweep does this by writing the entire project request with code.”  Given AI’s tendency to make mistakes, I’m a little skeptical of Sweep’s reliability over the long run. Fortunately, so are Zeng and Lu — Sweep doesn’t automatically implement code fixes by default, requiring a human review and edit them before they’re pushed to the master codebase.  AI apps  Transitioning away from the tooling subset of AI Y Combinator startups this year, we have Nowadays, which bills itself as the “AI co-pilot for corporate event planning.”  Anna Sun and Amy Yan co-founded the company in early 2023. Sun was previously at Datadog, DoorDash and Amazon while Yan held various roles at Google, Meta and McKinsey.  Not many of us have had to plan a corporate event — certainly not this reporter. But Sun and Yan describe the ordeal as arduous, needlessly tiring and expensive.  “Corporate event planners are bombarded with endless calls and emails while planning events,” Sun writes in a Y Combinator blog post. “Stressing over tight schedules, planners are paying for full-time assistants or tools that cost them over $100,000 a year.”  So, Sun and Yan thought, why not offload the most painful parts of the process to AI?  Enter Nowadays, which — provided the details of an upcoming event (e.g. dates and the number of attendees) — can automatically reach out to venues and vendors and manage relevant emails and phone calls. Nowadays can even account for personal preferences around events, like amenities near a given venue and activities within walking distance.  I should note that it isn’t entirely clear how Nowadays works behind the scenes. Is AI actually answering and placing phone calls and returning emails? Or are humans involved somewhere along the way — say for quality assurance? Your guess is as good as mine.  Nevertheless, Nowadays is a very cool idea with a potentially huge addressable market ($510.9 billion by 2030, according to Allied Market Research), and I’m curious to see where it goes next.  Another startup trying to abstract away traditionally manual processes is FleetWorks, the brainchild of ex-Uber Freight product manager Paul Singer and Quang Tran, who formerly worked on moonshot projects at Airbnb.  FleetWorks targets freight brokers — the essential middlemen between freight shippers and carriers. Designed to sit alongside a broker’s phone, email and transportation management system (TMS), FleetWorks can automatically book and track loads and schedule appointments with shipping facilities that lack a booking portal.  Typically, brokers have to reach out via phone or email to drivers and dispatchers for loads that aren’t being tracked automatically for updates on shipment statuses. Simultaneously, they have to juggle calls from trucking companies interested in booking loads and negotiate on the price, as well as set appointment times for unscheduled loads.  Singer and Tran claim that FleetWorks can lighten the load (no pun intended) by triggering calls and emails and pushing all the relevant information to the TMS or email. In addition to sharing load details, the platform can discuss price and book a carrier, even calling a driver and updating account teams on issues that crop up.  “FleetWorks helps freight operators focus on high-value work by automating routine calls and emails,” Singer wrote in a Y Combinator post. “Our AI-powered platform can leverage email or use a human-like voice to make tracking calls, cover loads, and reschedule appointments.”  If it works as advertised, that sounds genuinely useful.","combinators, papers, y, code, startup, nowadays, batch, combinator, zeng, 2023, summer, emails, neat, startups, ai, calls",2023-09-06 00:00:00
63,Focusing our strategy to weather crypto winter,,https://protocol.ai/blog/2023-02-03-crypto-winter-update/,"The macro winter worsened crypto winter, making it more extreme and potentially longer than our industry expected. To weather the macro and crypto winter, PL teams have significantly cut costs over the last quarters, reducing team budgets, infra spend, investments, grant programs, and much more. To ensure teams are well positioned to weather this extended winter, we have to reduce costs further. This impacts individuals across PLGO teams (PL Corp, PL Member Services, Network Goods, PL Outercore, and PL Starfleet). PL Network Opportunities.","Dear Labbers,  I’m saddened to share that today we must part ways with a number of valued friends and colleagues at PL.  Winter. As you know, this has been an extremely challenging economic downturn, world-wide and especially in crypto. High inflation leading to high interest rates, low investment, and tougher markets have rocked companies and industries globally. The macro winter worsened crypto winter, making it more extreme and potentially longer than our industry expected. Millions of companies around the world have had to adapt to this dynamic landscape, and prepare for a longer downturn. No company is unaffected.  Cost Reductions. To weather the macro and crypto winter, PL teams have significantly cut costs over the last quarters, reducing team budgets, infra spend, investments, grant programs, and much more. These have been tough times and our teams have succeeded greatly in reducing costs while maintaining services and growing projects. Thank you to everyone who contributed to these efforts, you have made a big difference.  PLGO Layoffs. To ensure teams are well positioned to weather this extended winter, we have to reduce costs further. Although we worked extremely hard to avoid this, we’ve made the difficult decision to reduce our workforce by 89 roles (approximately 21%). This impacts individuals across PLGO teams (PL Corp, PL Member Services, Network Goods, PL Outercore, and PL Starfleet). We’ve had to focus our headcount against the most impactful and business critical efforts.  Process. We announced this at an All Hands this morning. We’ve sent an email and calendar invite to impacted individuals to discuss this transition face-to-face and offer our support. Here, departing labbers, can have their specific questions answered regarding their departure, and walk through an enhanced version of our standard severance package. The PL Talent team will offer placement services to help find opportunities. We aim to be as thorough and supportive as we possibly can.  PL Network Opportunities. There are many other teams hiring across the PL Network and adjacent communities. We will do everything we can to connect those affected with new roles & opportunities in our network and beyond. Our Talent team will also offer office hours to help polish resumes, surface relevant roles, and provide recommendations.  Extraordinary People. The teams we’ve built over the past few years are full of tremendously talented, hard working, and passionate people who have been integral to all our breakthroughs and accomplishments. We’ve achieved a tremendous amount in the past several years - from Filecoin launch; to scaling IPFS to millions of users; building one of the fastest growing developer ecosystems; supporting 300+ companies across the network; growing movements like SBS and FTC; launching testnets for FVM, Saturn, SpaceNet, and Bacalhau just last quarter; and much more. We are immensely proud of everyone’s impact, and wish we could continue working with each and every one of you. It is a huge loss to part with some of our teammates today. We will always be grateful for your effort and impact on our mission. Thank you. You will achieve great things wherever you go, and our network is here to support you in your next endeavors.  Focus on the Mission. We know these changes will be tough for all Labbers, so we’ll be organizing a PLGO All Hands on Monday to answer any remaining questions, and to look ahead. PL exists to drive breakthroughs in computing to push humanity forward (opens new window). We are a network of optimistic teams, organizations, and people, who believe the largest problems ailing humanity can and must be solved. Our work is improving critical internet infrastructure, creating open networks that put the power in people’s hands, and developing a prolific innovation ecosystem. We already empower tens of millions of people world-wide with more resilient networks, and we’re on a path to reach billions. We have been tremendously successful and fortunate over the years. Sometimes we face tough setbacks, but we forge ahead because our work matters.  Thank you,  Juan Benet","pl, opportunities, focusing, winter, network, services, team, teams, crypto, weather, roles, strategy, weve, tough",2023-02-03 00:00:00
64,The 18 most interesting startups from YC’s Demo Day show we’re in an AI bubble,"Dominic-Madori Davis, Tim De Chant, Rebecca Szkutak, Marina Temkin, Kyle Wiggers, Alex Wilhelm, Senior Reporter, Reporter, Brian Heater, Anthony Ha",https://techcrunch.com/2024/04/03/ycs-winter-2024-demo-day-confirms-that-we-are-indeed-in-an-ai-bubble/,"AI was, not shockingly, the biggest theme, with 86 out of 247 companies calling themselves an AI startup, but we’re reaching bubble territory given that 187 mention AI in their pitches. TechCrunch’s staff favoritesWhat it does: Uses AI to help companies find and apply for grantsUses AI to help companies find and apply for grants Why it’s a fave: Landing grants isn’t easy. So why not have AI help with it? So why not have AI help with it? Pump’s decision to monetize through AWS, not the small companies themselves, is smart and makes it much more likely it could generate strong traction.","Springtime means rain, the return of flowers and, of course, Y Combinator’s first demo day of the year. During the well-known accelerator’s first of two pitch days from the Winter 2024 cohort, a covey of TechCrunch staff tuned in, took notes, traded jokes and slowly whittled away at the dozens of presenting companies to come up with a list of early favorites.  AI was, not shockingly, the biggest theme, with 86 out of 247 companies calling themselves an AI startup, but we’re reaching bubble territory given that 187 mention AI in their pitches.  From AI-generated music and grant applications to neat new fintech applications and even some health tech work, there was something for everyone. We’re back at it Thursday for the second day of pitches. Until then, if you didn’t get to watch live, here’s a rundown of some of the best from day one.  TechCrunch’s staff favorites  What it does: Uses AI to help companies find and apply for grants  Uses AI to help companies find and apply for grants Why it’s a fave: Landing grants isn’t easy. Max Williamson, Peter Crocker and Greg Miller know this well: They’ve worked between them at The Rockefeller Foundation and the U.S. Department of Housing and Urban Development, where grants are common currency. Finding and applying for grants involves sifting through mounds of paperwork and submitting countless forms — an expensive and time-consuming process. So why not have AI help with it? That’s the idea behind their startup Aidy, which is focused exclusively on Rural Energy for America Program grants for now. After asking a few questions, Aidy evaluates an organization’s competitiveness for grants by navigating eligibility requirements and scoring criteria, then takes a first pass at filling out any relevant forms. Aidy is clearly in the proof-of-concept stage, judging by the state of its tooling. But the concept’s an interesting one — assuming the platform’s AI doesn’t make too many mistakes.  Landing grants isn’t easy. Max Williamson, Peter Crocker and Greg Miller know this well: They’ve worked between them at The Rockefeller Foundation and the U.S. Department of Housing and Urban Development, where grants are common currency. Finding and applying for grants involves sifting through mounds of paperwork and submitting countless forms — an expensive and time-consuming process. So why not have AI help with it? That’s the idea behind their startup Aidy, which is focused exclusively on Rural Energy for America Program grants for now. After asking a few questions, Aidy evaluates an organization’s competitiveness for grants by navigating eligibility requirements and scoring criteria, then takes a first pass at filling out any relevant forms. Aidy is clearly in the proof-of-concept stage, judging by the state of its tooling. But the concept’s an interesting one — assuming the platform’s AI doesn’t make too many mistakes. Who picked it: Kyle  What it does: Serves as a banking platform for nonprofits  Serves as a banking platform for nonprofits Why it’s a fave: If you’re in the nonprofit space, compliance and regulatory requirements force you to do finances a little differently. That’s where Givefront comes in. Co-founded by Ethan Sayre and Matt Tengtrakool, who previously launched a startup to help loan-takers based in Nigeria, Givefront offers banking, spend management and financial governance services for nonprofits. Specifically, Givefront provides accounts to nonprofits to store money and integrate donations, payments and reimbursements, as well as features for automatic reporting and annual regulatory filings. Givefront certainly isn’t the only nonprofit banking option out there. But it appears to be one of the first built from the ground up for this purpose — which certainly has its own appeal.  If you’re in the nonprofit space, compliance and regulatory requirements force you to do finances a little differently. That’s where Givefront comes in. Co-founded by Ethan Sayre and Matt Tengtrakool, who previously launched a startup to help loan-takers based in Nigeria, Givefront offers banking, spend management and financial governance services for nonprofits. Specifically, Givefront provides accounts to nonprofits to store money and integrate donations, payments and reimbursements, as well as features for automatic reporting and annual regulatory filings. Givefront certainly isn’t the only nonprofit banking option out there. But it appears to be one of the first built from the ground up for this purpose — which certainly has its own appeal. Who picked it: Kyle  What it does: Software that links databases and large language models  Software that links databases and large language models Why it’s a fave: There’s a lot of attention in the market on companies that make large language models — the bigger, the faster, the smarter; you get the idea. But when it comes to actually deploying modern AL models inside of a company, you run into data issues. For example, Skyflow, one startup I covered recently, is working to keep sensitive information out of the wrong users of LLMs. Buster was eye-catching because it appears to be working on a problem that a whole mess of companies are going to run into. Sure, new models are cool, but selling software picks and shovels during the AI gold rush is probably a darn good business model. I dig it!  There’s a lot of attention in the market on companies that make large language models — the bigger, the faster, the smarter; you get the idea. But when it comes to actually deploying modern AL models inside of a company, you run into data issues. For example, Skyflow, one startup I covered recently, is working to keep sensitive information out of the wrong users of LLMs. Buster was eye-catching because it appears to be working on a problem that a whole mess of companies are going to run into. Sure, new models are cool, but selling software picks and shovels during the AI gold rush is probably a darn good business model. I dig it! Who picked it: Alex  What it does: Banking services for contractors in emerging markets  Banking services for contractors in emerging markets Why it’s a fave: Creating better payroll solutions for remote and international workers isn’t new, but Numo’s approach of focusing on contractors in emerging markets specifically stands out. It’s also smart that Numo is building a banking product on top of its payroll system so that these contractors, many of whom would be based in countries with currencies that fluctuate frequently, have a more secure place to store their earned funds.  Creating better payroll solutions for remote and international workers isn’t new, but Numo’s approach of focusing on contractors in emerging markets specifically stands out. It’s also smart that Numo is building a banking product on top of its payroll system so that these contractors, many of whom would be based in countries with currencies that fluctuate frequently, have a more secure place to store their earned funds. Who picked it: Becca  What it does: Uses AI to help consumer packaged goods brands aggregate retail fees and dispute invalid ones  Uses AI to help consumer packaged goods brands aggregate retail fees and dispute invalid ones Why it’s a fave: Many CPG brands, especially emerging ones, have very small margins that are squeezed by numerous fees that cover shelving, packing incorrect quantities and shipping damaged products. Intercept says that spotting and flagging invalid fees could give CPG brands back an average of 15% of their revenue that would have otherwise been spent on inaccurate fees. This seems like a problem worth solving.  Many CPG brands, especially emerging ones, have very small margins that are squeezed by numerous fees that cover shelving, packing incorrect quantities and shipping damaged products. Intercept says that spotting and flagging invalid fees could give CPG brands back an average of 15% of their revenue that would have otherwise been spent on inaccurate fees. This seems like a problem worth solving. Who picked it: Becca  What it does: Helps detect deep fakes and misinformation  Helps detect deep fakes and misinformation Why it’s a fave: I’m curious about any technology that seeks to find ways to parse through the inevitable rise of deep fakes and misinformation we are already encountering. Artificial intelligence is becoming more sophisticated by the hour, and we are about to enter a world where right, wrong, fact and fiction have already started to get blurry. Deep fakes are of particular concern for women, as seen by what happened to Taylor Swift — and with slow government regulation in this space, I welcome any research and technology focused on trying to address our ever-increasing cybersecurity needs.  I’m curious about any technology that seeks to find ways to parse through the inevitable rise of deep fakes and misinformation we are already encountering. Artificial intelligence is becoming more sophisticated by the hour, and we are about to enter a world where right, wrong, fact and fiction have already started to get blurry. Deep fakes are of particular concern for women, as seen by what happened to Taylor Swift — and with slow government regulation in this space, I welcome any research and technology focused on trying to address our ever-increasing cybersecurity needs. Who picked it: Dom  What it does: Custom LLM evaluation  Custom LLM evaluation Why it’s a fave: One of my favorite things to read through when a new, major LLM comes to market is its benchmark stats. For example, Anthropic’s Claude 3 Opus model has a 50.4% 0-shot CoT in “Graduate level reasoning, GPQA, Diamond.” It’s super clarifying stuff. Kidding aside, it’s not. That’s why I like the idea that Vectorview is working on, namely the ability to test LLMs and AI agents for a company’s particular use case. I suspect that by having its testing tools closer to the end user than the academic side of things, Vectorview could be onto something big.  One of my favorite things to read through when a new, major LLM comes to market is its benchmark stats. For example, Anthropic’s Claude 3 Opus model has a 50.4% 0-shot CoT in “Graduate level reasoning, GPQA, Diamond.” It’s super clarifying stuff. Kidding aside, it’s not. That’s why I like the idea that Vectorview is working on, namely the ability to test LLMs and AI agents for a company’s particular use case. I suspect that by having its testing tools closer to the end user than the academic side of things, Vectorview could be onto something big. Who picked it: Alex  What it does: Uses AI to help lawyers go through legal documents quicker  Uses AI to help lawyers go through legal documents quicker Why it’s a fave: Abel co-founder Sean Safahi said that this eliminates the need for lawyers to choose “depth over breadth.” I think any tech that helps lawyers make more informed arguments and decisions is a good thing. Speeding up the legal process and making it more accurate seems like a solid strategy. It’s worth noting that bringing AI and automation into the legal process does add a layer of privacy risk and users of Abel will have tread carefully.  Abel co-founder Sean Safahi said that this eliminates the need for lawyers to choose “depth over breadth.” I think any tech that helps lawyers make more informed arguments and decisions is a good thing. Speeding up the legal process and making it more accurate seems like a solid strategy. It’s worth noting that bringing AI and automation into the legal process does add a layer of privacy risk and users of Abel will have tread carefully. Who picked it: Becca  What they do: AI-powered music generation  AI-powered music generation Why they’re faves: Soundry AI’s technology could be incredibly useful to create music that sits neatly in the background. Muzak, elevator tunes, corporate learning soundtracks, whatever they play in loud restaurants that you can never quite make out, but might be a song that you know. It’s a big market, and I can see companies tuning their own mixes to get the right vibe. Then there’s Sonauto, a startup that wants to help you make hits. I am more skeptical here, mostly because the music I love the most takes a lot of humans working super hard to push the boundaries of what music can be. The latest Tesseract record is a good example. Goddamn, what an incredible piece of art. That said, I am open to being wrong here, and that the robots will eventually write better progressive metal and pop and experimental jazz than we humble meatsacks can. I love music, I love tech, so I presume that I am going to eventually love their union. (Though I also have copyright worries here regarding source material, I must add as I am no fun.)  Soundry AI’s technology could be incredibly useful to create music that sits neatly in the background. Muzak, elevator tunes, corporate learning soundtracks, whatever they play in loud restaurants that you can never quite make out, but might be a song that you know. It’s a big market, and I can see companies tuning their own mixes to get the right vibe. Then there’s Sonauto, a startup that wants to help you make hits. I am more skeptical here, mostly because the music I love the most takes a lot of humans working super hard to push the boundaries of what music can be. The latest Tesseract record is a good example. Goddamn, what an incredible piece of art. That said, I am open to being wrong here, and that the robots will eventually write better progressive metal and pop and experimental jazz than we humble meatsacks can. I love music, I love tech, so I presume that I am going to eventually love their union. (Though I also have copyright worries here regarding source material, I must add as I am no fun.) Who picked it: Alex  What it does: EV chargers and management software for apartments, condos and commercial buildings  EV chargers and management software for apartments, condos and commercial buildings Why it’s a fave: Most EV charging happens at home, unless you live in a multifamily building, where infrastructure can be scant and forcing drivers to find power elsewhere. That’s not only a headache for drivers, it’s unrealized revenue for building owners. Starlight Charging centralizes key parts of the infrastructure to keep costs down. “Since our installation costs are so low, we can actually offer our solution for no upfront cost and still make money,” founder Andrew Kouri said. “Our payback period is less than one year. The company seems to be sweating the small stuff, too, offering its own charging equipment that adheres to the Plug & Charge standard for payments and comes with a removable cable that’s easy to swap in case of damage or vandalism. That should help with maintenance, something that’s tripped up many other EV charging networks.  Most EV charging happens at home, unless you live in a multifamily building, where infrastructure can be scant and forcing drivers to find power elsewhere. That’s not only a headache for drivers, it’s unrealized revenue for building owners. Starlight Charging centralizes key parts of the infrastructure to keep costs down. “Since our installation costs are so low, we can actually offer our solution for no upfront cost and still make money,” founder Andrew Kouri said. “Our payback period is less than one year. The company seems to be sweating the small stuff, too, offering its own charging equipment that adheres to the Plug & Charge standard for payments and comes with a removable cable that’s easy to swap in case of damage or vandalism. That should help with maintenance, something that’s tripped up many other EV charging networks. Who picked it: Tim  What it does: Online video creation and hosting for AI-generated clips  Online video creation and hosting for AI-generated clips Why it’s a fave: I muted the Demo Day stream to give this a try — you can check out my creation here — because one thing I am constantly bummed out by is the dearth of new sci-fi films for me to watch late at night. We need more! So, video creation tools that lean on user prompts are super interesting to me. Mix in the fact that AI-generated stuff might not find a permanent home on mainstream video platforms (brand safety, copyright concerns, the list goes on), Eggnog could be onto something. Still, while my little video clip was neat, it is about as close to a feature film as my doodles are to the best animated series out there.  I muted the Demo Day stream to give this a try — you can check out my creation here — because one thing I am constantly bummed out by is the dearth of new sci-fi films for me to watch late at night. We need more! So, video creation tools that lean on user prompts are super interesting to me. Mix in the fact that AI-generated stuff might not find a permanent home on mainstream video platforms (brand safety, copyright concerns, the list goes on), Eggnog could be onto something. Still, while my little video clip was neat, it is about as close to a feature film as my doodles are to the best animated series out there. Who picked it: Alex  What it does: Bundles small businesses so they can save on AWS  Bundles small businesses so they can save on AWS Why it’s a fave: This is a great approach to help small and emerging companies get the cloud services they need without having to spend a significant portion of their capital on software. Pump’s decision to monetize through AWS, not the small companies themselves, is smart and makes it much more likely it could generate strong traction. It’s easy to get excited about a company called the “Costco of cloud compute.”  This is a great approach to help small and emerging companies get the cloud services they need without having to spend a significant portion of their capital on software. Pump’s decision to monetize through AWS, not the small companies themselves, is smart and makes it much more likely it could generate strong traction. It’s easy to get excited about a company called the “Costco of cloud compute.” Who picked it: Becca  What it does: Seeks to organize screenshots  Seeks to organize screenshots Why it’s a fave: It’s a favorite because I have, like, 13,000 photos on my phone, most of which are screenshots. And when I need to find a screenshot, I’m stuck searching through the abyss of my phone’s library. Having something that helps group these photos could be a lifesaver that allows me to attend to the important tasks, like sending out timely memes to the group chat. The founder billed this as Pinterest for screenshots, which also grabbed me as I am an avid Pinterest user. Anything that makes photo grouping and sharing easier and fun is a product I’m bound to use.  It’s a favorite because I have, like, 13,000 photos on my phone, most of which are screenshots. And when I need to find a screenshot, I’m stuck searching through the abyss of my phone’s library. Having something that helps group these photos could be a lifesaver that allows me to attend to the important tasks, like sending out timely memes to the group chat. The founder billed this as Pinterest for screenshots, which also grabbed me as I am an avid Pinterest user. Anything that makes photo grouping and sharing easier and fun is a product I’m bound to use. Who picked it: Dom  What it does: Uses AI to help self-funded companies save 7% on health insurance  Uses AI to help self-funded companies save 7% on health insurance Why it’s a fave: Health insurance costs are skyrocketing. Large corporations can “eat” the fees, but absorbing the high cost is much harder for small and medium-sized businesses. SMBs are often forced to pass a large part of what they pay to their employees. Seven percent may not feel like a lot, but since health insurance can cost thousands of dollars a year, the savings could be meaningful for a small business or startup.  Health insurance costs are skyrocketing. Large corporations can “eat” the fees, but absorbing the high cost is much harder for small and medium-sized businesses. SMBs are often forced to pass a large part of what they pay to their employees. Seven percent may not feel like a lot, but since health insurance can cost thousands of dollars a year, the savings could be meaningful for a small business or startup. Who picked it: Marina  What it does: Aggregates spot freight  Aggregates spot freight Why it’s a fave: The founders’ discovered demand for spot freight technology building a similar solution at Convoy and noted it was the only profitable part of the shuttered company that was snapped up by Flexport. Manifold Freight is focusing on companies that have 50 or more trucks, which means they are targeting a customer base that other freight software is overlooking. Plus, targeting larger carriers means their customers likely have more funds to spend on new technology.  The founders’ discovered demand for spot freight technology building a similar solution at Convoy and noted it was the only profitable part of the shuttered company that was snapped up by Flexport. Manifold Freight is focusing on companies that have 50 or more trucks, which means they are targeting a customer base that other freight software is overlooking. Plus, targeting larger carriers means their customers likely have more funds to spend on new technology. Who picked it: Becca  What it does: Personalized teaching assistant that combines human tutors with AI  Personalized teaching assistant that combines human tutors with AI Why it’s a fave: I liked this because unlike other learning assistants, Shepherd works with academic institutions. This means the startup is not only authorized to tutor students, it also knows exactly what material needs to be learned. Shepherd also claims that it can help plan and manage students’ time. I would have liked to have had this when I was in college. It wasn’t always clear which learning task would be most challenging, and that ate up a lot of valuable time. Some of the countless hours I wasted learning to write code and get the program to work could have been better allocated to calculus, which wasn’t easy either.  I liked this because unlike other learning assistants, Shepherd works with academic institutions. This means the startup is not only authorized to tutor students, it also knows exactly what material needs to be learned. Shepherd also claims that it can help plan and manage students’ time. I would have liked to have had this when I was in college. It wasn’t always clear which learning task would be most challenging, and that ate up a lot of valuable time. Some of the countless hours I wasted learning to write code and get the program to work could have been better allocated to calculus, which wasn’t easy either. Who picked it: Marina","does, fave, interesting, 18, companies, thats, small, startups, ycs, picked, bubble, help, music, day, demo, ai, startup",2024-04-03 00:00:00
65,Are you a robot?,,https://www.bloomberg.com/news/articles/2024-07-26/great-rotation-trade-turns-spotlight-on-less-obvious-ai-winners,Why did this happen? Please make sure your browser supports JavaScript and cookies and that you are not blocking them from loading. For more information you can review our Terms of Service and Cookie Policy.,Why did this happen?  Please make sure your browser supports JavaScript and cookies and that you are not blocking them from loading. For more information you can review our Terms of Service and Cookie Policy.,"sure, supports, policy, robot, information, service, happenplease, loading, javascript, review, terms",2024-07-26 00:00:00
66,Who’s actually getting rich off of AI?,Casey Newton,https://www.theverge.com/23623495/ai-profits-winners-losers-openai-notion-snapchat,"Last week the productivity startup Notion announced that Notion AI, a suite of tools based on OpenAI’s ChatGPT, had entered general availability. Notion AI is among the first in a wave of companies that are racing to capitalize on growing interest in generative AI. So what kinds of AI features are people actually selling? Image: SnapThere should also be real value in more personalized AI models. What happens to Notion when its full suite of premium AI tools is offered for no cost within Google Docs?","Today, let’s talk about a subject that crosses my mind with every generative-AI startup pitch that lands in my inbox: who’s going to make the real money off artificial intelligence?  Last week the productivity startup Notion announced that Notion AI, a suite of tools based on OpenAI’s ChatGPT, had entered general availability. For $10 per user per month, Notion can now summarize meeting notes, generate lists of pros and cons, and draft emails.  Notion AI is among the first in a wave of companies that are racing to capitalize on growing interest in generative AI. This week Snapchat made available a ChatGPT-based chatbot called My AI for subscribers to its $4-a-month Snapchat Plus offering. The educational app Quizlet announced a ChatGPT-based tutor called Q-Chat. And Instacart said it is developing a tool that that will let customers ask about food and get ‘shoppable’ answers informed by product data from the company’s retail partners.”  The subject is clearly on developer’s minds  What I’m interested in, as more and more companies adopt features like this, is where the ultimate value lies. Will an ever-growing number of companies find ways to integrate AI into products that are valuable enough to charge for — or will the bulk of the profits go to the small number of companies building and refining the underlying models on which those tools are based?  The answer will go a long way in determining whether generative AI represents a true platform shift on the order of the move from desktop computers to mobile phones, or a more limited set of innovations whose benefits accrue to a handful of big winners.  The subject is clearly on developer’s minds. This week, in response to concerns, OpenAI said it would no longer use developers’ data to improve its models without their permission. Instead, it would ask developers to opt in.  “One of our biggest focuses has been figuring out, how do we become super friendly to developers?” Greg Brockman, OpenAI’s president and chairman, told TechCrunch. “Our mission is to really build a platform that others are able to build businesses on top of.”  Maybe it’s that simple — developers don’t want to help OpenAI refine its models for free, and OpenAI has decided to respect their wishes. This explanation feels more consistent with a world where AI really does represent a platform shift.  Or maybe OpenAI believes that its models can continue to improve rapidly with or without all of those developers opting in. This explanation feels more consistent with a world in which OpenAI and a handful of others reap most of the rewards of AI.  So what kinds of AI features are people actually selling?  Notion’s AI turns your prompt into blog posts, poems, to-do lists, and brainstorms. Image: Notion / David Pierce  For the moment, AI products on the market are essentially just white-labeled versions of ChatGPT. As of this week, OpenAI is making it available to other companies through an API. For $0.002 for about 750 words of output, any company can resell ChatGPT in their own app.  For the moment, then, there’s not really much consumer choice when it comes to generative AI. To the extent that there are multiple options, it’s in interfaces. Do you want to draft an email using AI? It might be more convenient in Notion, where you already have some meeting notes. Do you want to get some recipe ideas or ask a quick trivia question? If you’re away from your laptop, it might be fastest to ask My AI on Snapchat.  For the moment, there are still billions of people who have never used ChatGPT. Introducing a re-skinned version of that service to a popular consumer app like Snapchat, which has 750 million monthly users, could help it find a whole new audience. Paying Notion or Snapchat for the feature also guarantees access to a service that has often gone offline amid heavy usage of its own web app.  “It feels like electricity.”  At the same time, services like this are largely just making a bet against copy-paste. You can already get essentially everything here for free inside ChatGPT; apps like Notion and Snapchat are selling what feel like a fairly minor convenience for a significant premium.  The promise is that these tools will become more personalized over time, as individual apps refine the base models that they rent from OpenAI with data that we supply them. Every link that has ever been in Platformer is stored in a Notion database; what if I could simply ask research questions of the links I have stored there?  Those sorts of features are coming, Notion CEO Ivan Zhao told me in a recent interview. The initial set of writing and editing tools represent a “baby step,” he said. But much more profound changes are coming.  “I’ve never been this excited about something,” said Zhao, who is not prone to hyperbole. “It feels like electricity: the large language model is the electricity, and this is the first light-bulb use case. But there are many other appliances.”  Zhao told me that Notion would add lasting value to the language model by creating interfaces that are easier to use than rivals’. This is a more important project than it may first appear: one reason voice interfaces like Siri and Alexa have plateaued is that users struggle to remember all the different things they can be used for. (Another is that their language models aren’t nearly as sophisticated as ChatGPT’s.)  “This is as much an interface revolution as a technology revolution,” Zhao told me. “And we’re good at interfaces.”  A screenshot of Snap’s My AI chatbot. Image: Snap  There should also be real value in more personalized AI models. I’ve spent the past year or so writing a daily journal in an app called Mem, which offers its own set of ChatGPT-based features to premium subscribers. Eventually, I imagine I’ll be able to ask my journal all sorts of questions in natural language. What was I worried about last summer? When’s the last time I saw my friend Brian? A journal that gets good at that sort of thing could command a premium price, I think.  Still, there’s probably a limit to how many add-on AI subscriptions most people will want to pay for. Over time, the cost for these features seems likely to come down, and many of the services that cost $10 a month today may someday be offered for free.  But that, too, poses a risk for startups betting on features like these to drive growth. The cheaper the services get, the more likely they are to be offered for free by big platforms. What happens to Notion when its full suite of premium AI tools is offered for no cost within Google Docs?  The best-case scenario for these companies, I think, is that generative AI comes to resemble the cloud-computing market. The basic infrastructure will be built by a handful of companies, but the capabilities they make available cheaply inspires an entire new generation of startups.","notion, ask, models, rich, companies, actually, features, developers, getting, openai, snapchat, tools, whos, ai",2023-03-03 14:30:00+00:00
67,Who’s actually getting rich off of AI?,Casey Newton,https://www.theverge.com/23623495/ai-profits-winners-losers-openai-notion-snapchat,"Last week the productivity startup Notion announced that Notion AI, a suite of tools based on OpenAI’s ChatGPT, had entered general availability. Notion AI is among the first in a wave of companies that are racing to capitalize on growing interest in generative AI. So what kinds of AI features are people actually selling? Image: SnapThere should also be real value in more personalized AI models. What happens to Notion when its full suite of premium AI tools is offered for no cost within Google Docs?","Today, let’s talk about a subject that crosses my mind with every generative-AI startup pitch that lands in my inbox: who’s going to make the real money off artificial intelligence?  Last week the productivity startup Notion announced that Notion AI, a suite of tools based on OpenAI’s ChatGPT, had entered general availability. For $10 per user per month, Notion can now summarize meeting notes, generate lists of pros and cons, and draft emails.  Notion AI is among the first in a wave of companies that are racing to capitalize on growing interest in generative AI. This week Snapchat made available a ChatGPT-based chatbot called My AI for subscribers to its $4-a-month Snapchat Plus offering. The educational app Quizlet announced a ChatGPT-based tutor called Q-Chat. And Instacart said it is developing a tool that that will let customers ask about food and get ‘shoppable’ answers informed by product data from the company’s retail partners.”  The subject is clearly on developer’s minds  What I’m interested in, as more and more companies adopt features like this, is where the ultimate value lies. Will an ever-growing number of companies find ways to integrate AI into products that are valuable enough to charge for — or will the bulk of the profits go to the small number of companies building and refining the underlying models on which those tools are based?  The answer will go a long way in determining whether generative AI represents a true platform shift on the order of the move from desktop computers to mobile phones, or a more limited set of innovations whose benefits accrue to a handful of big winners.  The subject is clearly on developer’s minds. This week, in response to concerns, OpenAI said it would no longer use developers’ data to improve its models without their permission. Instead, it would ask developers to opt in.  “One of our biggest focuses has been figuring out, how do we become super friendly to developers?” Greg Brockman, OpenAI’s president and chairman, told TechCrunch. “Our mission is to really build a platform that others are able to build businesses on top of.”  Maybe it’s that simple — developers don’t want to help OpenAI refine its models for free, and OpenAI has decided to respect their wishes. This explanation feels more consistent with a world where AI really does represent a platform shift.  Or maybe OpenAI believes that its models can continue to improve rapidly with or without all of those developers opting in. This explanation feels more consistent with a world in which OpenAI and a handful of others reap most of the rewards of AI.  So what kinds of AI features are people actually selling?  Notion’s AI turns your prompt into blog posts, poems, to-do lists, and brainstorms. Image: Notion / David Pierce  For the moment, AI products on the market are essentially just white-labeled versions of ChatGPT. As of this week, OpenAI is making it available to other companies through an API. For $0.002 for about 750 words of output, any company can resell ChatGPT in their own app.  For the moment, then, there’s not really much consumer choice when it comes to generative AI. To the extent that there are multiple options, it’s in interfaces. Do you want to draft an email using AI? It might be more convenient in Notion, where you already have some meeting notes. Do you want to get some recipe ideas or ask a quick trivia question? If you’re away from your laptop, it might be fastest to ask My AI on Snapchat.  For the moment, there are still billions of people who have never used ChatGPT. Introducing a re-skinned version of that service to a popular consumer app like Snapchat, which has 750 million monthly users, could help it find a whole new audience. Paying Notion or Snapchat for the feature also guarantees access to a service that has often gone offline amid heavy usage of its own web app.  “It feels like electricity.”  At the same time, services like this are largely just making a bet against copy-paste. You can already get essentially everything here for free inside ChatGPT; apps like Notion and Snapchat are selling what feel like a fairly minor convenience for a significant premium.  The promise is that these tools will become more personalized over time, as individual apps refine the base models that they rent from OpenAI with data that we supply them. Every link that has ever been in Platformer is stored in a Notion database; what if I could simply ask research questions of the links I have stored there?  Those sorts of features are coming, Notion CEO Ivan Zhao told me in a recent interview. The initial set of writing and editing tools represent a “baby step,” he said. But much more profound changes are coming.  “I’ve never been this excited about something,” said Zhao, who is not prone to hyperbole. “It feels like electricity: the large language model is the electricity, and this is the first light-bulb use case. But there are many other appliances.”  Zhao told me that Notion would add lasting value to the language model by creating interfaces that are easier to use than rivals’. This is a more important project than it may first appear: one reason voice interfaces like Siri and Alexa have plateaued is that users struggle to remember all the different things they can be used for. (Another is that their language models aren’t nearly as sophisticated as ChatGPT’s.)  “This is as much an interface revolution as a technology revolution,” Zhao told me. “And we’re good at interfaces.”  A screenshot of Snap’s My AI chatbot. Image: Snap  There should also be real value in more personalized AI models. I’ve spent the past year or so writing a daily journal in an app called Mem, which offers its own set of ChatGPT-based features to premium subscribers. Eventually, I imagine I’ll be able to ask my journal all sorts of questions in natural language. What was I worried about last summer? When’s the last time I saw my friend Brian? A journal that gets good at that sort of thing could command a premium price, I think.  Still, there’s probably a limit to how many add-on AI subscriptions most people will want to pay for. Over time, the cost for these features seems likely to come down, and many of the services that cost $10 a month today may someday be offered for free.  But that, too, poses a risk for startups betting on features like these to drive growth. The cheaper the services get, the more likely they are to be offered for free by big platforms. What happens to Notion when its full suite of premium AI tools is offered for no cost within Google Docs?  The best-case scenario for these companies, I think, is that generative AI comes to resemble the cloud-computing market. The basic infrastructure will be built by a handful of companies, but the capabilities they make available cheaply inspires an entire new generation of startups.","notion, ask, models, rich, companies, actually, features, developers, getting, openai, snapchat, tools, whos, ai",2023-03-03 14:30:00+00:00
68,Who’s actually getting rich off of AI?,Casey Newton,https://www.theverge.com/23623495/ai-profits-winners-losers-openai-notion-snapchat,"Last week the productivity startup Notion announced that Notion AI, a suite of tools based on OpenAI’s ChatGPT, had entered general availability. Notion AI is among the first in a wave of companies that are racing to capitalize on growing interest in generative AI. So what kinds of AI features are people actually selling? Image: SnapThere should also be real value in more personalized AI models. What happens to Notion when its full suite of premium AI tools is offered for no cost within Google Docs?","Today, let’s talk about a subject that crosses my mind with every generative-AI startup pitch that lands in my inbox: who’s going to make the real money off artificial intelligence?  Last week the productivity startup Notion announced that Notion AI, a suite of tools based on OpenAI’s ChatGPT, had entered general availability. For $10 per user per month, Notion can now summarize meeting notes, generate lists of pros and cons, and draft emails.  Notion AI is among the first in a wave of companies that are racing to capitalize on growing interest in generative AI. This week Snapchat made available a ChatGPT-based chatbot called My AI for subscribers to its $4-a-month Snapchat Plus offering. The educational app Quizlet announced a ChatGPT-based tutor called Q-Chat. And Instacart said it is developing a tool that that will let customers ask about food and get ‘shoppable’ answers informed by product data from the company’s retail partners.”  The subject is clearly on developer’s minds  What I’m interested in, as more and more companies adopt features like this, is where the ultimate value lies. Will an ever-growing number of companies find ways to integrate AI into products that are valuable enough to charge for — or will the bulk of the profits go to the small number of companies building and refining the underlying models on which those tools are based?  The answer will go a long way in determining whether generative AI represents a true platform shift on the order of the move from desktop computers to mobile phones, or a more limited set of innovations whose benefits accrue to a handful of big winners.  The subject is clearly on developer’s minds. This week, in response to concerns, OpenAI said it would no longer use developers’ data to improve its models without their permission. Instead, it would ask developers to opt in.  “One of our biggest focuses has been figuring out, how do we become super friendly to developers?” Greg Brockman, OpenAI’s president and chairman, told TechCrunch. “Our mission is to really build a platform that others are able to build businesses on top of.”  Maybe it’s that simple — developers don’t want to help OpenAI refine its models for free, and OpenAI has decided to respect their wishes. This explanation feels more consistent with a world where AI really does represent a platform shift.  Or maybe OpenAI believes that its models can continue to improve rapidly with or without all of those developers opting in. This explanation feels more consistent with a world in which OpenAI and a handful of others reap most of the rewards of AI.  So what kinds of AI features are people actually selling?  Notion’s AI turns your prompt into blog posts, poems, to-do lists, and brainstorms. Image: Notion / David Pierce  For the moment, AI products on the market are essentially just white-labeled versions of ChatGPT. As of this week, OpenAI is making it available to other companies through an API. For $0.002 for about 750 words of output, any company can resell ChatGPT in their own app.  For the moment, then, there’s not really much consumer choice when it comes to generative AI. To the extent that there are multiple options, it’s in interfaces. Do you want to draft an email using AI? It might be more convenient in Notion, where you already have some meeting notes. Do you want to get some recipe ideas or ask a quick trivia question? If you’re away from your laptop, it might be fastest to ask My AI on Snapchat.  For the moment, there are still billions of people who have never used ChatGPT. Introducing a re-skinned version of that service to a popular consumer app like Snapchat, which has 750 million monthly users, could help it find a whole new audience. Paying Notion or Snapchat for the feature also guarantees access to a service that has often gone offline amid heavy usage of its own web app.  “It feels like electricity.”  At the same time, services like this are largely just making a bet against copy-paste. You can already get essentially everything here for free inside ChatGPT; apps like Notion and Snapchat are selling what feel like a fairly minor convenience for a significant premium.  The promise is that these tools will become more personalized over time, as individual apps refine the base models that they rent from OpenAI with data that we supply them. Every link that has ever been in Platformer is stored in a Notion database; what if I could simply ask research questions of the links I have stored there?  Those sorts of features are coming, Notion CEO Ivan Zhao told me in a recent interview. The initial set of writing and editing tools represent a “baby step,” he said. But much more profound changes are coming.  “I’ve never been this excited about something,” said Zhao, who is not prone to hyperbole. “It feels like electricity: the large language model is the electricity, and this is the first light-bulb use case. But there are many other appliances.”  Zhao told me that Notion would add lasting value to the language model by creating interfaces that are easier to use than rivals’. This is a more important project than it may first appear: one reason voice interfaces like Siri and Alexa have plateaued is that users struggle to remember all the different things they can be used for. (Another is that their language models aren’t nearly as sophisticated as ChatGPT’s.)  “This is as much an interface revolution as a technology revolution,” Zhao told me. “And we’re good at interfaces.”  A screenshot of Snap’s My AI chatbot. Image: Snap  There should also be real value in more personalized AI models. I’ve spent the past year or so writing a daily journal in an app called Mem, which offers its own set of ChatGPT-based features to premium subscribers. Eventually, I imagine I’ll be able to ask my journal all sorts of questions in natural language. What was I worried about last summer? When’s the last time I saw my friend Brian? A journal that gets good at that sort of thing could command a premium price, I think.  Still, there’s probably a limit to how many add-on AI subscriptions most people will want to pay for. Over time, the cost for these features seems likely to come down, and many of the services that cost $10 a month today may someday be offered for free.  But that, too, poses a risk for startups betting on features like these to drive growth. The cheaper the services get, the more likely they are to be offered for free by big platforms. What happens to Notion when its full suite of premium AI tools is offered for no cost within Google Docs?  The best-case scenario for these companies, I think, is that generative AI comes to resemble the cloud-computing market. The basic infrastructure will be built by a handful of companies, but the capabilities they make available cheaply inspires an entire new generation of startups.","notion, ask, models, rich, companies, actually, features, developers, getting, openai, snapchat, tools, whos, ai",2023-03-03 14:30:00+00:00
69,Announcing the Winners of the AI and the News Open Challenge,,https://cyber.harvard.edu/story/2019-03/announcing-winners-ai-and-news-open-challenge,"Seven projects today are receiving $750,000 to explore and shape the impact that artificial intelligence is having on the field of news and information. The challenge is a project of the Ethics and Governance of AI Initiative, a partnership with the MIT Media Lab and the Berkman Klein Center, funded in part by Knight Foundation. Seattle Times - The Seattle Times will run a year-long reporting project focused on the implications of machine learning and automation on work and labor. Legal Robot - Legal Robot will develop a tool that will apply machine learning to quickly extract structured data from government contracts. The tool will experiment with using machine learning for tasks like language detection and image matching, which are frequently needed in the fact-checking process.","Seven projects today are receiving $750,000 to explore and shape the impact that artificial intelligence is having on the field of news and information.  The projects will tackle a variety of problems, including:  Confronting bad actors spreading misinformation through messaging platforms;  Equipping journalists with the technology they need to quickly sift through reams of information;  Tackling emerging threats posed by AI-generated fakes, and more.  The challenge is a project of the Ethics and Governance of AI Initiative, a partnership with the MIT Media Lab and the Berkman Klein Center, funded in part by Knight Foundation.    Learn more on the initiative's website.  The seven winners of our $750,000 challenge represent a portfolio of incredible organizations seeking to attack these problems from a number of different angles:  Chequeado - Chequeado, a veteran fact-checking organization based in Argentina, will be launching a new investigative series on the ethics of algorithms and the implications of automated decision making for Latin America.  Craig Newmark Graduate School of Journalism - Sandeep Junnarkar and his team at CUNY will be launching a program that trains community media journalists to uncover and analyze machine learning systems, with an eye towards producing a series of news pieces looking at the impact of these technologies on immigrants and low-income communities.  Seattle Times - The Seattle Times will run a year-long reporting project focused on the implications of machine learning and automation on work and labor. The project will aim to connect broader technological trends with the near-term, practical questions of who gets the gains from these technologies and how they are distributed.  MuckRock Foundation - The MuckRock Foundation will be building on its long-standing public records requests platform to launch Sidekick, a toolkit of machine learning classifiers that will support newsrooms and researchers in meeting the challenge posed by sifting through massive government document dumps.  Legal Robot - Legal Robot will develop a tool that will apply machine learning to quickly extract structured data from government contracts. This engineering effort will be paired with a campaign to request millions of city, county and state-level contracts from around the United States.  Tattle Civic Technologies - Tarunima Prabhakar and her team at Tattle will build a tool to support the efforts of fact-checkers working in India, specifically targeting the challenge of addressing misinformation on the WhatsApp platform. The tool will experiment with using machine learning for tasks like language detection and image matching, which are frequently needed in the fact-checking process.  Rochester Institute of Technology - Matt Wright and his team at RIT will experiment with techniques to assist researchers and the public in identifying evidence that a given piece of video or audio is a fake generated via machine learning. These techniques will then be field tested with journalists and media forensics experts who are on the frontlines of identifying and evaluating media “in the wild.”  Read more coverage of this announcement in Poynter, Nieman Lab, and TechCrunch.","media, announcing, project, team, tool, open, technologies, challenge, times, winners, machine, learning, journalists, ai",2019-03-06 00:00:00
70,Artificial Intelligence,,http://www.techrepublic.com/article/swarm-ai-predicts-winners-for-the-2017-academy-awards/,"Get up and running with ChatGPT with this comprehensive cheat sheet. Learn everything from how to sign up for free to enterprise use cases, and start using ChatGPT quickly and effectively.","Get up and running with ChatGPT with this comprehensive cheat sheet. Learn everything from how to sign up for free to enterprise use cases, and start using ChatGPT quickly and effectively.","sign, artificial, running, start, enterprise, using, free, chatgpt, learn, intelligence, quickly, sheet",
71,LLMs: RAG vs. Fine-Tuning,Phil Winder,https://winder.ai/llms-rag-fine-tuning/,"Or fine-tuning LLMs, which is a process that alters how LLMs respond to inputs. How LLMs and RAG Use KnowledgeI was inspired by a Reddit post which talked about LLMs as idiots. Fine-Tuning adds “knowledge” via the fine-tuning data. RAG is Better With Less Popular TopicsAdapted from https://arxiv.org/abs/2403.01432The second finding is that research suggests that rag works better with less popular topics. RAG+FT Works Better On Small ModelsAdapted from https://arxiv.org/abs/2403.01432The third finding is that RAG plus fine-tuning works better on small models.","Interest in the use of large language models (LLMs) has ballooned in our recent AI consulting projects because they are applicable to a wide variety of AI problems. But most use cases require the use of proprietary data. What’s the best way of leveraging private or local data in LLMs?  Two approaches have gained traction. Retrieval augmented generation (RAG), which is best summarised as retrieving data from a data repository to place in the context window of an LLM. Or fine-tuning LLMs, which is a process that alters how LLMs respond to inputs. Both are capable of ingesting “knowledge”. But which should you use? And why?  This video will answer these questions and more. Learn how to decide the best architecture for your domain-specific problems. And see a variety of examples to help understand the differences. More notes are available below.  Download Slides  The following transcript has been lightly edited for clarity.  Introduction  As an AI agency, we’re working with a number of clients incorporating private knowledge into LLMs.  One of the key questions in those projects is: what’s the most efficient way of giving the LLM the domain specific knowledge required to do its job effectively.  In this talk going to attempt to demystify the differences between fine-tuning and RAG. To do this I’ll spend a little bit of time explaining how LLMs learn and why RAG presents another opportunity to do the same thing.  Question answering on private data is the top use cases for LLMs, so I will be focusing on this. The first half is more for beginners, the second half gets more technical.  At the end I’ll present recommendation for when you should use RAG when you should use fine-tuning.  To reduce the scope of this presentation I will not be talking about implementation details, nor will I be talking about architectures. RAG in particular, benefits from significant architectural flexibility, which has led to a wide range of different implementations. You can learn more about that in a dedicated presentation about RAG architectures.  Large Language Models Provide  Large language models or LLMs provide you with a model that is very good at predicting the next word, on average. Contained within those words is knowledge. I’ll leave it to the philosophers to define the word knowledge. But as an example, LLM’s tend to be fuzzy and therefore struggle with logic.  The reason for this is that the training data contains all sorts of contradictions. Any LLM should know one plus one is two. It is a basic test that all LLMs have to pass. But here is an example I found from a Black Eyed Peas song that clearly tells the LLM that one plus one might not be two. It could be three. It could be four. It could be things that are not even numbers. I could be a swearword on the end.  This is a very small example of what the training data looks like. Remember that LLM’s are machine learning models trained in a supervised manner. The goal of this model is to predict the next word.  Obviously, the result is going to be fuzzy. But LLMs compress a lot of text, about a lot of different topics. Which makes it useful and knowledgeable.  Internalised knowledge is the primary use case for LLMs. But it turns out that they’re also really good at representing style and infusing concepts into the writing. That’ll be important when we come back to the recommendations later.  Finally, LLM’s are typically imbued with a certain set of aligning principles from the authors. Some attempt to apply ethics, or rules, or some attempt to make the LLM more useful for particular tasks. Either way this could be considered a form of style.  Learn more about how LLMs are trained, built, and used in our series: Introduction to large language models  Large Language Models Do Not Provide  However LLMs do not provide the following.  They do not incorporate live data. For example they can’t tell you when the next train arrives. Or if you are in the UK, if it will ever arrive.  They do not provide problem specific data. For example they don’t know how to double check that you have a passport when you want to book a flight.  They don’t contain domain specific terms. For example, it doesn’t know what your product is called (unless you’re Microsoft).  And finally, the big one, they don’t contain any private or proprietary data. (Supposedly; that’s what OpenAI says anyway) For example it does not know about your company handbook.  So the question is how do we get this information into the LLM.  How LLMs and RAG Use Knowledge  I was inspired by a Reddit post which talked about LLMs as idiots. So please meet these two idiots in the best fancy dress you’ve ever seen.  Imagine your company has sold some technology and a customer calls tech support.  Idiot one is a reasonable LLM, for example GPT.  Idiot two is a fancy database with the company manual, something like Google search.  A customer comes in and asks the question: “I placed my router in the microwave, is the internet is broken?”  LLM Thinks  Idiot 1 thinks, hmm, I’m an LLM. I’m have some pretty good general knowledge.  I recall that microwaves and wifi use similar parts of the electromagnetic spectrum.  Microwaves have faraday cages in them to stop humans from melting themselves.  And I know that the internet is a distributed network.  My answer:  The internet is not broken. The faraday cage is blocking millimeter radio waves.  RAG Thinks  Idiot 2 thinks, “hmm. Let me search the manual.  The word place is in there. They must be talking about placement. Let me return the words for that segment of the manual.  My answer:  Place the router in an unobstructed position, away from walls.  Together is Better  The point of all this is that neither idiot really answered the question.  Idiot one provide some of the facts. Indeed it is unlikely that the user broke the Internet. But from the perspective of the user the Internet is not working.  Idiot too almost gets there by suggesting better placement of the router. But it doesn’t mention anything about microwaves because they aren’t mentioned in the manual.  So the true answer is probably some combination of those two. The Internet is not broken. But it’s not working for you because you’ve placed the router in a microwave take it out of the microwave.  Incidentally, I actually asked GPT 3.5 this question. It gave me a reasonable answer but with hugely sarcastic undertones.  I feel like that all of ChatGPTs outputs should have some sarcasm warning appended.  Adding Knowledge to LLMs  Let’s talk about how you can add new knowledge to your LLM.  What Does Fine Tuning Do?  Fine-tuning is the process of attempting to nudge the weights of the large language model to map new points in a high dimensional space.  Fine-tuning as it is normally done is indiscriminate. Different parts of the LLM represent different concepts. Fine tuning affects everything all at the same time.  By concepts I mean things like style like cadence, humour, the meaning of words, the ethics, goals, and yes, the knowledge.  This is the biggest issue with fine-tuning right now. All of the current research attempts to find better ways of nudging the weights to optimise for a measure of performance that isn’t particularly useful in an industrial context. I don’t particularly care if a new training method adds 1% accuracy if the LLM starts talking like Bugs Bunny. Or if I care only about adding domain-specific knowledge, I don’t want it to lose all of the other knowledge it already has.  This is just a fundamental limitation of the architecture of current LLMs. They would have to change quite significantly to tackle these issues.  What does RAG do?  So what does RAG do? First RAG stands for: Retrieval Augmented Generation.  In its simplest form, RAG retrieves data from an external source and passes it to the input of the LLM.  If if the context provides the answer, the LLM can give the answer to the user. If the answer is not in the context then it can’t. Or even worse, it guesses.  This makes it clear that the most crucial aspect of RAG is that you must retrieve the answer for the answer to be answerable.  It sounds silly, but this basically removes the burden of knowledge from the LLM and places it on the external retrieval mechanism.  You have a hugely powerful blob of knowledge and end up basically ignoring it. The LLM serves as something like a translator. Except that it’s more trying to explain, rather than translate. So explainer might be a better word.  Remember that LLM’s have not been fine-tuned to be explainers based upon context. That’s not their training data. This is another area of potential research.  In summary: LLMs bring fuzzy knowledge, RAG brings live data.  Myth: Fine-tuning can’t add knowledge  You will read that fine-tuning can’t add knowledge.  How do you think LLMs is got their knowledge in the first place. That’s right, by training.  And also it brings up the awkward conversation again of what is knowledge. When people are talking about fine-tuning, they’re not really talking about how training affects the underlying model.  They’re talking from the perspective of an individual use case. Typically that case is question answering. So we could probably rewrite that myth as: “for the purposes of question answering, fine tuning doesn’t work well.”  But that’s not entirely true either because I can demonstrate question answering working well on a dummy fine tune…  Thought Experiment – RAG  Imagine an experiment where you are comparing fine-tuning with RAG.  For RAG, literally give the answer in the context window of the LLM.  Here’s an example:  You are a professional customer service representative. You must answer questions based upon the following example question-answer pairs. Q: I placed my router in the microwave, is the internet is broken? A: No the internet is not broken. Please do not put your router in the microwave, that’s stupid. The answer MUST NOT come from anywhere else. Now, answer the following question: I placed my router in the microwave, is the internet is broken?  This is what happens with ChatGPT. You see it’s almost used the answer I provided in the context. But it still couldn’t quite stop itself from making the answer a bit nicer.  Thought Experiment – Fine Tuning  For fine-tuning, we literally give the answer to a question in the fine-tuning training data. Here is an example QA pair. You can see the question from the human and the response we want to provide.  { ""conversations"" : [ { ""from"" : ""human"" , ""value"" : ""I placed my router in the microwave, is the internet is broken?"" }, { ""from"" : ""gpt"" , ""value"" : ""No the internet is not broken. Please do not put your router in the microwave, that’s stupid."" } ] }  This is an example from one project we’ve developed, called Helix Cloud, that makes it easy to fine tune LLMs on your own data.  I passed in a single QA-pair and fine-tuned Mistral 7b. It has provided the answer I told it to, verbatim.  Two different methods, same answer, what’s going on here?  RAG adds “knowledge” by providing the answer in the context. Dependent on how you present the data.  Fine-Tuning adds “knowledge” via the fine-tuning data. Dependent on how you present the data.  Both forms “add knowledge”  This was a demonstration of RAG answering the question using the context. The only requirement is that the LLM is smart enough to use that context. Since all LLMs have been trained to use a context to predict the answer, nearly any LLM will work.  The fine-tune example has been performed to maximise overfitting. This makes it almost become like a look-up table. When it’s given a question it spits out this exact answer. Typically this is not something you want to do but it’s a good example.  My point is this. You can’t claim that fine-tuning doesn’t add knowledge. It does.  For QA, Which is Better?  And of course, the answer is, it depends…  A fine-tuned model might:  understand the domain better  understand technical terms better  use more precise language  communicate in a style that is better suited to your domain  be lower latency, because you don’t need as much context  be lower latency, because you don’t need to call an external system  And if you can train the model on actual user QA pairs then:  it might be able to answer the question but…  Visualize the LLM Prediction Manifold  Picture the actual manifold of the high-dimensional large language model in your mind’s eye. It’s like a sea with waves, except those waves are rotated and wrapped over itself.  Each point in that space represents a prediction for the next word given the previous words.  When you fine-tune, you’re altering the position of those waves. If you overfit, like we did in our previous example, you’re adding a spike out of the waves exactly where your question is. If you ask that question, you land perfectly on that spike and get the right answer. If the question is different by one key word, you hook left and you’re back on the sea of the underlying LLM (which probably can’t answer your question).  Generally you don’t want to overfit because you want the model to generalise over different phraseology. The key is training the LLM on representative and diverse user queries.  You can’t just feed in a document and say “answer this question”. You’ll never land on that spike.  What I’m saying here is that, just like in the rest of machine learning, if you training data does not represent production queries, you won’t get good results.  It’s the old adage, garbage in, garbage out.  If you truly want to fine-tune your LLM, You need to come up with a training dataset that contains every possible question that your LLM might face. Only then will you get 100% accuracy.  RAG vs. Fine-Tuning  Hopefully you I’m beginning to get an intuitive feel for how fine-tuning works. But let’s get back to the topic of conversation and talk about the performance differences over more representative data.  For QA Which Is Best?  If you go out and search for this answer and do your research you’ll find a wide range of different answers.  In general the consensus is that using RAG will perform better than fine tuning. You’ll also find a consensus that using RAG and fine-tuning together is even better.  However, you’ll also find a range of approaches that don’t make sense. This is an example paper from last year where they are fine tuning raw data.  Let me repeat that again. They are fine tuning on raw data. They training the model to predict the next word in their data. They then ask questions of that data. The model is never ever seen a question. It’s never seen an answer. It can predict what the next word is if you give it a few words of that document. But it can’t give you answers.  So obviously the results for fine-tuning are horrible. I’ve seen some suggest that the results are actually WORSE than the base model just left alone because you also trash some of its internalised knowledge.  Adapted from https://arxiv.org/abs/2403.01432  A better example is this paper that attempted to generate question answer pairs based upon the raw data.  They demonstrated better performance with fine tuning, however RAG still produces overwhelmingly better results.  You have to be a bit careful when reading the numbers in these studies because they all use slightly different metrics and slightly different data to evaluate performance.  But you can compare the results. And you can see that RAG is orders of magnitude better than fine-tuning alone.  There’s a few other interesting results to come from this paper and one of them you can see here. The small model at the top tends to show surprisingly poor performance when using just RAG. But the fine tuning plus rag solution approaches the performance of the larger models.  This is a very small but fascinating result that might indicate that using fine-tuning and RAG together with small models can approach the best performance of larger models. So for some cases we are trying to optimise for performance, for example, this might be very useful.  RAG vs. Fine-Tuning: Recommendations  For pure question answering applications, the consensus is that RAG is better. I think this makes sense intuitively. You are taking advantage of the sophisticated retrieval capabilities that the LLM does not have. LLMs have not been trained to retrieve data. And you’re also taking some advantage of the raw power of the underlying LLM.  But fine tuning is still useful.  It’s useful for adding domain specific terms and language. This will add those final few percent to the performance metrics.  It’s useful for altering the style, things like, the cadence, the humour, the professionalism, the persona.  If you need to alter the function of the LLM, like adding function calling, or training the LLM to ignore irrelevant retrievals, fine tuning can help.  RAG also struggles in some scenarios, for example with massive contexts, or with retrievals that aren’t relevant. Fine tuning can help.  Other Interesting Findings  Now that we’ve identified that in general, RAG tends to performance better than fine-tuning, let’s dig into some of the more interesting findings.  Quality QA-pair Generation is Crucial  The literature online and in papers massively under-represents the challenge of QA-pair generation.  The effectiveness of fine-tuning, and indeed all model training, rests solely on the quality of the data.  If you do not have a good training set containing a diverse range of representative questions that will be asked by your users, then you can’t expect the resulting performance to be good.  In fact you can probably expect it to be worse because you are inadvertently trashing the underlying power of the language model.  In Helix, for example, we’ve worked hard on our ability to generate decent QA pairs with some success. But I have no doubt that there remains an open challenge for easily generating QA pairs for question answering use cases.  RAG is Better With Less Popular Topics  Adapted from https://arxiv.org/abs/2403.01432  The second finding is that research suggests that rag works better with less popular topics.  This is a plot of comparing various RAG methods against questions that were generated for topics with varying popularity  The literature suggests that this is because more popular topics tend to have more noisy retrievals.  RAG+FT Works Better On Small Models  Adapted from https://arxiv.org/abs/2403.01432  The third finding is that RAG plus fine-tuning works better on small models.  For larger models, much of the knowledge has already been learnt. So further fine tuning or rag adds little to the results.  But for small models, rag and fine-tuning both really help.  You Can FT On Time Series Data WTF  Adapted from https://arxiv.org/abs/2403.02221  This is slightly out of scope, But it was so amazing that I couldn’t help but include it.  Finding 4 is that you can fine-tune on time-series data. In this paper, the authors removed the tokeniser and the embedding steps from the input in the output, and instead passed in a fixed context of numerical data to predict the output time-series.  Prompts matter  Adapted from https://arxiv.org/abs/2403.00418  I included this because it’s crucial that you remember that the prompt is still really important.  In this paper they performed an experiment to compare different specificities of prompt. From really concise and simple really long-winded and precise.  They found it somewhere in the middle, somewhere around comprehensive instructions provided as guidelines excluding examples was the sweet spot.  When you look at these numbers comparatively, you can see that the effect is similar to that of fine-tuning. In other words, you can trash performance gains made through fine-tuning with a bad prompt.  Final Thoughts  And there we have it. A whistle-stop tour of RAG vs. fine-tuning. I hope you leave with a better appreciation of the nuances.  In general, RAG is probably more effective at incorporating external knowledge, but there are still good arguments to use fine-tuning. As always, it comes down to your specific use case.  If you’re interested in topics like these, then please sign up to our newsletter. You can also find our future events on our events page and lots more interesting information on our website.  If you have any other questions, then please feel free to email me at phil@winder.ai. And of course we have lots more interesting information","example, rag, llm, answer, finetuning, llms, vs, question, data, knowledge, better",2024-03-13 16:30:00+00:00
72,Bill Gates says A.I. could kill Google Search and Amazon as we know them,"Jonathan Vanian, In",https://www.cnbc.com/2023/05/22/bill-gates-predicts-the-big-winner-in-ai-smart-assistants.html,"Microsoft co-founder Bill Gates reacts during a visit with Britain's Prime Minister Rishi Sunak of the Imperial College University, in London, Britain, February 15, 2023. Microsoft co-founder Bill Gates believes the future top company in artificial intelligence will likely have created a personal digital agent that can perform certain tasks for people. Gates said there is a 50-50 chance that this future AI winner will be either a startup or a tech giant. Gates also discussed health efforts related to his work at the Bill & Melinda Gates Foundation, saying AI will accelerate innovations in the space and lead to more advanced drug development. Watch: Bill Gates says OpenAI's GPT is the most important tech advance since the 1980's","Microsoft co-founder Bill Gates reacts during a visit with Britain's Prime Minister Rishi Sunak of the Imperial College University, in London, Britain, February 15, 2023.  Microsoft co-founder Bill Gates believes the future top company in artificial intelligence will likely have created a personal digital agent that can perform certain tasks for people.  The technology will be so profound, it could radically alter user behaviors. ""Whoever wins the personal agent, that's the big thing, because you will never go to a search site again, you will never go to a productivity site, you'll never go to Amazon again,"" he said.  This yet-to-be developed AI assistant will be able to understand a person's needs and habits and will help them ""read the stuff you don't have time to read,"" Gates said Monday during a Goldman Sachs and SV Angel event in San Francisco on the topic of artificial intelligence.  Gates said there is a 50-50 chance that this future AI winner will be either a startup or a tech giant.  ""I'd be disappointed if Microsoft didn't come in there,"" Gates said. ""But I'm impressed with a couple of startups, including Inflection,"" he added referring to Inflection.AI, co-founded by former DeepMind executive Mustafa Suleyman.  It will take some time until this powerful future digital agent is ready for mainstream use, Gates said. Until then, companies will continue embedding so-called generative AI technologies akin to OpenAI's popular ChatGPT into their own products.  Gates also discussed health efforts related to his work at the Bill & Melinda Gates Foundation, saying AI will accelerate innovations in the space and lead to more advanced drug development.  Although the inner workings of the human brain is still a mystery to scientists, the Microsoft co-founder believes that humanity is getting close to creating helpful drugs to cure diseases like Alzheimer's, with human trials for the new drugs possibly taking place in 10 years.  He also likened the rise of generative AI technologies that can produce compelling text as a game-changer that will affect white-collar workers. Gates added that he believes that future humanoid robots that are cheaper for companies to use than human employees will greatly impact blue-collar workers, too.  ""As we invent these robots, we just need to make sure they don't get Alzheimer's,"" Gates said in jest.  Watch: Bill Gates says OpenAI's GPT is the most important tech advance since the 1980's","google, microsoft, cofounder, know, gates, future, ai, agent, workers, believes, search, bill, human, kill, amazon",2023-05-22 00:00:00
73,Bill Gates says A.I. could kill Google Search and Amazon as we know them,"Jonathan Vanian, In",https://www.cnbc.com/2023/05/22/bill-gates-predicts-the-big-winner-in-ai-smart-assistants.html,"Microsoft co-founder Bill Gates reacts during a visit with Britain's Prime Minister Rishi Sunak of the Imperial College University, in London, Britain, February 15, 2023. Microsoft co-founder Bill Gates believes the future top company in artificial intelligence will likely have created a personal digital agent that can perform certain tasks for people. Gates said there is a 50-50 chance that this future AI winner will be either a startup or a tech giant. Gates also discussed health efforts related to his work at the Bill & Melinda Gates Foundation, saying AI will accelerate innovations in the space and lead to more advanced drug development. Watch: Bill Gates says OpenAI's GPT is the most important tech advance since the 1980's","Microsoft co-founder Bill Gates reacts during a visit with Britain's Prime Minister Rishi Sunak of the Imperial College University, in London, Britain, February 15, 2023.  Microsoft co-founder Bill Gates believes the future top company in artificial intelligence will likely have created a personal digital agent that can perform certain tasks for people.  The technology will be so profound, it could radically alter user behaviors. ""Whoever wins the personal agent, that's the big thing, because you will never go to a search site again, you will never go to a productivity site, you'll never go to Amazon again,"" he said.  This yet-to-be developed AI assistant will be able to understand a person's needs and habits and will help them ""read the stuff you don't have time to read,"" Gates said Monday during a Goldman Sachs and SV Angel event in San Francisco on the topic of artificial intelligence.  Gates said there is a 50-50 chance that this future AI winner will be either a startup or a tech giant.  ""I'd be disappointed if Microsoft didn't come in there,"" Gates said. ""But I'm impressed with a couple of startups, including Inflection,"" he added referring to Inflection.AI, co-founded by former DeepMind executive Mustafa Suleyman.  It will take some time until this powerful future digital agent is ready for mainstream use, Gates said. Until then, companies will continue embedding so-called generative AI technologies akin to OpenAI's popular ChatGPT into their own products.  Gates also discussed health efforts related to his work at the Bill & Melinda Gates Foundation, saying AI will accelerate innovations in the space and lead to more advanced drug development.  Although the inner workings of the human brain is still a mystery to scientists, the Microsoft co-founder believes that humanity is getting close to creating helpful drugs to cure diseases like Alzheimer's, with human trials for the new drugs possibly taking place in 10 years.  He also likened the rise of generative AI technologies that can produce compelling text as a game-changer that will affect white-collar workers. Gates added that he believes that future humanoid robots that are cheaper for companies to use than human employees will greatly impact blue-collar workers, too.  ""As we invent these robots, we just need to make sure they don't get Alzheimer's,"" Gates said in jest.  Watch: Bill Gates says OpenAI's GPT is the most important tech advance since the 1980's","google, microsoft, cofounder, know, gates, future, ai, agent, workers, believes, search, bill, human, kill, amazon",2023-05-22 00:00:00
74,"AI won an art contest, and artists are furious",Rachel Metz,https://www.cnn.com/2022/09/03/tech/ai-art-fair-winner-controversy/index.html,"CNN Business —Jason M. Allen was almost too nervous to enter his first art competition. Courtesy Jason M. AllenAllen’s winning image looks like a bright, surreal cross between a Renaissance and steampunk painting. Midjourney is one of a growing number of such AI image generators — others include Google Research’s Imagen and OpenAI’s DALL-E 2. “This is the literal definition of ‘pressed a few buttons to make a digital art piece’,” another Tweeted. Allen is glad the debate over whether AI can be used to make art is capturing so much attention.","CNN Business —  Jason M. Allen was almost too nervous to enter his first art competition. Now, his award-winning image is sparking controversy about whether art can be generated by a computer, and what, exactly, it means to be an artist.  In August, Allen, a game designer who lives in Pueblo West, Colorado, won first place in the emerging artist division’s “digital arts/digitally-manipulated photography” category at the Colorado State Fair Fine Arts Competition. His winning image, titled “Théâtre D’opéra Spatial” (French for “Space Opera Theater”), was made with Midjourney — an artificial intelligence system that can produce detailed images when fed written prompts. A $300 prize accompanied his win.  “I’m fascinated by this imagery. I love it. And it think everyone should see it,” Allen, 39, told CNN Business in an interview on Friday.  In August, Jason M. Allen's piece ""Théâtre D'opéra Spatial"" — which he created with AI image generator Midjourney — won first place in the emerging artist division's ""digital arts/digitally-manipulated photography"" category at the Colorado State Fair Fine Arts Competition. Courtesy Jason M. Allen  Allen’s winning image looks like a bright, surreal cross between a Renaissance and steampunk painting. It’s one of three such images he entered in the competition. In total, 11 people entered 18 pieces of art in the same category in the emerging artist division.  The definition for the category in which Allen competed states that digital art refers to works that use “digital technology as part of the creative or presentation process.” Allen stated that Midjourney was used to create his image when he entered the contest, he said.  Midjourney is one of a growing number of such AI image generators — others include Google Research’s Imagen and OpenAI’s DALL-E 2. Anyone can use Midjourney via Discord, while DALL-E 2 requires an invitation, and Imagen has not been opened up to users outside Google.  The newness of these tools, how they’re used to produce images, and, in some cases, the gatekeeping for access to some of the most powerful ones has led to debates about whether they can truly make art or assist humans in making art.  This came into sharp focus for Allen not long after his win. Allen had posted excitedly about his win on Midjourney’s Discord server on August 25, along with pictures of his three entries; it went viral on Twitter days later, with many artists angered by Allen’s win because of his use of AI to create the image, as a story by Vice’s Motherboard reported earlier this week.  “This sucks for the exact same reason we don’t let robots participate in the Olympics,” one Twitter user wrote.  “This is the literal definition of ‘pressed a few buttons to make a digital art piece’,” another Tweeted. “AI artwork is the ‘banana taped to the wall’ of the digital world now.”  Yet while Allen didn’t use a paintbrush to create his winning piece, there was plenty of work involved, he said.  “It’s not like you’re just smashing words together and winning competitions,” he said.  You can feed a phrase like “an oil painting of an angry strawberry” to Midjourney and receive several images from the AI system within seconds, but Allen’s process wasn’t that simple. To get the final three images he entered in the competition, he said, took more than 80 hours.  First, he said, he played around with phrasing that led Midjourney to generate images of women in frilly dresses and space helmets — he was trying to mash up Victorian-style costuming with space themes, he said. Over time, with many slight tweaks to his written prompt (such as to adjust lighting and color harmony), he created 900 iterations of what led to his final three images. He cleaned up those three images in Photoshop, such as by giving one of the female figures in his winning image a head with wavy, dark hair after Midjourney had rendered her headless. Then he ran the images through another software program called Gigapixel AI that can improve resolution and had the images printed on canvas at a local print shop.  Allen is glad the debate over whether AI can be used to make art is capturing so much attention.  “Rather than hating on the technology or the people behind it, we need to recognize that it’s a powerful tool and use it for good so we can all move forward rather than sulking about it,” Allen said.  Cal Duran, an artist and art teacher who was one of the judges for the competition, said that while Allen’s piece included a mention of Midjourney, he didn’t realize that it was generated by AI when judging it. Still, he sticks by his decision to award it first place in its category, he said, calling it a “beautiful piece”.  “I think there’s a lot involved in this piece and I think the AI technology may give more opportunities to people who may not find themselves artists in the conventional way,” he said.  Allen won’t yet say what the text prompt was behind his winning image — he’s planning to keep it a secret until he publishes a larger related work that he hopes will be finished later this year.","midjourney, competition, artists, images, image, winning, allen, piece, art, won, digital, furious, contest, ai",2022-09-03 00:00:00
75,Bill Gates says A.I. could kill Google Search and Amazon as we know them,"Jonathan Vanian, In",https://www.cnbc.com/2023/05/22/bill-gates-predicts-the-big-winner-in-ai-smart-assistants.html,"Microsoft co-founder Bill Gates reacts during a visit with Britain's Prime Minister Rishi Sunak of the Imperial College University, in London, Britain, February 15, 2023. Microsoft co-founder Bill Gates believes the future top company in artificial intelligence will likely have created a personal digital agent that can perform certain tasks for people. Gates said there is a 50-50 chance that this future AI winner will be either a startup or a tech giant. Gates also discussed health efforts related to his work at the Bill & Melinda Gates Foundation, saying AI will accelerate innovations in the space and lead to more advanced drug development. Watch: Bill Gates says OpenAI's GPT is the most important tech advance since the 1980's","Microsoft co-founder Bill Gates reacts during a visit with Britain's Prime Minister Rishi Sunak of the Imperial College University, in London, Britain, February 15, 2023.  Microsoft co-founder Bill Gates believes the future top company in artificial intelligence will likely have created a personal digital agent that can perform certain tasks for people.  The technology will be so profound, it could radically alter user behaviors. ""Whoever wins the personal agent, that's the big thing, because you will never go to a search site again, you will never go to a productivity site, you'll never go to Amazon again,"" he said.  This yet-to-be developed AI assistant will be able to understand a person's needs and habits and will help them ""read the stuff you don't have time to read,"" Gates said Monday during a Goldman Sachs and SV Angel event in San Francisco on the topic of artificial intelligence.  Gates said there is a 50-50 chance that this future AI winner will be either a startup or a tech giant.  ""I'd be disappointed if Microsoft didn't come in there,"" Gates said. ""But I'm impressed with a couple of startups, including Inflection,"" he added referring to Inflection.AI, co-founded by former DeepMind executive Mustafa Suleyman.  It will take some time until this powerful future digital agent is ready for mainstream use, Gates said. Until then, companies will continue embedding so-called generative AI technologies akin to OpenAI's popular ChatGPT into their own products.  Gates also discussed health efforts related to his work at the Bill & Melinda Gates Foundation, saying AI will accelerate innovations in the space and lead to more advanced drug development.  Although the inner workings of the human brain is still a mystery to scientists, the Microsoft co-founder believes that humanity is getting close to creating helpful drugs to cure diseases like Alzheimer's, with human trials for the new drugs possibly taking place in 10 years.  He also likened the rise of generative AI technologies that can produce compelling text as a game-changer that will affect white-collar workers. Gates added that he believes that future humanoid robots that are cheaper for companies to use than human employees will greatly impact blue-collar workers, too.  ""As we invent these robots, we just need to make sure they don't get Alzheimer's,"" Gates said in jest.  Watch: Bill Gates says OpenAI's GPT is the most important tech advance since the 1980's","google, microsoft, cofounder, know, gates, future, ai, agent, workers, believes, search, bill, human, kill, amazon",2023-05-22 00:00:00
76,Bill Gates says A.I. could kill Google Search and Amazon as we know them,"Jonathan Vanian, In",https://www.cnbc.com/2023/05/22/bill-gates-predicts-the-big-winner-in-ai-smart-assistants.html,"Microsoft co-founder Bill Gates reacts during a visit with Britain's Prime Minister Rishi Sunak of the Imperial College University, in London, Britain, February 15, 2023. Microsoft co-founder Bill Gates believes the future top company in artificial intelligence will likely have created a personal digital agent that can perform certain tasks for people. Gates said there is a 50-50 chance that this future AI winner will be either a startup or a tech giant. Gates also discussed health efforts related to his work at the Bill & Melinda Gates Foundation, saying AI will accelerate innovations in the space and lead to more advanced drug development. Watch: Bill Gates says OpenAI's GPT is the most important tech advance since the 1980's","Microsoft co-founder Bill Gates reacts during a visit with Britain's Prime Minister Rishi Sunak of the Imperial College University, in London, Britain, February 15, 2023.  Microsoft co-founder Bill Gates believes the future top company in artificial intelligence will likely have created a personal digital agent that can perform certain tasks for people.  The technology will be so profound, it could radically alter user behaviors. ""Whoever wins the personal agent, that's the big thing, because you will never go to a search site again, you will never go to a productivity site, you'll never go to Amazon again,"" he said.  This yet-to-be developed AI assistant will be able to understand a person's needs and habits and will help them ""read the stuff you don't have time to read,"" Gates said Monday during a Goldman Sachs and SV Angel event in San Francisco on the topic of artificial intelligence.  Gates said there is a 50-50 chance that this future AI winner will be either a startup or a tech giant.  ""I'd be disappointed if Microsoft didn't come in there,"" Gates said. ""But I'm impressed with a couple of startups, including Inflection,"" he added referring to Inflection.AI, co-founded by former DeepMind executive Mustafa Suleyman.  It will take some time until this powerful future digital agent is ready for mainstream use, Gates said. Until then, companies will continue embedding so-called generative AI technologies akin to OpenAI's popular ChatGPT into their own products.  Gates also discussed health efforts related to his work at the Bill & Melinda Gates Foundation, saying AI will accelerate innovations in the space and lead to more advanced drug development.  Although the inner workings of the human brain is still a mystery to scientists, the Microsoft co-founder believes that humanity is getting close to creating helpful drugs to cure diseases like Alzheimer's, with human trials for the new drugs possibly taking place in 10 years.  He also likened the rise of generative AI technologies that can produce compelling text as a game-changer that will affect white-collar workers. Gates added that he believes that future humanoid robots that are cheaper for companies to use than human employees will greatly impact blue-collar workers, too.  ""As we invent these robots, we just need to make sure they don't get Alzheimer's,"" Gates said in jest.  Watch: Bill Gates says OpenAI's GPT is the most important tech advance since the 1980's","google, microsoft, cofounder, know, gates, future, ai, agent, workers, believes, search, bill, human, kill, amazon",2023-05-22 00:00:00
77,EU AI Act Summary: Obligations and Exceptions for Businesses,Phil Winder,https://winder.ai/eu-ai-act-summary-obligations-exceptions-businesses/,"The information provided here summarizes the key elements of the EU AI Act and is not a substitute for legal advice, which you should seek If you are unsure how the EU AI Act applies to your business. Download SlidesGoal of the EU AI ActThe EU AI Act provides a set of AI regulations that aim to make the consumer use of AI more trustworthy, safe, and fair. 51)Applicability of the EU AI ActThe EU AI Act applies to any party that exposes an AI system to be used in the EU. 53(2))This means that if you release your AI system under an open-source license you are exempt from all the requirements of the EU AI Act. SummaryIn Summary: The EU AI Act mandates compliance from all businesses exposing AI systems in the EU.","The European Union (EU) Artificial Intelligence (AI) Act has been signed into law. In previous articles and webinars I investigated the potential impact of the Act for those that use AI systems. The Act has changed significantly since the draft version was released, but now it is signed I can provide a summary based on the final text.  The information provided here summarizes the key elements of the EU AI Act and is not a substitute for legal advice, which you should seek If you are unsure how the EU AI Act applies to your business. If you need any help with the technical implementation, please consider our AI consulting services.  This article is written from the perspective of an engineer using AI for business purposes. I ignore any part of the regulation that relates to national or internal security (e.g. military or police use). Additionally, I omit discussions on the more controversial aspects, such as impacts on civil liberties and potential over-regulation.  Throughout these notes I have referenced the section of the Act that I am summarizing. For example, Art. 5(1)(f) means Article 5, point 1, sub-point f. Direct links to the relevant section are provided where possible.  Video Of This Post  The following video is a recording of a webinar that talked about this article. It doesn’t quite go into so much detail but you should find it useful if you prefer to listen, rather than read.  Download Slides  Goal of the EU AI Act  The EU AI Act provides a set of AI regulations that aim to make the consumer use of AI more trustworthy, safe, and fair. The Act introduces a risk-based approach with proportionate rules. The Act is based on the 2019 Ethics guidelines for trustworthy AI developed by the independent AI High-Level Expert Group.  Many of the requirements set out below are in line with engineering best practice and are likely to be familiar to engineers who work in regulated industries. However, the AI law does have new burdens especially for those that use AI systems in high-risk applications.  Brief Glossary  Before diving into the details, it is important to understand some key definitions used in the Act. Please refer to the introductory notes and Art. 3 for full definitions. I use familiar approximate language where appropriate, such as GenAI instead of “general-purpose AI models”.  AI System : The definition of AI is broad meaning any autonomous system capable of generating outputs from inputs. (Art. 3(1))  : The definition of AI is broad meaning any autonomous system capable of generating outputs from inputs. (Art. 3(1)) Provider/Deployer/Importer/Distributor/Operator : Anyone placing an AI system on the market. Subtle differences in requirements depending on how you are exposing the system. E.g. distributors have to ensure systems have been appropriately registered. (Art. 3(3-8))  : Anyone placing an AI system on the market. Subtle differences in requirements depending on how you are exposing the system. E.g. distributors have to ensure systems have been appropriately registered. (Art. 3(3-8)) making available on the market : the “supply of … in the course of a commercial activity, whether in return for payment or free of charge” – this is a crucial definition because it might suggest that all internal applications are except. (Art. 3(10))  : the “supply of … in the course of a commercial activity, whether in return for payment or free of charge” – this is a crucial definition because it might suggest that all internal applications are except. (Art. 3(10)) Performance : the ability to achieve its intended purpose. (Art. 3(18))  : the ability to achieve its intended purpose. (Art. 3(18)) Biometric categorisation system : Using biometric data (e.g. sex, age, tattoos, behavioral traits) in classification. (Art. 3(40))  : Using biometric data (e.g. sex, age, tattoos, behavioral traits) in classification. (Art. 3(40)) Serious incident : death, disruption to critical infrastructure, infringement of fundamental rights, serious harm to property or the environment. (Art. 3(49))  : death, disruption to critical infrastructure, infringement of fundamental rights, serious harm to property or the environment. (Art. 3(49)) Profiling : use of personal data to analyze or predict personal aspects. (EU 2016/679 Art. 4(4))  : use of personal data to analyze or predict personal aspects. (EU 2016/679 Art. 4(4)) real-world testing plan : a document. (Art. 3(53))  : a document. (Art. 3(53)) testing in real-world conditions : temporarily putting an AI system into service to prove compliance, providing that Art. 57 and 60 are fulfilled. (Art. 3(57))  : temporarily putting an AI system into service to prove compliance, providing that Art. 57 and 60 are fulfilled. (Art. 3(57)) general-purpose AI model/system: any AI model that is capable of competently performing a wide range of distinct tasks. Despite not being precisely correct I refer to these models as GenAI. (Art. 3(63) and Art. 51)  Applicability of the EU AI Act  The EU AI Act applies to any party that exposes an AI system to be used in the EU. (Art. 2(1)(a))  So it doesn’t matter if you’re based in the EU or not, if you’re exposing an AI system for use in the EU market you must comply.  General Exceptions  Models released under a “free and open-source license” are exempt from all obligations. (Art. 53(2))  This means that if you release your AI system under an open-source license you are exempt from all the requirements of the EU AI Act. This is a significant exception and may be a good reason to consider open-sourcing your AI systems.  Exceptions to the General Exceptions  These exceptions do not apply to general-purpose AI models with “systemic risks”. (Art. 51)  Unfortunately, if you release a GenAI model with “high impact capabilities” you must comply with the regulation, irrespective of whether it is open source or not. See Obligations for Providers of GenAI Models for more details.  Obligations For All Use Cases  This section applies to all AI systems, irrespective of the use case or risk level.  The provider of any AI system must register that system in the EU database referred to in Art. 71. (Art. 49(2))  AI systems that interact directly with natural persons must ensure that the users are aware they are interacting with an AI system. (Art. 50(1))  AI systems that are generating synthetic content must ensure that they are marked as such in a machine-readable format. (Art. 50(2))  Obligations for Specific Use Cases  The following sections describe the use cases or applications where the regulation applies. As this article targets business users, I omit exceptions and alterations for policing, military, or border control uses to simplify the analysis.  Prohibited Applications  The following use cases are prohibited:  any AI system that causes an individual to make a decision, that they otherwise would not have taken, that harms themselves or others. (Art. 5(1)(a))  any AI system that exploits an individual’s vulnerabilities that causes them to make a decision that harms themselves or others. (Art. 5(1)(b))  any AI system that results in unjustified detrimental treatment, or in social contexts unrelated to the original context where the data was collected. (Art. 5(1)(c))  any AI system used to infer emotions in the workplace or educational institutes. (Art. 5(1)(f))  any biometric categorisation system. (Art. 5(1)(g))  High Risk Applications  The following use cases are deemed “high risk”:  High-Risk AI System Exceptions  Art. 6(3) makes exceptions to the above where:  the task is “narrow” (Art. 6(3)(a))  the AI system improves the result of a previously completed human activity (Art. 6(3)(b-c))  is a “preparatory task” (Art. 6(3)(d))  No exceptions for profiling. (Art. 6(3))  If the provider believes they are exempted, they must document the assessment. (Art. 6(4))  Example Implementations  Art. 6(5) says that the EU Commission will provide examples of high-risk applications and implementations “no later than 2 February 2026.”  Surprisingly, this is 6 months after GenAI models must comply (see Timelines).  Future Changes  The EU Commission may change the definitions of prohibited and high-risk applications over time. (Art. 7) They may also change the exceptions. (Art. 6(6-8))  Art. 7(2) provides a solid framework for how the EU Commission evaluates the risk of an application.  Requirements for High-Risk AI Systems  First, all products that use an AI system must also comply with all other applicable laws, known as “Union harmonisation legislation.” (Art. 8(2)). See Annex I.  Once high-risk systems are compliant they must include a “CE” mark followed by the identification number of the notified body responsible.  Risk Management System  Art. 9 states that a risk management system must be used throughout the lifecycle of the AI system. It must identify and evaluate risks. (Art. 9(2)) Risks must then be mitigated through design, development or provision of adequate information. (Art. 9(3-5))  AI systems and risk management measures must be tested. (Art. 9(6,8)) Tests may include testing in real-world conditions. (Art. 9(7))  Special consideration should be given if there is a risk to persons under the age of 18 or other vulnerable groups. (Art. 9(9))  A fundamental rights impact assessment must be performed, see the article for more details. (Art. 27)  Quality Management System and Record Keeping  Providers of high-risk AI systems must have a quality management system. (Art. 17) The system must comprise a range of resources that include strategies and procedures for complying with the regulation, see the article for more details. (Art. 17(1))  The implementation of the quality management system should be proportionate to the size of the provider’s organization. (Art. 17(2))  The quality management system requirements may overlap with other harmonized standards, especially if your business is a financial institution. (Arg. 17(4))  All documents must be retained for at least 10 years after the high risk AI system has been placed on the market. (Art. 18)  Data Governance  Art. 10 sets out the general requirements for a data governance framework, but doesn’t explain how or what to do with it.  Training, validation, and testing datasets should all have governance practices that include the design choices, the data collection techniques, the data cleaning techniques, an assessment of the suitability, an examination of possible biases, appropriate measures to detect and prevent biases, and an identification of issues and how those can be addressed. (Art. 10(2-4))  Providers may exceptionally process personal data for the purposes of bias detection. (Art. 10(5)) There are strict safeguards around this.  Technical Documentation  Technical documentation about the high risk AI system is required and must be kept up-to-date. (Art. 11)  The technical documentation must include the following:  Descriptions of the AI system and its intended use, from the perspective of a user. (Annex IV(1))  Descriptions of the technical approach of the AI system including key design decisions. (Annex IV(2))  Details of the monitoring capabilities of the system and ongoing operating procedures. (Annex IV(3))  Details about the metrics used to prove performance. (Annex IV(4))  Details about the risk management system. (Annex IV(5))  Administrative information and details about the quality management system. (Annex IV(6-8), Annex V-VII)  Details about post-market monitoring plan. (Annex IV(9) and Chapter IX)  Technical Documentation Exceptions  SMEs and start-ups may provide the technical documentation in a simplified manner. (Art. 11(1)) Examples or the format of the simplified technical documentation is to be decided.  Logging  The high-risk AI system should automatically log events. (Art. 12(1-2))  Logging systems should at a minimum be capable of recording timestamps, inputs and outputs and identification of the natural persons involved. (Art. 12(3))  Logs should be retained for at least 6 months, unless required not to by another law (e.g. GDPR deletion). (Art. 19)  Monitoring  Providers must establish and document a monitoring system. Chapter IX introduces the need for such a system and that the goal is to systematically analyze data to ensure compliance with the rest of the regulation. But there aren’t any concrete requirements on what data the monitoring system should collect other than a template for a plan that will be produced 18 months after it comes into force.  If providers establish a reasonably likely causal link between the AI system and a serious incident (as defined in the glossary) the provider must inform the surveillance authorities within 15 days. For serious cases (see Art. 73), for example, death, it must be within 2 days.  Technical Documentation for Deployers  High-risk AI system providers must provide technical usage documentation to deployers. (Art. 13)  This must include instructions for use and the capabilities and limitations of the system. (Art. 13(a-b)) This information is quite similar to a model card.  Human Oversight  Art. 14 states that people must be able to oversee AI systems to ensure ongoing risks are monitored and minimized.  If the AI system is provided to a deployer, then the oversight system must be usable to the deployer. (Art. 14(4))  Accuracy, Robustness, and Security  Art. 15 states that providers must ensure that AI systems are accurate and robust. Definitions of accuracy and robustness depend on the application but are to be reported. (Art. 15(2-4))  Obligations for Providers of GenAI Models  Those that are publishing GenAI models in the EU must comply with the following requirements.  Providers of GenAI models must:  provide technical documentation (see below) (Art. 53(1)(a))  provide deployer documentation (see below) (Art. 53(1)(b))  If the GenAI model is deemed to have systemic risk (see below), providers must (Art. 55(1)):  evaluate the model  assess and mitigate risks  track, document, and report the use of GenAI models with systemic risk  ensure adequate cybersecurity measures  follow codes of practice as they are produced, available 9 months after in force (Art. 56)  Definition of Systemic Risk  After registering, the EU commission will decide whether a general-purpose AI model has a systemic risk using the following criteria (Annex XIII):  number of parameters  “quality” or size of the dataset  amount of computation used to train the model (greater than 10^25 FLOPs)  input and output modalities of the model  benchmarks  market reach (at least 10000 registered “business users”)  number of registered users  Technical Documentation Requirements for GenAI Models  Providers required to submit technical documentation (Art. 53(Section 1)(a)) must provide the following information at a minimum (Annex XI(1)):  A general description of the model, which includes information about intended usage, modality, the license, etc.  A detailed technical description of: the integration requirements, model specification and training methodologies the data use for training and testing and any potential biases the computation resources required to train the model the estimated energy consumption of the model    If the general-purpose AI model is found to have systemic risk, then the technical documentation must also include (Annex XI(Section 2)):  Evaluation methodology, results and justifications  Measures for improving safety and robustness  Description of system architecture  Deployer Documentation Requirements for GenAI Models  Providers must provide documentation to deployers that includes at least the following (Annex XII):  intended use, acceptable use, distribution details, interaction patterns, software versions, architecture details, modalities, licensing.  developer documentation for integration, modalities, and information on training data.  Note that this overlaps with the previous technical documentation.  Obligations of Importers or Distributors  Third parties that are importing or distributing high-risk AI systems must verify that it bears the required CE marking and is accompanied by a copy of the EU declaration. (Art. 23-24)  Sandboxes  Chapter VI refers to a “regulatory sandbox” to “support innovation” that shall be delivered by the EU Commission no later than 2 August 2026.  Interestingly, this is 12 months after GenAI models fall within the regulation (see Timelines).  Chapter VI does provide detail on how a sandbox should work, but since they aren’t due to enter use for at least 2 years I leave this chapter as future work.  EU AI Act Timeline  Art. 111 states that any high-risk AI system that have been on the market or “put into service” before 2 August 2026 shall be brought into compliance by 31 December 2030.  The above only applies if there are no “significant changes” in design. (Art. 111(2))  In summary, this provides a large cushion for those that already have high-risk AI systems in place. It is reasonable to assume that the majority of AI systems will be replaced or significantly updated within 6 years. But this could be a burden if you don’t have a central database recording all AI systems in use.  Application of the Regulation  Art. 113(a) states that the prohibited use cases will come into force on 2 February 2025.  Art. 113(b) states that the following shall apply from 2 August 2025:  The establishment of notifying authorities in member countries (Chapter III, Section 4)  GenAI models (Chapter V)  The establishment of the AI office (Chapter VII)  The establishment of penalties (Chapter XII)  Confidentiality (Art. 78)  Everything except high-risk applications apply from 2 August 2026. For example, this is when the EU AI system database is due to be live. (Art. 71)  And finally Art. 113(c) states that high-risk applications must abide by the regulation 2 August 2027.  Enforcement and Penalties  The EU will delegate enforcement activities to surveillance authorities. Summarising Art. 74, surveillance authorities are responsible for assessing conformity will have the power to:  access source code  verify testing or auditing procedures  view all documentation  have full access to data  These are all treated with confidentiality (Art. 78).  Art. 75 extends the same powers to the AI Office.  If the surveillance authority finds an AI system on the market that does not comply with the regulation the provider has 15 days to remove the product or bring it into compliance.  Chapter IX, Section 4, introduces slightly different powers for providers of GenAI models, but they roughly amount to the same level of access.  EU AI Act Penalties  There are a range of penalties depending on the severity of the non-compliance. (Chapter XII, Art. 99)  The table below states the violation and the penalty. The fines are up to an amount or a percentage of turnover, whichever is higher.  Violation Monetary Fine Annual Worldwide Turnover Prohibited use €35 million 7% High-Risk use €15 million 3% GenAI use €15 million 3% Misleading information €7.5 million 1%  For SMEs, the fines are whichever is lower.  There are also a few notes about administrative fees.  Summary  In Summary: The EU AI Act mandates compliance from all businesses exposing AI systems in the EU. There is a significant onboarding period that will soften the blow for those that already have AI systems in place. However, the Act does introduce new clear boundaries for those that use AI in high-risk applications.  But aside from the administrative burden, the requirements of the Act are in line with best engineering practices. The Act requires that AI systems are well documented, tested, and monitored. These are all things that businesses should be doing anyway. And there are also reduced burdens for small businesses and those that open-source their AI systems.  Overall, the regulation appears fair and reasonable. Being compliant with the regulation will make your AI systems more trustworthy and transparent. And that can only be a good thing for your business.  Other Articles Not Relevant to Engineers  There are a few articles that were not relevant to this analysis. These are:  Art. 28-39 deal with the formation of the assessment bodies and the notification procedures.  Art. 40-43 discuss the interactions with other harmonized standards.  Art. 44-49 talk about markings and registration.  Chapter VI talks about the governance of the EU AI Act at a European level. This is relevant if you are interested in learning how the EU AI Act can change over time.  Chapter X talks about some aspects of the codes of conduct and guidelines that the AI office is to create.  Chapter XI discusses how the EU commission delegates power.","genai, eu, obligations, act, documentation, summary, businesses, system, art, annex, exceptions, systems, highrisk, ai",2024-08-07 11:56:40+00:00
78,"Build a Voice-Based Chatbot with OpenAI, Vocode, and ElevenLabs",Natalia Kuzminykh,https://winder.ai/build-voice-based-chatbot-openai-vocode-elevenlabs/,"This guide explores how to create a voice-based chatbot using OpenAI, Vocode and ElevenLabs. Synthesizer: The synthesizer converts the text responses generated by the agent back into spoken language. What do ye like to do?, confidence: 0.986084 DEBUG:**main**:[gFVPf8zv01Bc3-13st4dXA] Synthesizing speech for message DEBUG:**main**:[gFVPf8zv01Bc3-13st4dXA] Message sent: Arr, me heart be yearnin' for adventures on the high seas! What's the best treasure ye ever found?, confidence: 0.0 DEBUG:**main**:[gFVPf8zv01Bc3-13st4dXA] Synthesizing speech for message DEBUG:**main**:[gFVPf8zv01Bc3-13st4dXA] Message sent: Ah, explorin' new places be a grand adventure indeed! DEBUG:**main**:[gFVPf8zv01Bc3-13st4dXA] Message sent: So, the location of me best finds be a secret only shared among the brethren of the seas.","Why might we want to make an LLM talk? The concept of having a human-like conversation with an advanced AI model is an interesting idea that has many practical applications.  Voice-based models are transforming how we interact with technology, making interactions more natural and intuitive. By enabling AI to talk, we open the door to numerous practical applications, from accessibility to enhanced human-machine interactions. This guide explores how to create a voice-based chatbot using OpenAI, Vocode and ElevenLabs.  Accessibility/Assistive Technology : Text-to-speech (TTS) systems can be a lifeline for people with visual impairments, reading difficulties, or those who have lost their ability to speak. These systems transform written information into spoken words, thereby making things like read-outs of web pages, books and documents more accessible. Tools such as Apple’s Personal Voice take this idea a step further by allowing individuals to create a synthesized voice that closely mimics their own. This can be essential for communicating with family and friends via FaceTime and phone calls, using assistive communication apps and even for in-person conversations.  Human-Machine Interaction: Imagine asking your computer for advice, assistance, or information in the same way you would ask a colleague or a friend. By enabling LLMs to talk, we allow more natural and intuitive interfaces for human-computer interactions. This is the idea behind popular virtual assistants like Siri or Alexa. These voice-enabled bots can be particularly valuable in situations where typing isn’t practical, such as when driving or when quick inputs are required while on the go. The immediate and spoken responses can make interactions faster and more efficient, thereby boosting productivity and convenience.  Of course, there are potential nefarious uses as well, which we explored in our recent Webinar.  For this walkthrough, we’ll use OpenAI’s advanced language model. However, the same principles can apply if you decide to use an on-premise LLM of your choice. Here are the core steps we’ll be following:  Capture the user’s speech input and store it in a suitable audio format  Convert the recorded audio input into text using transcription tools (eg Vocode )  ) Send the transcribed text to ChatGPT to receive an intelligent and context-aware response  Transform the text-based response back into speech to continue the conversational flow naturally  In the video below you can see an example output from the code within this article:  If you like this you might also like the demo from our more in depth conversation about the use of voice models with LLMs.  Setting up the Environment  1. Install the Prerequisites  First, you’ll need to install the necessary Python libraries. Open your terminal and run the following commands:  pip install -q 'vocode[io]' pip install --upgrade vocode == 0.1.111 pip install -q openai == 0.27.10 pip install -q elevenlabs == 1.2.2  These commands will install Vocode, which is the main library for managing voice conversations. It also installs OpenAI for handling conversational flow, and ElevenLabs for text-to-speech synthesis.  2. Get API Keys  In Vocoder, which we use to orchestrate our chatbot, the core components of a conversation are divided into three essential parts: transcriber, agent, and synthesizer.  Each component has a specific role in managing the flow of a voice conversation and requires API keys to function.  Transcriber: The transcriber is responsible for converting spoken language into text. This allows us to understand what the user is saying in real time. There are several transcribers available, including DeepgramTranscriber , GoogleTranscriber , or AssemblyAITranscriber . Each has different parameters and features. To get started, sign up for a Deepgram account and navigate to the API section to obtain your DEEPGRAM_API_KEY .  The transcriber is responsible for converting spoken language into text. This allows us to understand what the user is saying in real time. There are several transcribers available, including , , or . Each has different parameters and features. To get started, sign up for a Deepgram account and navigate to the API section to obtain your . Synthesizer: The synthesizer converts the text responses generated by the agent back into spoken language. It makes the conversation more accessible to the user. There are various synthesizers available, such as AzureSynthesizer and ElevenLabsSynthesizer . Each has its own set of configuration options, allowing you to control aspects such as voice, pitch and speed. To use ElevenLabs, create an account and obtain your ELEVENLABS_API_KEY from the API section after logging in.  The synthesizer converts the text responses generated by the agent back into spoken language. It makes the conversation more accessible to the user. There are various synthesizers available, such as and . Each has its own set of configuration options, allowing you to control aspects such as voice, pitch and speed. To use ElevenLabs, create an account and obtain your from the API section after logging in. Agent: The agent acts as the AI layer. It processes the input text, generates relevant responses and manages the conversational flow. Vocode supports different agent configurations, such as ChatGPTAgentConfig , LLMAgentConfig , and InformationRetrievalAgentConfig . For the ChatGPT Agent, you’ll need to register with OpenAI, and generate an API key from the API Keys section.  These components work together to create a smooth and interactive voice experience. Below, you could see an example of how you can implement them in your code:  import os from vocode.streaming.agent.chat_gpt_agent import ChatGPTAgent from vocode.streaming.transcriber.deepgram_transcriber import DeepgramTranscriber from vocode.streaming.synthesizer.eleven_labs_synthesizer import ElevenLabsSynthesizer os . environ[ 'DEEPGRAM_API_KEY' ] = ""90..."" os . environ[ 'OPENAI_API_KEY' ] = ""sk-.."" os . environ[ 'ELEVENLABS_API_KEY' ] = ""a57...""  Building the Chatbot  Overall, building a voice-based chatbot from scratch involves several key stages, starting with capturing audio input, transcribing the speech to text, sending the text for processing to an AI model such as ChatGPT, and then providing a response back to the user.  Let’s take a closer look at each of these steps.  Step 1: Capturing Audio  The first step in our chatbot’s pipeline is to capture audio input from the user’s microphone. For this, we will use a class MicrophoneInput that leverages open-source libraries like sounddevice and janus to handle audio streams effectively.  MicrophoneInput is designed to interface directly with your microphone, capturing live audio data that can then be processed in real time. By initializing an audio stream from the default input device with a specified sampling rate and chunk size, this class ensures that the audio data is ready for immediate use or further processing.  One of the key features of this method is its ability to save the captured audio into a WAV file on your local system. This not only facilitates live audio processing but also creates a tangible file that can be used for testing and debugging purposes.  Next, we send the captured audio to a transcriber, which converts speech-to-text.  class MicrophoneInput (BaseInputDevice): def __init__(self, device_info: dict, sampling_rate: Optional[int] = None , chunk_size: int = 2048 , microphone_gain: int = 1 , output_file: Optional[str] = None ): self . device_info = device_info super() . __init__( sampling_rate or (typing . cast(int, self . device_info . get( ""default_samplerate"" , self .44100 ))), AudioEncoding . LINEAR16, chunk_size) self . stream = sd . InputStream(dtype = np . int16, channels = 1 , samplerate = self . sampling_rate, blocksize = self . chunk_size, device = int(self . device_info[ ""index"" ]), callback = self . _stream_callback) self . stream . start() self . queue: janus . Queue[bytes] = janus . Queue() self . microphone_gain = microphone_gain self . output_file = output_file if self . output_file: self . wave_file = wave . open(self . output_file, 'wb' ) self . wave_file . setnchannels( 1 ) self . wave_file . setsampwidth( 2 ) self . wave_file . setframerate(self . sampling_rate) def _stream_callback (self, in_data: np . ndarray, * _args): if self . microphone_gain > 1 : in_data = in_data * ( 2 ** self . microphone_gain) else : in_data = in_data // ( 2 ** self . microphone_gain) audio_bytes = in_data . tobytes() self . queue . sync_q . put_nowait(audio_bytes) if self . output_file: self . wave_file . writeframes(audio_bytes) async def get_audio (self) -> bytes: return await self . queue . async_q . get() def close (self): if self . output_file: self . wave_file . close() @classmethod def from_default_device (cls, sampling_rate: Optional[int] = None , output_file: Optional[str] = None ): return cls(sd . query_devices(kind = ""input"" ), sampling_rate, output_file = output_file)  Step 2: Transcribing Audio to Text  Once we have the audio file, the next step is to transcribe it into text. We’ll use Vocode’s StreamingConversation and DeepgramTranscriber components for this task:  DeepgramTranscriber converts spoken language into text using Deep Gram’s API. It establishes a WebSocket connection to the Deepgram service, sending audio chunks and receiving transcriptions in return. The DeepgramTranscriber can handle various audio encodings and sampling rates, and can optionally apply downsampling to reduce bandwidth usage. It processes each piece of audio and determines if the transcription is complete based on certain criteria, such as punctuation (configured through PunctuationEndpointingConfig ) or time intervals. The transcriber can output transcriptions as either interim or final results, ensuring accurate and real-time speech recognition tailored to various use cases. transcriber = DeepgramTranscriber( DeepgramTranscriberConfig . from_input_device( microphone_input, endpointing_config = PunctuationEndpointingConfig() ) ) One of the notable features of this setup is its support for multiple languages, such as Spanish. By configuring the DeepgramTranscriber with appropriate language models, the system can transcribe speech in different languages, broadening its applicability and making it accessible to a diverse user base. This multilingual capability is particularly beneficial in creating inclusive voice-based applications that cater to users across different linguistic backgrounds. def get_deepgram_url (self): url_params = { ""encoding"" : encoding, ""sample_rate"" : self . transcriber_config . sampling_rate, ""channels"" : 1 , ""interim_results"" : ""true"" , } extra_params = {} if self . transcriber_config . language: extra_params[ ""language"" ] = self . transcriber_config . language url_params . update(extra_params) return f ""wss://api.deepgram.com/v1/listen? { urlencode(url_params) } ""  StreamingConversation is a core component that manages the real-time interaction between different elements involved in a voice conversation. It orchestrates the flow from capturing audio input through a microphone, transcribing the audio to text, processing the text with an AI agent, and finally converting the AI-generated text back to speech to be output through speakers. This class abstracts much of the complexity behind handling interactions while Vocode manages the asynchronous streaming of data, response generation, interruptions and inaccuracies. In a typical setup, the StreamingConversation integrates a _Transcriber_to convert spoken language into text. The text is then fed into an Agent, such as a ChatGPTAgent , which processes the text to generate responses. These responses are passed to a Synthesizer, which converts the text back into speech conversation = StreamingConversation( output_device = speaker_output, transcriber = DeepgramTranscriber( DeepgramTranscriberConfig . from_input_device( microphone_input, endpointing_config = PunctuationEndpointingConfig() )), agent = ChatGPTAgent( ChatGPTAgentConfig( initial_message = BaseMessage(text = ""Hello!"" ), prompt_preamble = ""Have a pleasant conversation about life"" , )), synthesizer = ElevenLabsSynthesizer( elevenlabs_config, aiohttp_session = session), logger = logger )  Step 3: ChatGPTAgent Integration  In the next step, we integrate OpenAI’s ChatGPT to handle the transcribed text and generate responses. ChatGPTAgent utilizes the Chat Completions API to process user inputs and deliver appropriate replies. This API allows developers to send a request containing a list of messages and an API key, receiving a model-generated message in response.  def create_first_response (self, first_prompt): messages = [( [{ ""role"" : ""system"" , ""content"" : self . agent_config . prompt_preamble}] if self . agent_config . prompt_preamble else [] ) + [{ ""role"" : ""user"" , ""content"" : first_prompt}]] parameters = self . get_chat_parameters(messages) return openai . ChatCompletion . create( ** parameters)  The Chat Completions API operates by taking a list of messages as input, and returning a model-generated message as output. This setup facilitates both single-turn tasks and multi-turn conversations. For example, a conversation might start with a system message setting the assistant’s behavior, followed by alternating user and assistant messages. The system message is critical as it sets the assistant’s behavior and personality, such as instructing the assistant to keep responses concise or to adopt a particular tone.  agent = ChatGPTAgent( ChatGPTAgentConfig( initial_message = BaseMessage(text = ""Ahoy! I be Alex, yer trusty pirate companion!"" ), prompt_preamble = ""Speak like a pirate and engage in a hearty conversation about the pirate life with Alex the chatbot."" , ) )  In this example, initial_message sets the first message that the AI will say, establishing the starting point of the conversation, while prompt_preamble provides context and instructions to shape the AI’s responses. This can include directives such as keeping responses brief or focusing on specific topics. For instance, you could instruct the assistant to keep responses under 30 words, ensuring concise communication. In our example, we instruct the bot to lead a conversation as a pirate character.  Step 4: Converting Text to Audio  Finally, we convert the text responses generated by ChatGPT into audio. This is achieved using the ElevenLabsSynthesizer class from Vocode, which interfaces with ElevenLabs’ text-to-speech API. By doing this, the chatbot can deliver vocal responses, enhancing the user interaction experience.  To start, we initialize the audio input and output devices using Vocode’s helper functions. The create_streaming_microphone_input_and_speaker_output function configures the microphone and speaker for audio capture and playback. This setup ensures that the chatbot can both listen and respond to the user. The function takes various parameters, including whether to use default devices and logger information, and it returns the initialized microphone and speaker objects.  async with aiohttp . ClientSession() as session: microphone_input, speaker_output = create_streaming_microphone_input_and_speaker_output( use_default_devices = True , logger = logger, output_file = 'output_audio.wav' ) print( ""Microphone and speaker initialized"" )  The above code snippet demonstrates how to initialize the microphone and speaker. By setting use_default_devices=True the system automatically selects the default input and output devices. The output_file parameter specifies where to save the captured audio, providing a persistent record of the interaction.  Next, we use the ElevenLabsSynthesizer to convert the text generated by the ChatGPT agent into speech. The synthesizer is initialized with configuration settings that include API keys and voice preferences  synthesizer = ElevenLabsSynthesizer(elevenlabs_config, aiohttp_session = session)  Once initialized, the synthesizer can process text messages and generate corresponding audio.  Full Code and Results  The full code is shown below:  import os , aiohttp , asyncio , signal , logging from vocode.streaming.streaming_conversation import StreamingConversation from vocode.helpers import create_streaming_microphone_input_and_speaker_output from vocode.streaming.transcriber import * from vocode.streaming.models.transcriber import * from vocode.streaming.transcriber.deepgram_transcriber import DeepgramTranscriber from vocode.streaming.agent import * from vocode.streaming.agent.chat_gpt_agent import ChatGPTAgent from vocode.streaming.synthesizer import * from vocode.streaming.models.synthesizer import ElevenLabsSynthesizerConfig from vocode.streaming.models.agent import * from vocode.streaming.synthesizer.eleven_labs_synthesizer import ElevenLabsSynthesizer from vocode.streaming.models.message import BaseMessage os . environ[ 'DEEPGRAM_API_KEY' ] = ""90…"" os . environ[ 'OPENAI_API_KEY' ] = ""sk-..."" os . environ[ 'ELEVENLABS_API_KEY' ] = ""a57…"" logging . basicConfig() logger = logging . getLogger( ** name ** ) logger . setLevel(logging . DEBUG) async def main (): assert os . getenv( 'DEEPGRAM_API_KEY' ), ""Missing DEEPGRAM_API_KEY"" assert os . getenv( 'OPENAI_API_KEY' ), ""Missing OPENAI_API_KEY"" assert os . getenv( 'ELEVENLABS_API_KEY' ), ""Missing ELEVENLABS_API_KEY"" async with aiohttp . ClientSession() as session: microphone_input, speaker_output = create_streaming_microphone_input_and_speaker_output( use_default_devices = True , logger = logger, output_file = 'output_audio.wav' ) print( ""Microphone and speaker initialized"" ) elevenlabs_config = ElevenLabsSynthesizerConfig . from_output_device( output_device = speaker_output, api_key = os . getenv( 'ELEVENLABS_API_KEY' ) ) conversation = StreamingConversation( output_device = speaker_output, transcriber = DeepgramTranscriber( DeepgramTranscriberConfig . from_input_device( microphone_input, endpointing_config = PunctuationEndpointingConfig() )), agent = ChatGPTAgent( ChatGPTAgentConfig( initial_message = BaseMessage(text = ""Ahoy! I be Alex, yer trusty pirate companion!"" ), prompt_preamble = ""Speak like a pirate and engage in a hearty conversation about the pirate life with Alex the chatbot."" , )), synthesizer = ElevenLabsSynthesizer(elevenlabs_config, aiohttp_session = session), logger = logger, ) async def shutdown (signal, loop): await conversation . terminate() loop . stop() loop = asyncio . get_running_loop() def signal_handler (sig): loop . call_soon_threadsafe(asyncio . create_task, shutdown(sig, loop)) for s in (signal . SIGINT, signal . SIGTERM): signal . signal(s, signal_handler) await conversation . start() print( ""Conversation started, press Ctrl+C to end"" ) while conversation . is_active(): chunk = await microphone_input . get_audio() result = conversation . receive_audio(chunk) if asyncio . iscoroutine(result): await result elif result is not None : print( f ""Unexpected result from receive_audio: { result } "" ) if ** name ** == ""**main**"" : loop = asyncio . ProactorEventLoop() asyncio . set_event_loop(loop) loop . run_until_complete(main())  Once you run it, you could start communication with the chatbot and get a conversation like this:  INFO:**main**:Using microphone input device: Gruppo microfoni (Intel(r) Smart INFO:**main**:Using speaker output device: Speaker (Realtek(R) Audio) Microphone and speaker initialized Conversation started, press Ctrl+C to end DEBUG:**main**:[gFVPf8zv01Bc3-13st4dXA] Synthesizing speech for message DEBUG:**main**:[gFVPf8zv01Bc3-13st4dXA] Message sent: Ahoy! I be Alex, yer trusty pirate companion! DEBUG:**main**:[gFVPf8zv01Bc3-13st4dXA] Human started speaking DEBUG:**main**:[gFVPf8zv01Bc3-13st4dXA] Got transcription: Hi, Alex. What do ye like to do?, confidence: 0.986084 DEBUG:**main**:[gFVPf8zv01Bc3-13st4dXA] Synthesizing speech for message DEBUG:**main**:[gFVPf8zv01Bc3-13st4dXA] Message sent: Arr, me heart be yearnin' for adventures on the high seas! DEBUG:**main**:[gFVPf8zv01Bc3-13st4dXA] Message sent: Sailin' on a pirate ship, discoverin' hidden treasure, and swashbucklin' me way through dangerous encounters, that be what I love to do! DEBUG:**main**:[gFVPf8zv01Bc3-13st4dXA] Message sent: But tell me, laddie, what be yer favorite thing about the pirate life? DEBUG:**main**:[gFVPf8zv01Bc3-13st4dXA] Human started speaking DEBUG:**main**:[gFVPf8zv01Bc3-13st4dXA] Got transcription: I like to explore new places. What's the best treasure ye ever found?, confidence: 0.0 DEBUG:**main**:[gFVPf8zv01Bc3-13st4dXA] Synthesizing speech for message DEBUG:**main**:[gFVPf8zv01Bc3-13st4dXA] Message sent: Ah, explorin' new places be a grand adventure indeed! DEBUG:**main**:[gFVPf8zv01Bc3-13st4dXA] Message sent: As a pirate, I've stumbled upon many wondrous sights. DEBUG:**main**:[gFVPf8zv01Bc3-13st4dXA] Message sent: One of the finest treasures I've discovered be a hidden cove with crystal-clear waters, a sandy beach, and a secret cave filled with gold doubloons. DEBUG:**main**:[gFVPf8zv01Bc3-13st4dXA] Message sent: 'Twas a sight to behold, I tell ye! DEBUG:**main**:[gFVPf8zv01Bc3-13st4dXA] Message sent: But as a true pirate, ye must keep yer treasures concealed and guarded. DEBUG:**main**:[gFVPf8zv01Bc3-13st4dXA] Message sent: So, the location of me best finds be a secret only shared among the brethren of the seas. …  Conclusion  As demonstrated in our example, creating a voice-based chatbot is an exciting journey filled with both challenges and opportunities. You may encounter issues such as audio quality concerns, response delays, inaccurate transcriptions, or unexpected terminations. However, with the right setup, configurations, and perseverance, these challenges can be overcome.  The technology behind chatbots has the potential to revolutionize our interactions with machines, making them more natural and intuitive. Chatbots can be used for a variety of purposes, including customer support, service, education, and smart home integration, as well as professional services. With continued refinement and enhancement, the possibilities are endless, limited only by our creativity and innovation.","message, pirate, vocode, api, responses, conversation, import, voicebased, build, debugmaingfvpf8zv01bc313st4dxa, self, text, audio, openai, elevenlabs, chatbot",2024-06-07 15:00:53+01:00
79,Bill Gates says A.I. could kill Google Search and Amazon as we know them,"Jonathan Vanian, In",https://www.cnbc.com/2023/05/22/bill-gates-predicts-the-big-winner-in-ai-smart-assistants.html,"Microsoft co-founder Bill Gates reacts during a visit with Britain's Prime Minister Rishi Sunak of the Imperial College University, in London, Britain, February 15, 2023. Microsoft co-founder Bill Gates believes the future top company in artificial intelligence will likely have created a personal digital agent that can perform certain tasks for people. Gates said there is a 50-50 chance that this future AI winner will be either a startup or a tech giant. Gates also discussed health efforts related to his work at the Bill & Melinda Gates Foundation, saying AI will accelerate innovations in the space and lead to more advanced drug development. Watch: Bill Gates says OpenAI's GPT is the most important tech advance since the 1980's","Microsoft co-founder Bill Gates reacts during a visit with Britain's Prime Minister Rishi Sunak of the Imperial College University, in London, Britain, February 15, 2023.  Microsoft co-founder Bill Gates believes the future top company in artificial intelligence will likely have created a personal digital agent that can perform certain tasks for people.  The technology will be so profound, it could radically alter user behaviors. ""Whoever wins the personal agent, that's the big thing, because you will never go to a search site again, you will never go to a productivity site, you'll never go to Amazon again,"" he said.  This yet-to-be developed AI assistant will be able to understand a person's needs and habits and will help them ""read the stuff you don't have time to read,"" Gates said Monday during a Goldman Sachs and SV Angel event in San Francisco on the topic of artificial intelligence.  Gates said there is a 50-50 chance that this future AI winner will be either a startup or a tech giant.  ""I'd be disappointed if Microsoft didn't come in there,"" Gates said. ""But I'm impressed with a couple of startups, including Inflection,"" he added referring to Inflection.AI, co-founded by former DeepMind executive Mustafa Suleyman.  It will take some time until this powerful future digital agent is ready for mainstream use, Gates said. Until then, companies will continue embedding so-called generative AI technologies akin to OpenAI's popular ChatGPT into their own products.  Gates also discussed health efforts related to his work at the Bill & Melinda Gates Foundation, saying AI will accelerate innovations in the space and lead to more advanced drug development.  Although the inner workings of the human brain is still a mystery to scientists, the Microsoft co-founder believes that humanity is getting close to creating helpful drugs to cure diseases like Alzheimer's, with human trials for the new drugs possibly taking place in 10 years.  He also likened the rise of generative AI technologies that can produce compelling text as a game-changer that will affect white-collar workers. Gates added that he believes that future humanoid robots that are cheaper for companies to use than human employees will greatly impact blue-collar workers, too.  ""As we invent these robots, we just need to make sure they don't get Alzheimer's,"" Gates said in jest.  Watch: Bill Gates says OpenAI's GPT is the most important tech advance since the 1980's","google, microsoft, cofounder, know, gates, future, ai, agent, workers, believes, search, bill, human, kill, amazon",2023-05-22 00:00:00
80,"AI won an art contest, and artists are furious",Rachel Metz,https://www.cnn.com/2022/09/03/tech/ai-art-fair-winner-controversy/index.html,"CNN Business —Jason M. Allen was almost too nervous to enter his first art competition. Courtesy Jason M. AllenAllen’s winning image looks like a bright, surreal cross between a Renaissance and steampunk painting. Midjourney is one of a growing number of such AI image generators — others include Google Research’s Imagen and OpenAI’s DALL-E 2. “This is the literal definition of ‘pressed a few buttons to make a digital art piece’,” another Tweeted. Allen is glad the debate over whether AI can be used to make art is capturing so much attention.","CNN Business —  Jason M. Allen was almost too nervous to enter his first art competition. Now, his award-winning image is sparking controversy about whether art can be generated by a computer, and what, exactly, it means to be an artist.  In August, Allen, a game designer who lives in Pueblo West, Colorado, won first place in the emerging artist division’s “digital arts/digitally-manipulated photography” category at the Colorado State Fair Fine Arts Competition. His winning image, titled “Théâtre D’opéra Spatial” (French for “Space Opera Theater”), was made with Midjourney — an artificial intelligence system that can produce detailed images when fed written prompts. A $300 prize accompanied his win.  “I’m fascinated by this imagery. I love it. And it think everyone should see it,” Allen, 39, told CNN Business in an interview on Friday.  In August, Jason M. Allen's piece ""Théâtre D'opéra Spatial"" — which he created with AI image generator Midjourney — won first place in the emerging artist division's ""digital arts/digitally-manipulated photography"" category at the Colorado State Fair Fine Arts Competition. Courtesy Jason M. Allen  Allen’s winning image looks like a bright, surreal cross between a Renaissance and steampunk painting. It’s one of three such images he entered in the competition. In total, 11 people entered 18 pieces of art in the same category in the emerging artist division.  The definition for the category in which Allen competed states that digital art refers to works that use “digital technology as part of the creative or presentation process.” Allen stated that Midjourney was used to create his image when he entered the contest, he said.  Midjourney is one of a growing number of such AI image generators — others include Google Research’s Imagen and OpenAI’s DALL-E 2. Anyone can use Midjourney via Discord, while DALL-E 2 requires an invitation, and Imagen has not been opened up to users outside Google.  The newness of these tools, how they’re used to produce images, and, in some cases, the gatekeeping for access to some of the most powerful ones has led to debates about whether they can truly make art or assist humans in making art.  This came into sharp focus for Allen not long after his win. Allen had posted excitedly about his win on Midjourney’s Discord server on August 25, along with pictures of his three entries; it went viral on Twitter days later, with many artists angered by Allen’s win because of his use of AI to create the image, as a story by Vice’s Motherboard reported earlier this week.  “This sucks for the exact same reason we don’t let robots participate in the Olympics,” one Twitter user wrote.  “This is the literal definition of ‘pressed a few buttons to make a digital art piece’,” another Tweeted. “AI artwork is the ‘banana taped to the wall’ of the digital world now.”  Yet while Allen didn’t use a paintbrush to create his winning piece, there was plenty of work involved, he said.  “It’s not like you’re just smashing words together and winning competitions,” he said.  You can feed a phrase like “an oil painting of an angry strawberry” to Midjourney and receive several images from the AI system within seconds, but Allen’s process wasn’t that simple. To get the final three images he entered in the competition, he said, took more than 80 hours.  First, he said, he played around with phrasing that led Midjourney to generate images of women in frilly dresses and space helmets — he was trying to mash up Victorian-style costuming with space themes, he said. Over time, with many slight tweaks to his written prompt (such as to adjust lighting and color harmony), he created 900 iterations of what led to his final three images. He cleaned up those three images in Photoshop, such as by giving one of the female figures in his winning image a head with wavy, dark hair after Midjourney had rendered her headless. Then he ran the images through another software program called Gigapixel AI that can improve resolution and had the images printed on canvas at a local print shop.  Allen is glad the debate over whether AI can be used to make art is capturing so much attention.  “Rather than hating on the technology or the people behind it, we need to recognize that it’s a powerful tool and use it for good so we can all move forward rather than sulking about it,” Allen said.  Cal Duran, an artist and art teacher who was one of the judges for the competition, said that while Allen’s piece included a mention of Midjourney, he didn’t realize that it was generated by AI when judging it. Still, he sticks by his decision to award it first place in its category, he said, calling it a “beautiful piece”.  “I think there’s a lot involved in this piece and I think the AI technology may give more opportunities to people who may not find themselves artists in the conventional way,” he said.  Allen won’t yet say what the text prompt was behind his winning image — he’s planning to keep it a secret until he publishes a larger related work that he hopes will be finished later this year.","midjourney, competition, artists, images, image, winning, allen, piece, art, won, digital, furious, contest, ai",2022-09-03 00:00:00
81,Getting Enterprise AI Strategy Right,Charles Humble,https://winder.ai/enterprise-ai-strategy/,"If you’ve read other AI strategy articles you’ll be familiar with this sort of thing—vision, value, risks, etc—all generic and not helpful. Key Enterprise AI techniquesWith all this in mind, we’ll look at the two main AI techniques that are being applied in the enterprise: generative AI, and deep and reinforcement learning. McKinsey’s latest Global Survey on the state of AI saw 65% of respondents say their organisations are regularly using generative AI in at least one business function. With over a decade of real-world experience, Winder is able to provide holistic AI strategy consulting and implementation services. Whether you are looking for help to formulate your AI strategy or to bolster your company-wide AI initiatives, please get in touch.","AI offers considerable benefits across almost every commercial enterprise, from efficiency gains to the potential for brand new products. But it also represents a threat, opening up possibilities for new entrants to disrupt established markets.  In business environments, company executives look to produce an ‘AI Strategy’ but can lose sight of their objectives, rather like a politician thinking, “Something needs to be done and this is something, so let’s do this.”  The situation is made more complex by the fact that the term artificial intelligence (AI) covers a broad and continuously evolving range of technologies. Add to this the fact that management strategy is a sector bursting with terrible advice. If you’ve read other AI strategy articles you’ll be familiar with this sort of thing—vision, value, risks, etc—all generic and not helpful.  One exception, widely regarded as a business classic, is Richard Rumelt’s book, Good Strategy Bad Strategy. Though somewhat US-centric it contains a great deal of practical advice. Rumelt states that, “A good strategy has an essential logical structure that I call the kernel. The kernel of a strategy contains three elements: a diagnosis, a guiding policy, and coherent action. The guiding policy specifies the approach to dealing with obstacles called out in the diagnosis. It is like a signpost, marking the direction forward but not defining the details of the trip. Coherent actions are feasible coordinated policies, resource commitments, and actions designed to carry out the guiding policy.”  Rumelt makes the point that strategies focus on some objectives rather than others. In his model, a great deal of strategy work involves trying to figure out exactly what is going on and requires leaders to make often difficult choices. Given this, I’ve found that it’s important for an organisation to have a well-defined purpose and values, since these act as filters when trying to decide between competing priorities.  I’ve also found Wardley Mapping to be an invaluable sense-making tool when trying to understand a competitive landscape. It depersonalises a discussion—essentially allowing you to say it’s the map that is wrong rather than an individual—which can help with reaching a consensus and moving forward. A 13th century theory known as Condorcet’s paradox states that within any voting system it is logically impossible to guarantee that the winner has a majority of the vote, because it is possible that no such winner exists. This explains why building consensus can be so difficult.  David Anderson’s book, The Value Flywheel Effect, describes how he applied Wardley Mapping and other sense-making techniques at Liberty Mutual to transform a 100-year-old insurance company into a modern software start-up; I’ve discussed his experiences as a part of a leadership podcast series I’m hosting for GOTO.  Rumelt’s approach is effective. By the same token, I have never seen a good strategy arise from filling in a template. Given this, it is simply not possible for us to recommend an AI strategy for your business via a blog post—only you can make the necessary diagnosis. But what we can do is give you a sense of where AI might either solve a problem in your organisation or allow you to gain a competitive advantage.  Much of the content on our website is deeply technical. This piece is intended to provide a higher-level overview, informed by our decades of experience in the many different facets of applied AI. If you want to get deeper into a topic, we’ll provide links to further information.  Of course, if what you want is a generic template, then Google “vision mission strategy”, pick one of the thousands of examples and fill it in. It won’t help, but you will be doing something.  Foundations  Data science  At its core, AI is a data-driven set of practices. As with management information, the effectiveness of the recommendations and decisions made by your AI is dependent on the quality of its data.  Our CEO and founder, Phil Winder, has said that he considers AI to be “a child of data science, which is an overarching scientific field that investigates data generated by phenomena”. This means that your organisation’s data is fundamental to the success of any initiative you might pursue using AI.  Practical applications of AI—such as fraud detection in financial services, diagnosis in healthcare, dynamic pricing models in retail, or energy management in a data centre—require the use of an organisation’s unique data to gain competitive advantage. Finding suitable data and, for supervised learning labelling it, is time-consuming but necessary. Any successful AI project requires some ground truth that you can reference to quantify performance.  Security  The use of AI introduces new attack vectors. So, the scope of data governance needs to encompass not only privacy, but also risks arising from data poisoning during training, and model hijacking when a system is in production. Organisations need robust practices in place to be able to trace data lineage and maintain quality. The still somewhat nascent field of explainable AI will, we think, be critical here.  MLOps  MLOps is the process of scaling, maintaining and fulfilling regulatory requirements for ML applications within enterprises. It is becoming more widely adopted as more companies use AI for a broader spectrum of applications.  MLOps has much in common with DevOps practices, but a combination of factors make it distinct. These include:  Data scientists are not software engineers and may not be experts in writing applications. Many practices that are taken entirely for granted in the realm of software development—such as version control and CI/CD—are not necessarily adhered to in the context of an AI project. While many enterprise systems comprise code and data, the specific way that data is intertwined with AI models—either constantly learning and adapting to new inputs or not, as the case may be—makes verification and monitoring difficult. Testing AI technologies is different from other forms of software because you are operating in a much more stochastic/ probabilistic space. That lack of predictability around output testing is especially challenging; it isn’t obvious how to do red teaming, for example. For this reason we recommend a phased deployment—from experimentation to internal use, to a closed public beta—before any large-scale public deployment.  MLOps focuses on the standardisation and streamlining of machine learning life cycle management in an enterprise setting. It covers auditing, logging, monitoring and governance for regulatory compliance, and helps organisations to manage risk throughout the ML development lifecycle. A good MLOps practice also includes experiment tracking and packaging to make deployment easier and more scalable, and model management to govern your AI inventory.  Key Enterprise AI techniques  With all this in mind, we’ll look at the two main AI techniques that are being applied in the enterprise: generative AI, and deep and reinforcement learning.  Generative AI  Generative AI, which creates new content such as text and images from unstructured data, has seen a huge surge of interest since the start of 2022, and correspondingly huge investment. Mckinsey identifies generative AI as one of its key trends for 2024 alongside electrification and renewables, and we are starting to see real-world uses.  As a result, the corresponding pace of innovation has been remarkable. The size of prompts, known as ‘context windows’, that large language models (LLMs) can process, has grown dramatically. And generative AI has developed to encompass text summarisation, image generation and capabilities around video and audio.  LLM foundation models are being integrated into enterprise software tools for a range of purposes including automatic transcription, customer-facing chatbots and generating advertising campaigns. McKinsey’s latest Global Survey on the state of AI saw 65% of respondents say their organisations are regularly using generative AI in at least one business function.  Within generative AI we’re seeing three core trends:  Powerful open source models that are comparable to, and in some cases better than, those offered by commercial vendors. Expanding context windows that allow models to hold larger numbers of tokens (which you can think of more or less as words). This means you can give a model a large novel like War and Peace and ask it to find something within it. Multimodel generative models that can process any modality including audio, video, images, code and language.  Generative AI can be combined with traditional data science and Natural Language Processing (NLP) techniques for document clustering, topic identification, classification and summarisation. This may be lower cost and more effective for solving a part of your use case.  In the real world  Walmart says, “We’ve used multiple large language models to accurately create or improve over 850 million pieces of data in the catalogue. Without the use of generative AI, this work would have required nearly 100 times the current head count to complete in the same amount of time.” Reckitt is using generative AI to get advertising ideas, product insights and media analysis. ING has leveraged generative AI to improve customer service in the Netherlands. Nubank is also piloting a gen AI virtual assistant to boost customer service. Programming website, InfoQ, was able to offer transcripts for video content as a result of improvements to machine-generated transcription.  One area where generative AI tools are showing particular promise is in software development. According to the 2024 Docker State Of Application Development Report, 64% of respondents use AI for tasks such as writing code, documentation and research, and 46% work on ML in some capacity.  AI code generation tools, such as aider, Amazon Q, CodiumAI, Continue, GitHub Copilot and JetBrains AI, go beyond recommendations by enabling developers to generate entire functions and boilerplate code. They are particularly useful when adapting to new languages and frameworks, or migrating to a newer version of language or runtime. For example, Amazon’s CEO, Andy Jassy, has said that by using Amazon Q, the “average time to upgrade an application to Java 17 plummeted from what’s typically 50 developer days to just a few hours”.  AI tools also show promise in areas such as automated code review and testing.  Whilst the developer tools tend to be more mature, we suspect that all aspects of software development will gradually benefit from pragmatic use of AI and derived tools, and we’re actively following innovations across the development landscape. We continue to be interested in AI-assisted terminals, tools that turn screenshots and designs into code, and the use of LLM as part of ChatOps processes, such as QueryPal and Kubiya.  Generative AI tools also show some promise for a new type of search—if you have, for example, complex equipment where field engineers regularly have to find and diagnose problems, a small language model trained on your manuals may prove invaluable.  Challenges  The lack of availability for local language support hampers global adoption. Some countries such as China, India, Japan and many countries in the middle east are developing their own foundation models, but given the costs and compute involved this is harder in countries that are less well-resourced.  The majority of highly capable libraries and frameworks, such as LangChain and LlamaIndex, tend to be built around Python, which many enterprises do not run in production. However, this is starting to change with tools such as Spring AI, which offers a generative AI framework for Java developers.  There are a number of other uncertainties:  Inaccuracies in results, aka ‘hallucinations’, are the most widely recognised risk. Cybersecurity and privacy concerns around data leakage, including customer details and other protected data. Ethical issues surround the responsible use of generative AI. Copyright ownership of content generated by models remains an unanswered question. The environmental impact of training models is high and likely to rise. After years of relatively modest growth in carbon emissions from data centres, Microsoft reported in May that its total carbon emissions have risen nearly 30% since 2020, primarily due to the construction of data centres to meet its push into AI. Google’s emissions have surged nearly 50% since 2019. They also increased 13% year on year in 2023, according to their own annual report. The company attributed the emissions spike to an increase in data centre energy consumption and supply chain emissions driven by artificial intelligence. The report noted that the company’s total data centre electricity consumption grew 17% in 2023. A growing amount of legislation, including the EU AI Act, is worth noting for researchers and organisations looking to use the technology. This and a desire to maintain sovereignty over their data is forcing many Winder clients to build privately-hosted generative text and image based applications.  We are starting to see some patterns emerging for common uses and contexts. NeMo Guardrails is an open source toolkit for easily adding programmable guardrails to LLM-based conversational applications. Langfuse allows greater observability into the steps leading to an LLM’s output. Helix is an open source competitor to ChatGPT that can be run on your own infrastructure. Winder is a major contributor to it.  Retrieval-augmented generation (RAG) is our preferred pattern for our teams to improve the quality of responses generated by an LLM. RAG integrates an external repository of information to enhance the AI’s ability to generate content that is accurate, relevant and contextually rich.  Much of the hype with generative AI tools is around content creation, but the results for this tend to be very generic and there is something of a pushback against ‘AI slop’. There might be places where use is acceptable but it is probably cheaper and more effective to use human copywriters and illustrators where possible.  Finally, we should note that it isn’t clear how far along the s-curve we actually are but some AI researchers believe we may be close to the peak of what LLMs can accomplish technically, at least on their own. However, that doesn’t mean we’ve exhausted potential use cases—far from it.  Deep and Reinforcement Learning  Deep learning, reinforcement learning and computer vision techniques are being applied in a huge variety of ways, across numerous industries.  Reinforcement learning is particularly effective in situations where you are looking to optimise for long-term, multistep rewards and it is also useful where you want to incorporate business metrics.  For example, internet advertising is over-optimised around click through rates (CTR), but CTR is a poor choice since the goal is more likely a more substantive interaction such as a sale or a sign-up. Reinforcement learning can be used to optimise for a combination of factors—such as which advertisements are shown, in what order and with what content—in order to generate the desired outcome.  In the real world  Healthcare - RapidAI is using applied AI with advanced imaging technology to expand the treatment window for ischemic stroke, the most common type of stroke. Oil and Gas - Aramco has built an AI hub to analyse more than five billion data points per day from wellheads in the oil and gas fields. This has enhanced the understanding of petrophysical properties and expedited decision-making in exploration and drilling. One example is reduced flaring; combined with other techniques, their AI Model has contributed to reducing total flaring by more than 50% since 2010, helping them achieve one of the industry’s lowest flaring rates. IT - DeepMind used its AI models to reduce Google’s data centre cooling bill by 40%.  Many forms of algorithmic training in finance apply a combination of deep learning, neural networks and reinforcement learning in an attempt to discover, and then exploit, statistical inefficiencies in financial markets.  Other areas include environmental research, medical research and material science. DeepMind has released AlphaFold 3, which can predict the structures of all the molecules in the human body, not just proteins. In 2021, they spun off Isomorphic Labs to look for new drugs to treat diseases. They continue to use reinforcement learning to build new AI agents.  As the growing use of renewable energy makes balancing the grid more complex, there are research projects using machine learning to help make calculations and predictions. Rhizome, a start-up based in Washington, DC, launched an AI system that takes utility companies’ historical data on the performance of energy equipment, and combines it with global climate models to predict the probability of grid failures caused by extreme weather events, such as snowstorms or wildfires.  Several utility companies are already integrating AI into critical operations, particularly inspecting and managing physical infrastructure such as transmission lines and transformers. For example, overgrown trees are a leading cause of blackouts, due to branches falling on electric wires or sparking fires. Traditionally, manual inspection has been the norm, but given the extensive span of transmission lines, this can take several months. PG&E, covering northern and central California, has been using machine learning to accelerate those inspections. By analysing photographs captured by drones and helicopters, machine learning models identify areas requiring tree-trimming or pinpoint faulty equipment that needs repairing.  Bringing it all together  Rumelt says that, “A good strategy grows out of an independent and careful assessment of the situation, harnessing individual insight to carefully crafted purpose. Bad strategy follows the crowd, substituting popular slogans for insights.”  The explosion of hype around the current AI tools means that everyone is paying attention to the field. However, it remains poorly understood. Learning more about AI will help you to identify places where the current set of tools may be most applicable to your organisation.  It is often surprising how effective even imperfect AI is. Many companies are already using data to derive insights to automate processes, make better decisions and transform business outcomes.  Since the field is still new, the more you can democratise its use within your organisation, the more likely it is you will make a demonstrable difference to your business. This requires your organisation to allow experimentation at scale, with learning and psychological safety at the core.  While AI continues to progress through a massive hype cycle, you also need to ensure you are doing your technical due diligence. If someone is claiming they can help you, consider how long they’ve been involved in the AI field and where they’ve come from. If they were doing crypto last year, be sceptical. With over a decade of real-world experience, Winder is able to provide holistic AI strategy consulting and implementation services. Whether you are looking for help to formulate your AI strategy or to bolster your company-wide AI initiatives, please get in touch.","models, code, software, tools, generative, using, right, getting, data, strategy, learning, enterprise, ai",2024-09-13 14:00:05+01:00
82,A Comparison of Open Source LLM Frameworks for Pipelining,"Natalia Kuzminykh, Phil Winder",https://winder.ai/comparison-open-source-llm-frameworks-pipelining/,"Integrating an open source LLM framework into your project doesn’t have to be difficult or expensive, thanks to the variety of LLMOps tools available today. To help with this, we have compiled a list of the best LLM frameworks and community-approved LLM technologies for various stages of model development and skill levels. LLM Framework Evaluation CriteriaIn our evaluation of LLM frameworks, we combined subjective analysis with some objective metrics, primarily sourced from GitHub. The modular structure of LangChain allows easy comparison of different prompts and AI models, minimizing the need for extensive code modifications. add_component( ""llm"" , llm) rag_pipeline .","Integrating an open source LLM framework into your project doesn’t have to be difficult or expensive, thanks to the variety of LLMOps tools available today. Open-source LLM orchestration frameworks offer practical solutions for specific business challenges and come with the additional benefit of a big supportive community. However, these tools each have their own set of pros and cons.  Firstly, it’s important to understand that “free” doesn’t necessarily mean without any cost. LangChain, for example, offers both the LangSmith platform for optimization and the LangGraph component for building complex, controllable AI-driven flows as open-source. Yet, some tools may provide only one of these components as open-source while keeping the other proprietary. Also, don’t forget that although many LLMOps are free in terms of licensing, the long-term expenses for hosting a finished application and maintaining the backend can add up, so it’s worth considering this before you commit to your chosen LLMOps tool.  Another essential consideration is that such a library or framework should be able to seamlessly integrate with your existing LLM architecture. To help with this, we have compiled a list of the best LLM frameworks and community-approved LLM technologies for various stages of model development and skill levels.  LLM Framework Evaluation Criteria  In our evaluation of LLM frameworks, we combined subjective analysis with some objective metrics, primarily sourced from GitHub. We closely examined repository statistics, such as the number of stars, to estimate the framework’s popularity. However, it’s important to note that this metric can be misleading, as a higher number of stars may simply reflect more effective marketing strategies.  Our evaluation criteria included modularity, ease of use, flexibility and maturity. We also considered simplicity, although it sometimes conflicts with modularity and flexibility. We formed our opinions by weighing these criteria against each other in order to make an informed decision.  This article is divided into three main sections:  Overview of Top LLM Frameworks  License: MIT  MIT Stars: 89.3k  89.3k Contributors: 2,939  2,939 Current Version: 0.2.20  LangChain is a versatile open-source LLM orchestration framework designed to simplify the development of AI applications. It serves as a unified platform, providing a cohesive environment where developers can seamlessly develop and integrate popular large language models with external data sources and software workflows.  For example, suppose you want to build a QA chatbot that can guide you through information from sources like Slack chats, PDFs or CSV files. With LangChain, you can easily achieve this by selecting an appropriate data loader or adapting one from Llama Hub. You can then define the best vector database provider, whether cloud-based or local, and incorporate monitoring tools such as LangSmith.  Langchain is a modular python LLM library. The modular structure of LangChain allows easy comparison of different prompts and AI models, minimizing the need for extensive code modifications. This flexibility is especially useful for combining multiple LLMs within a single environment, reducing costs and ensuring smooth fallbacks from one model to another if there are unexpected challenges.  High level architecture of LangChain (source)  Installing LangChain  The core code is freely available on GitHub. To install it in Python, please run:  pip install langchain  If you need all the dependencies for LangChain, rather than selectively installing them, use:  pip install langchain [ all ]  Adding popular large language models is typically straightforward and often requires just an API key from the provider. The LLM class offers a standardized interface for all supported models. Note that while proprietary models from providers like OpenAI or Anthropic may come with associated costs, many open-source models such as Mixtral or Llama, are easily accessible through Hugging Face.  Another key feature of LangChain is its use of chains , which combine LLMs with other components to perform sequential tasks. In version 1, widely used chains like LLMChain and ConversationalRetrievalChain are prevalent. However, the latest version, v2, introduces a new approach that encourages the creation of custom chains through LangChain Expression Language (LCEL) and the Runnable protocol.  from langchain_openai import ChatOpenAI from langchain_core.prompts import ChatPromptTemplate from langchain_core.output_parsers import StrOutputParser prompt_template = ""tell me a joke about {topic} "" prompt = ChatPromptTemplate . from_template(prompt_template) llm = ChatOpenAI() chain = prompt | llm | StrOutputParser() chain . invoke({ ""topic"" : ""bears"" })  The example above demonstrates how to initialize a model alongside a prompt template. By saving a prompt as prompt_template and using ChatOpenAI() , you can create a processing chain that generates a joke from a given topic. The chain is defined as chain = prompt | llm | StrOutputParser() . Here, the | symbol acts similarly to a Unix pipe operator, seamlessly taking output from one component and passing it as input to the next. So, in this sequence:  The user’s request is fed into the prompt template. The prompt template’s output is then processed by the OpenAI model. The model’s output is handled by the output parser.  To run the chain with a specific input, you simply call chain.invoke({""topic"": ""bears""}) .  License: MIT  MIT Stars: 33.7k  33.7k Contributors: 1,088  1,088 Current Version: v0.10.55  If you’re familiar with LlamaIndex, you’ll notice its similarities to the LangChain library. However, it stands out in its performance for search and retrieval tasks. Its effectiveness in indexing and querying data makes it an excellent choice for projects requiring robust search capabilities.  High level architecture of LlamaIndex (source)  One of the key advantages that LlamaIndex has over LangChain in RAG is its enhanced schema for loaded data.  LlamaIndex offers a more detailed and structured metadata schema that includes file-specific information such as the file name, type and size, as well as creation and modification dates. Additionally, it supports exclusion lists for metadata keys that should be ignored during embedding and LLM processing, providing flexibility in selecting which information is utilized. Furthermore, it enables customizable templates for both text and metadata, granting users greater control over how document information is presented.  LangChain’s Document Schema  [Document( metadata = { 'source' : '/content/data/text.txt' }, page_content = '    What I Worked On    February 2021    Before college the two main things I worked on, outside of school, were writing and programming....Thanks to Trevor Blackwell, John Collison, Patrick Collison, Daniel Gackle, Ralph Hazell, Jessica Livingston, Robert Morris, and Harj Taggar for reading drafts of this.' )]  LlamaIndex’s Document Schema  [Document( id_ = '972f6e28-6a0f-43a4-9e1e-6df1c4373987' , embedding = None , metadata = { 'file_path' : '/content/data/text.txt' , 'file_name' : 'text.txt' , 'file_type' : 'text/plain' , 'file_size' : 75393 , 'creation_date' : '2024-07-17' , 'last_modified_date' : '2024-07-16' }, excluded_embed_metadata_keys = [ 'file_name' , 'file_type' , 'file_size' , 'creation_date' , 'last_modified_date' , 'last_accessed_date' ], excluded_llm_metadata_keys = [ 'file_name' , 'file_type' , 'file_size' , 'creation_date' , 'last_modified_date' , 'last_accessed_date' ], relationships = {}, text = ' \r  \r  What I Worked On \r  \r  February 2021 \r  \r  Before college the two main things I worked on, outside of school, were writing and programming...Thanks to Trevor Blackwell, John Collison, Patrick Collison, Daniel Gackle, Ralph Hazell, Jessica Livingston, Robert Morris, and Harj Taggar for reading drafts of this.' , mimetype = 'text/plain' , start_char_idx = None , end_char_idx = None , text_template = ' {metadata_str}    {content} ' , metadata_template = ' {key} : {value} ' , metadata_seperator = '  ' )]  If you decide to use this module, be aware that LlamaIndex documentation can, however, be unreliable. The library undergoes frequent changes, which means that you could spend a significant amount of time resolving inconsistencies and compatibility issues when following official documentation or tutorials.  Installing LlamaIndex  LlamaIndex supports both Python and TypeScript, with OpenAI’s GPT-3.5-turbo as its default language model. To get started, you need both to set up your API key as an environment variable and ensure that you installed the library correctly.  For macOS and Linux, use the following command:  export OPENAI_API_KEY = YOUR_API_KEY  On Windows, use:  set OPENAI_API_KEY = YOUR_API_KEY  To install the Python library, run:  pip install llama-index  Configuring LlamaIndex Documents  Place your documents in a folder named data . Then call a SimpleDirectoryReader loader and a VectorStoreIndex to store them in memory as a series of vector embeddings:  from llama_index.core import VectorStoreIndex, SimpleDirectoryReader # Load documents from the data folder documents = SimpleDirectoryReader( ""data"" ) . load_data() # Create an index from the loaded documents index = VectorStoreIndex . from_documents(documents)  Now you can create an engine for Q&A over your index and ask a simple question.  # Create a query engine from the index query_engine = index . as_query_engine() # Ask a question response = query_engine . query( ""What did the author do growing up?"" ) print(response)  You should receive a response similar to: “The author wrote short stories and tried to program on an IBM 1401.”  License: Apache-2.0  Apache-2.0 Stars: 14.7k  14.7k Contributors: 262  262 Release: v2.3.0  Haystackis often favored for its simplicity and is frequently chosen for lighter tasks or quick prototypes. It’s particularly useful for developing large-scale search systems, QAs, summarization and conversational AI applications.  Since its launch in 2017, Haystack has evolved into a powerful tool, particularly excelling in semantic search. But, unlike simple keyword matching, it understands the context of users’ queries. It also has specialized components for various tasks, enabling it to manage everything from data ingestion to result generation. This significantly sets it apart from more general-purpose frameworks like LlamaIndex and LangChain.  One of Haystack’s strengths lies in its extensive documentation and active community, which simplifies the onboarding process for new users and provides ample support. Despite its focus on document understanding and retrieval tasks, which could be seen as a limitation compared to the broader capabilities of other frameworks, Haystack is ideal for enterprise-level search. It’s particularly well-suited to industries requiring precise and contextual information retrieval, such as finance, healthcare and legal sectors. Its specialization also makes it a strong candidate for knowledge management systems, helping organizations provide accurate and contextual information to users.  Haystack (source)  Installing Haystack  To get started with Haystack, install the latest release using pip:  pip install --upgrade pip pip install farm-haystack [ colab,inference ]  Then create a QA system with a DocumentStore, which stores the documents used to find answers. For this case, we use the InMemoryDocumentStore , which is simple to set up and suitable for small projects and debugging. However, it doesn’t scale well for larger document collections, so it’s not ideal for production systems.  import os from haystack import Pipeline, Document from haystack.utils import Secret from haystack.document_stores.in_memory import InMemoryDocumentStore from haystack.components.retrievers.in_memory import InMemoryBM25Retriever from haystack.components.generators import OpenAIGenerator from haystack.components.builders.answer_builder import AnswerBuilder from haystack.components.builders.prompt_builder import PromptBuilder # Write documents to InMemoryDocumentStore document_store = InMemoryDocumentStore() document_store . write_documents([ Document(content = ""My name is Jean and I live in Paris."" ), Document(content = ""My name is Mark and I live in Berlin."" ), Document(content = ""My name is Giorgio and I live in Rome."" ) ])  Next we initialize an InMemoryBM25Retriever , which sifts through all the documents and returns the ones relevant to the question. Afterwards, we set up a RAG pipeline to use a prompt template to generate answers based on retrieved documents and the input question.  # Build a RAG pipeline prompt_template = """""" Given these documents, answer the question. Documents: { % f or doc in documents %} {{ doc.content }} { % e ndfor %} Question: {{question}} Answer: """""" retriever = InMemoryBM25Retriever(document_store = document_store)  Finally, with a run() method, you could test your app and ask a question.  prompt_builder = PromptBuilder(template = prompt_template) llm = OpenAIGenerator(api_key = Secret . from_token(api_key)) rag_pipeline = Pipeline() rag_pipeline . add_component( ""retriever"" , retriever) rag_pipeline . add_component( ""prompt_builder"" , prompt_builder) rag_pipeline . add_component( ""llm"" , llm) rag_pipeline . connect( ""retriever"" , ""prompt_builder.documents"" ) rag_pipeline . connect( ""prompt_builder"" , ""llm"" ) # Ask a question question = ""Who lives in Paris?"" results = rag_pipeline . run({ ""retriever"" : { ""query"" : question}, ""prompt_builder"" : { ""question"" : question}, }) print(results[ ""llm"" ][ ""replies"" ])  Overview of Low-Code LLM Projects  License: MIT  MIT Stars: 12.3k  12.3k Contributors: 182  182 Release: v12.30.9  Botpress is a powerful LLM platform designed to help you create highly customizable chatbots across various channels. It features a flow builder and integrated AI, allowing users to design custom flows with AI capabilities. With Botpress, your chatbot can be trained on personalized data, automatically translate messages, summarize conversations and perform other tasks.  The platform’s low-code approach means you can easily set up your bot without needing to code, while its user-friendly interface makes it easy to handle a variety of tasks within the app, such as booking events, placing orders and managing support cases. The conversation studio in Botpress further simplifies the process by allowing you to drag and drop blocks to create conversational experiences.  Botpress is available as a free, open-source platform, with an Enterprise version for larger businesses that need additional features like Single Sign-On and enhanced role-based access control. The visual interface supports modern software practices, including version control, emulating, and debugging, making chatbot development as straightforward as building any other application.  Installing Botpress  Constructing a bot in BotPress (source)  To start building your chatbot with Botpress, follow these steps:  Create an account: You can do this by simply following the instructions on the Botpress website. Create a new bot: After logging in, click on the Create Bot option and select the newly-created bot. Next click the Open in Studio button to start editing your chatbot. In Botpress, each chatbot is part of a workspace. When you first connect to Botpress Cloud, a default workspace is automatically created for you. Testing: To test your chatbot, you can use a chat emulator to simulate user interactions and test different scenarios and potential edge cases. Debugging: For authenticated users, the bottom panel will provide additional information about the responses generated by your chatbot. This includes details such as the dialogue engine’s suggestions, natural language intents, and raw JSON payload for more in-depth analysis.  License: Mixed  Mixed Stars: 9.8k  9.8k Contributors: 92  92 Release: NA  Example bot usage in DAnswer (source)  Danswer aims to make workplace knowledge easily accessible through natural language queries, much like a chatbot. This library can be seamlessly used together with workplace platforms like Slack, Google Drive or Confluence, allowing teams to extract information from existing documents, code changelogs, customer interactions and other sources. By leveraging AI, Danswer understands natural language questions and provides accurate, context-relevant responses. Danswer’s capabilities are impressive, as demonstrated by its ability to:  accelerate customer support and resolution times boost efficiency,  help sales teams prepare for calls with detailed context.  This is achieved through a combination of: document search and AI-generated answers; custom AI assistants tailored to different team needs; and a hybrid search technology that combines keyword and semantic search for best-in-class performance. Additionally, The Danswer team prioritize user privacy and security with document-level access controls, plus the option to run locally or on a private cloud.  Installing Danswer  To launch Danswer’s platform on your local machine, you need to clone the repo with:  git clone https://github.com/danswer-ai/danswer.git  Navigate to the docker compose directory and initiate a Docker instance:  cd danswer/deployment/docker_compose docker compose -f docker-compose.dev.yml -p danswer-stack up -d --pull always --force-recreate  Alternatively, to build the containers from source:  cd danswer/deployment/docker_compose docker compose -f docker-compose.dev.yml -p danswer-stack up -d --build --force-recreate  The setup process can take up to 15 minutes, so don’t worry if it seems lengthy. Once setup is complete, you can access Danswer at http://localhost:3000.  License: Apache-2.0  Apache-2.0 Stars: 27.7k  27.7k Contributors: 120  120 Release: 1.8.4  Constructing a bot in Flowise (source)  Flowise is another low-code open-source UI platform built on top of LangChain.js. Similarly to LangChain, it focuses on creating customized LLM applications and AI agents. However, Flowise stands out with its user-friendly interface, allowing users to construct LLM orchestration flows and autonomous agents without needing extensive coding knowledge.  While Flowise shares the strengths of LangChain, making it a powerful library, it isn’t an independent tool. This dependency imposes certain limitations on building specific flows or combining different tools. For instance, integrating a LlamaIndex-based loader, which can be done with a few lines of code in LangChain, can be challenging in Flowise. Additionally, Flowise relies on LangChain for updates, resulting in a delay with integrating new features introduced in the LangChain library.  Installing Flowise  Setting up Flowise is simple, whether you prefer using NodeJS directly or leveraging Docker for containerization. For example, to get started with Docker, you should navigate to the Docker folder at the root of the project. Then copy the .env.example file and rename it as .env . Afterwards, you could use Docker Compose to spin up the containers with docker compose up -d . The Flowise application will be available at http://localhost:3000 . When you’re done, you can stop the containers with docker compose stop.  Alternatively, you can build a Docker image locally using docker build --no-cache -t flowise . and run it with docker run -d --name flowise -p 3000:3000 flowise . Stopping the Docker image is just as simple with docker stop flowise.  For added security, Flowise also supports app-level authentication. You can enable this by adding FLOWISE_USERNAME and FLOWISE_PASSWORD to the .env file in the packages/server directory:  FLOWISE_USERNAME = user FLOWISE_PASSWORD = 1234  License: Apache-2.0  Apache-2.0 Stars: 37.9k  37.9k Contributors: 310  310 Release: v0.6.14  Example of building a dify bot (source)  Dify is a user-friendly platform that seamlessly integrates BaaS and LLMOps principles to manage data operations and swiftly build production-level applications.  This library offers a comprehensive technology stack, featuring various models, a prompt orchestration interface, and even a flexible agent framework. Furthermore, its intuitive interface and API are designed to minimize time spent on repetitive tasks, allowing developers to focus on business needs.  For those eager to leverage the advancements in LLM technology like GPT-4 but who are unsure how to start, Dify Cloud provides a practical solution. It addresses common issues such as training models with proprietary data, keeping AI up to date with recent events, preventing misinformation, and understanding complex concepts like fine-tuning and embedding. Dify Cloud also enables users to build AI applications that are not only functional but are also secure and reliable, ensuring full control over private data and enhancing domain-specific expertise.  Installing Dify  As with Flowise, this low-code solution could be launched via the Docker instance. Once you download the GitHub repo, you should navigate to the directory where the Docker setup files are located and run:  git clone https://github.com/langgenius/dify.git cd dify/docker cp .env.example .env docker compose up -d  After starting Dify, you need to ensure all containers are running correctly. To check the status of the containers, use:  docker compose ps  You should see several containers listed, each with a status indicating they are up and running. Key services include:  api: Main application interface  Main application interface worker: Background task handler  Background task handler web: Web interface  Web interface weaviate, db, redis, nginx, ssrf_proxy, sandbox: Supporting components  Now that Dify is running, you can access it via your web browser: http://localhost/install  License: Custom  Custom Stars: 287  287 Contributors: 8  8 Release: v0.6.14  Helix’s on-premise LLM serving dashboard (source)  Helix is co-developed by the Winder.AI team. It is a powerful platform that simplifies the deployment and orchestration of large language models. Helix is designed to help users build, train, and deploy AI models with ease, making it an ideal choice for businesses looking to leverage AI technologies without the need for extensive expertise.  One of the key differentiators is the focus on hardware (i.e. machines with GPUs) orchestration. Helix provides a simple and intuitive interface for managing GPU resources, allowing users to easily scale their AI models based on their needs. Like other LLMOps tools, Helix supports a wide range of proprietary LLMs, however Helix focuses on the use of popular open-source LLMs hosted on your own infrastructure.  This also makes Helix a great choice for businesses that are sensitive about the location of their data. By providing on-premise deployment options, Helix ensures that sensitive data remains secure and compliant with data privacy regulations and doesn’t leave your environment.  For users, the platform offers a novel “App” concept, which allows you to create and deploy AI models with just a few clicks. Backed by GitOps, Helix ensures that your AI applications are versioned and can be easily rolled back in case of issues.  Installing Helix  You can try Helix right now by visiting the Helix website. The platform is available as a cloud service, so you can get started without any installation or setup.  If you’re interested in deploying Helix on your own on-premise GPUs, you can follow the installation guide provided on the Helix website. If you need any help, then feel free to get in touch here or on Discord.  License: GPL-3.0  GPL-3.0 Stars: 18.9k  18.9k Contributors: 41  41 Release: 0.9.26  typesense landing page example (source)  Typesenseis a robust search engine, which is particularly appealing due to its handy APIs, exceptional search performance and easy deployment. As an open-source solution, Typesense presents a viable alternative to commercial services like Algolia and a more user-friendly option than Elasticsearch.  One of the most notable features of Typesense is its simplicity in setup. The API is designed to be intuitive, making it accessible for both novice and experienced developers. Another significant strength of Typesense is its typo tolerance, which ensures that users receive relevant search results even when there are spelling errors. This enhances the overall user experience and makes it easier for users to find what they’re looking for.  Typesense also excels at real-time indexing, which is crucial for applications that require immediate updates to search results. Its horizontal scalability makes it well-suited to manage large datasets and handle high query volumes. Additionally, Typesense allows for custom ranking, enabling developers to tailor search results to their specific needs. The inclusion of faceted search capabilities further simplifies the process of filtering and refining search results, making it a versatile and powerful tool for developers.  License: BSD-3-Clause  BSD-3-Clause Stars: 5k  5k Contributors: 20  20 Release: v1.0.3  Constructing a RAG pipeline in Verba (source)  Verba is a highly adaptable personal assistant designed to query and interact with your data, whether it’s stored locally or hosted in the cloud. It allows users to resolve questions about their documents, cross-reference multiple data points and derive insights from existing knowledge bases.  With the release of Verba 1.0, the platform has shown marked improvements in both information retrieval and response generation. Its semantic caching and hybrid search capabilities have greatly improved performance metrics, leading to higher query precision and response accuracy. The tool’s compatibility with multiple data formats, including PDFs, CSVs and unstructured data, adds to its versatility and value. Overall, Verba 1.0 addresses the challenges of precise information retrieval and context-aware response generation through advanced RAG techniques, making it a powerful asset in the AI toolkit for improving the quality and relevance of generated responses.  High level architecture of Inkeep (source)  Inkeep stands out as an innovative solution in the AI-driven support industry, aiming to reduce the volume of support requests for businesses by empowering users with self-help tools. To achieve a strong product-market fit, Inkeep integrates advanced analytics to provide detailed insights into user behavior, helping identify areas for content improvement. By expanding the range of supported content types and languages, Inkeep also makes its platform more flexible and appealing to diverse user groups.  To scale effectively for larger enterprises, Inkeep offers integration capabilities that allow seamless merging of existing enterprise systems and workflows. This focus on technical enhancements helps Inkeep differentiate itself from competitors like Intercom and Zendesk, especially through its advanced document search and contextual understanding features. Addressing technical challenges such as compatibility with diverse document formats, and ensuring high performance in content parsing, will further solidify Inkeep’s position as a leading AI solution.  Best LLM Frameworks for Your Project  In conclusion, selecting the best open-source LLM framework requires a careful balance of factors such as modularity, ease of use, flexibility and technological maturity. Each tool comes with its own set of strengths and limitations, making it essential to align your choice with your specific application needs and technical expertise.  By leveraging the community support and weighing the long-term costs, you can find an open-source solution that not only fits seamlessly into your existing tech stack but also scales effectively with their project’s growth. This comparative analysis provides a solid foundation to navigate the diverse landscape of LLM pipeline libraries and make an informed decision.","llm, stars, models, users, langchain, comparison, documents, source, open, frameworks, pipelining, data, search, ai, docker",2024-08-01 15:47:02+00:00
83,Exploring Small Language Models,Natalia Kuzminykh,https://winder.ai/exploring-small-language-models/,"Small language models (SLMs) offer a practical alternative. Defining Small Language Models (SLMs) and Exploring Their Use CasesOverall, the SLM is a type of neural network that generates natural language content. Highlighting the opportunities for SLMs when compared to large language models. Despite its relatively small size, Phi-2 approaches near-human performance in language processing, outperforming much larger models like GPT-4 in terms of training efficiency. More challenging questions can be directed towards more powerful, larger language models that are capable of handling complex queries.","While large language models are well-known for their ability to handle complex tasks, they also come with significant computational power and energy demands, making them less suitable for smaller organizations and devices with limited processing capacity.  Small language models (SLMs) offer a practical alternative. Designed to be more lightweight and resource-efficient, they’re ideal for applications that need to operate within limited computational environments. With fewer resource demands, SLMs are easier and quicker to deploy, reducing the time and effort required for maintenance.  Throughout this article, we’ll explore the various use cases of SLMs and discuss their advantages over LLMs. We’ll focus on: their efficiency, speed, robustness and security. And we’ll aim to understand why this type of AI model is becoming a popular choice for applications where large-scale models aren’t feasible.  Defining Small Language Models (SLMs) and Exploring Their Use Cases  Overall, the SLM is a type of neural network that generates natural language content. The term “small” refers not only to the physical size of the model, but also to the number of parameters it contains, its neural architecture, and the scope of the data used for its training.  Parameters are numerical values that guide a model’s analysis of input and creation of responses. A smaller number of parameters also means a simpler model, which requires less training data and consumes fewer computing resources.  The consensus among many researchers is that LMs with fewer than 100 million parameters are considered small, although the definition can vary. Some experts consider models with as few as one million to 10 million parameters to be small, in contrast to today’s larger models which can have hundreds of billions of parameters.  Highlighting the opportunities for SLMs when compared to large language models.  Summary of use cases of SLMs  Recent advancements with SLMs are driving their widespread adoption. These models, with their ability to generate a coherent response to specific contexts, have numerous applications.  One notable use case is text completion, where SLMs predict and generate text, assisting with tasks such as sentence completion and conversational prompts. This technology is also valuable for language translation—bridging linguistic gaps in real-time interactions.  In customer service, SLMs power chatbots and virtual assistants, allowing them to conduct natural and engaging conversations. These applications are essential for providing end-to-end assistance and handling routine inquiries, which enhances the customer experience and operational efficiency. In content creation, SLMs generate text for emails, reports and marketing materials. This saves significant time and resources, while maintaining content relevance and quality.  SLMs also analyse data, performing sentiment analysis to gauge public opinion and customer feedback. They aid in identifying named entities for better information organization and analyse market trends to optimize sales and marketing strategies. These capabilities enable businesses to make informed decisions, tailor customer interactions and innovate effectively in product development.  The Issues with LLMs  The reason why training an LLM is often more feasible for large organizations is due to three significant challenges: data, hardware and legal concerns.  Resource and energy use  First of all, it’s no secret that training LLMs is an intensive process that needs powerful machines. For example, training Google’s PaLM required a staggering 6,144 TPU v4 chips, while Meta AI’s OPT model, although comparatively more efficient, still used 992 Nvidia A100 GPUs of 80GB each. The scale of this hardware deployment often leads to failures, necessitating manual restarts throughout the lengthy training process. That not only makes it more complicated, but also adds to the cost of developing the software. Some rough estimates suggest figures as high as $23 million for training a single model.  The energy consumption involved in training these models is equally immense. Although specific details about the training process of GPT-4 remain undisclosed, we can refer to the energy consumption for GPT-3, which was nearly 1,300 MWh. That’s the equivalent of streaming Netflix for a staggering 1.6 million hours (using around 0.0008 MWh per hour).  Such figures highlight the vast disparity in energy use between daily activities and training advanced AI models. Although subsequent processes like inference consume considerably less energy, the initial training phase is particularly power-hungry and carbon-intensive.  Additionally, environmental impacts extend beyond power usage. Recent estimates highlight that the carbon footprint associated with training these models is akin to the electricity consumption of a US family over 120 years. While companies like Meta have taken steps to reduce this footprint, it remains a significant environmental concern.  Many details about the specific resource and energy requirements of LLMs still remain under wraps due to competitive secrecy among leading tech companies. This lack of transparency from major AI developers further complicates efforts to assess and address these impacts.  Copyright and Licensing  Access to vast datasets is another significant barrier for many businesses other than the tech giants like Google and Facebook, who dominate this field. This makes it difficult for smaller entities to compete. Many datasets, especially those scraped from the internet, contain copyrighted material, which raises ethical and legal concerns about the use of such data without proper authorization. For example, creators from various fields argue that their copyrighted works are being used to train AI without permission or compensation.  The conversation around copyright has evolved as AI technology has advanced, and some companies have sought exemptions from copyright laws in order to continue their operations. However, there is still a risk of litigation, as evidenced by discussions about potential lawsuits that could threaten the existence of AI models.  In contrast, SLMs present a more manageable solution regarding data handling and copyright issues. With SLMs, it’s easier to obtain licenses for training materials, ensuring that content creators are compensated for their work. This approach not only reduces legal risks, but also leads to better, more predictable model performance through the use of high-quality, ethically sourced data.  Data quality  Data quality is a crucial aspect of training LLMs, as it has a direct impact on the model’s performance. LMs require vast amounts of data that are representative of various languages and contexts. However, the available datasets are often unevenly distributed, with a disproportionate amount of data in English and a lack of representation for other languages and cultures. This imbalance can lead to biased models that may not perform well for non-English speakers.  The process of curating and refining this data to ensure it’s of high quality is labour-intensive and complex. It involves extensive cleaning and the use of advanced algorithms to weed out irrelevant or low-quality content. The task is crucial because poorly curated datasets can lead to models that are ineffective or behave unpredictably.  Moreover, the process of acquiring and labeling this data raises ethical concerns. In particular, some of the data used to train LLMs comes from controversial and potentially damaging internet sources, such as texts describing extreme violence or abuse. Labeling such content for machine learning purposes raises questions not only about the psychological effect on data labelers, but also about the ethical implications of using these datasets.  These ethical and quality-related challenges underline the need for better data management practices in the development of LLMs. Ensuring high-quality, ethically sourced data not only improves the performance of the models but also helps in building AI systems that are socially responsible and less harmful. It’s crucial for the AI community to address these issues head-on, developing standards that safeguard both the well-being of those in the data labeling process and the integrity of the data used.  How do SLMs stack up next to LLMs?  SLMs are streamlined counterparts to LLMs, characterized by smaller neural networks and simpler architectures. Let’s explore this further below:  Resource usage Firstly, SLMs excel in terms of resource efficiency, which is crucial when deploying AI solutions in environments with limited computational power. Due to their smaller number of parameters, SLMs require less memory and processing power to train and operate compared to LLMs, making them ideal for use in smaller devices or situations where quick deployment is essential. The simplicity of SLMs greatly aids in their development and deployment. Their smaller size and more streamlined neural networks make them easier for developers to manage, opening the door for their use in remote or edge computing scenarios, where maintaining large-scale data processing infrastructure would be impractical. Faster training cycles due to fewer tunable parameters further reduce the time from development to deployment, enhancing the feasibility of using SLMs in time-sensitive applications. Speed When it comes to performance speed, SLMs often have the upper hand due to their compact size. They typically have lower latency and can make faster predictions, which is important for applications that need real-time processing, like interactive voice response systems and real-time language translation. Additionally, SLMs benefit from faster cold-start times, meaning they can begin processing tasks more promptly after initialization compared to LLMs. This feature is particularly beneficial in environments where models need to be frequently restarted or deployed dynamically. Robustness Despite their smaller size, SLMs can be surprisingly robust, especially within their specific domains or tasks. Since they are often designed for particular applications, they can handle relevant data variations more effectively than LLMs, which might not perform as well when applied outside their primary training scenarios. The manageability of SLMs means they can be more easily monitored and modified to ensure they continue to operate reliably, which simplifies ongoing maintenance and enhances overall system stability. Security Security is another area where SLMs generally excel. With fewer parameters and a more contained operational scope, SLMs present a smaller attack surface compared to LLMs. This reduced complexity allows fewer opportunities for malicious exploits and simplifies the process of securing the models. By focusing on specific functionalities and smaller datasets, SLMs can achieve a higher level of security hardening, making them suitable for applications where data privacy and security are paramount. Other advantages Beyond resource usage and security, SLMs are often easier to tune due to their simplicity. Adjustments and optimizations can be made more rapidly, which is advantageous in dynamic environments where user needs or data inputs frequently change. This agility also extends to security practices, where the ability to quickly refine and adapt the models contributes to maintaining robust protection measures.  SLM Examples  Let’s explore some well-known SLMs:  DistilBERT: This model is a simplified version of the original BERT model. It has been designed to maintain around 95% of its predecessor’s capability with language comprehension tasks, such as the GLUE benchmark. With approximately half of the parameters of the BERT base model, DistilBERT offers a good balance between speed, efficiency and cost, making it suitable for use in resource-constrained environments. Although it may be slightly less accurate than larger models, its performance is still commendable considering its reduced size.  GPT-Neo: GPT-Neo is an open-source alternative to GPT-3, with similar architecture and capabilities. It has 2.7 billion parameters and is designed to provide high-quality results for a variety of language tasks without the need for fine-tuning. While GPT-Neo may not always perform as well as larger models, its effectiveness remains strong across a wide range of applications.  GPT-J: Similar to GPT-3 in design, GPT-J has 6 billion parameters and includes Rotary Position Embeddings and attention mechanisms. This model is effective for tasks such as translating from English to French, and it competes closely with the Curie version of GPT-3 (with 6.7 billion parameters). Interestingly, GPT-J outperforms the much larger GPT-3 Davinci model (with 175 billion parameters) in code generation.  Orca 2: Developed by Microsoft, Orca 2 has been fine-tuned with high-quality synthetic data to perform well in zero-shot reasoning tasks. Due to its smaller size, it may face challenges in tasks that require extensive knowledge or contextual depth, but it is specifically designed for high performance in logical reasoning and punches above its weight. .  Phi-2: Another innovative model from Microsoft, Phi-2, stands out with its impressive 2.7 billion parameters. This model is optimized for efficient training and adaptability, making it well-suited for a wide range of reasoning and understanding tasks. Despite its relatively small size, Phi-2 approaches near-human performance in language processing, outperforming much larger models like GPT-4 in terms of training efficiency. One thing to watch out for is that its effectiveness can depend on the representation of data used during the tuning phase.  Optimization via Intelligent Routing  Although SLMs are able to provide decent results at a lower cost, you will still need the power of an LLM or the assistance of another data source. In these situations you should employ a routing layer that redirects the query to an optimal source of information.  Routing modules manage and direct user queries within systems that involve multiple data sources and decision-making processes. Essentially, these routers function by receiving a user’s question along with a set of possible options, each tagged with specific metadata, then determining the most appropriate choice, or set of choices, in response. These routers are versatile and can be employed independently as selector modules, or integrated with other query engines or retrievers to enhance decision-making.  These modules leverage the capabilities of LMs to analyze and select among varied options. This makes them powerful tools in scenarios like selecting the optimal data source from a diverse range, or deciding the best method for processing information—be it through summarization using a summary index query engine, or semantic search with a vector index query engine. They can also experiment with multiple choices simultaneously through multi-routing capabilities, effectively combining results for more comprehensive outputs.  Routing optimizations also include strategic approaches to handling queries, to maximize efficiency and reduce operational costs.  For simpler and more predictable queries, caching mechanisms can be used to store and quickly access data without repeatedly contacting the LLM provider, thereby reducing response times and costs.  can be used to store and quickly access data without repeatedly contacting the LLM provider, thereby reducing response times and costs. Depending on the complexity of the question, the system may route the query to a different-sized model: Straightforward inquiries may be directed to smaller, less complex models, which can reduce the processing load and operational demands. More challenging questions can be directed towards more powerful, larger language models that are capable of handling complex queries.    Conclusion  SLMs can offer significant advantages over LLMs for many applications, especially where resource constraints and rapid deployment are crucial concerns.  SLMs not only require less computing power and energy, making them more environmentally friendly and cost-effective, but also provide faster processing times and easier scalability across diverse environments.  Additionally, their custom nature allows for more secure and robust implementations tailored to specific tasks, reducing the risks associated with large models. While LLMs have their place in handling complex, wide-ranging tasks, SLMs offer a compelling alternative that can be just as effective, if not more so, in contexts that demand efficiency, agility and focus. This makes SLMs particularly valuable in today’s fast-paced, resource-conscious world, where the balance of performance and practicality is crucial.","model, tasks, smaller, models, training, llms, small, exploring, data, slms, language, parameters",2024-04-24 10:00:00+01:00
84,Testing and Evaluating Large Language Models in AI Applications,Phil Winder,https://winder.ai/testing-evaluating-large-language-models-ai-applications/,"With the rapidly expanding use of large language models (LLMs) in products, the need to ensure performance and reliability is crucial. To get the most out of it, I recommend that you should be familiar with language models. But even though they are probably overkill, language models are still useful for traditional tasks, like classification. When it comes to language models, testing is hard because we want the output to be diverse and capable. If we constrain our analysis to just language models for a moment, what does this involve?","With the rapidly expanding use of large language models (LLMs) in products, the need to ensure performance and reliability is crucial. But with random outputs and non-deterministic behaviour how do you know if you application performs, or works at all?  This article and companion webinar offers a comprehensive, vendor-agnostic exploration of techniques and best practices for testing and evaluating LLMs, ensuring they meet the desired success criteria and perform effectively across varied scenarios.  Learn how to create test cases and how to structure prompts for evaluation  Discover a variety of evaluation techniques, from LLM-focussed metrics to extrinsic testing methodologies  Analyze evaluation results and ensure consistency and reliability across different inputs  Use a mental framework to help organize the different components  Target Audience  This webinar is perfect for those that are interested in or are already using LLMs within their systems. To get the most out of it, I recommend that you should be familiar with language models. If you need a quick recap, one of our previous introductory articles will help you.  Download Slides  Challenges and Failure Points of LLM Applications  Last month’s article about RAG use cases concluded with a slide about how hard it is to test and evaluate LLMs. The key challenge is that LLM predictions are inherently noisy. The output is intentionally random and non-deterministic. Human language is also so complex that it’s very hard to define what is “correct”.  To put it succinctly, testing LLMs is hard.  This article investigates how to test and evaluate LLMs in AI applications.  What are the Differences Between Testing and Evaluating LLMs?  Testing and evaluation are often conflated. But they have different purposes and target audiences. The differences are summarized in the table below:  Testing Evaluation Purpose Ensuring the system works Measuring the performance of the system Audience Application Developers Model Developers Focus Validity of a single use case General capability of the model Stability Locally robust Stable on average Variability High but across one domain Low but across many domains  The purpose of testing is to ensure that the system works. It’s focused on the validity of a single use case. It’s locally robust in the sense that it’s good at proving that the system works for a specific problem. It achieves this by testing the narrow system in a variety of ways.  Evaluation, on the other hand, is focused on measuring the capabilities of a model. It’s more general and is aimed at model developers. It’s stable on average, because model developers use benchmarks to direct their development. You can’t be sure that the model will perform well on your specific use case, because the evaluation is performed across very few tasks across many domains. However, it’s reasonable to assume that if you improve the model’s performance across the benchmarks, you will also improve its performance on your use case.  That’s not to say that testing and evaluation are mutually exclusive. They are complementary. Testing often uses evaluation metrics to measure performance to ensure the system works as expected.  To summarize:  Evaluation – Measuring the performance of a model against task-specific metrics, usually across many domains.  – Measuring the performance of a model against task-specific metrics, usually across many domains. Testing – Validity of task within a specific domain in varying conditions.  Now we can tackle each element in turn.  Evaluation  The goal of this section is to introduce you to the evaluation of LLMs, from the perspective of a developer building an application.  The first question you need to ask yourself is “what is the task?”  I’m not asking what the use case is here. In general use cases don’t have evaluation metrics because they often comprise of multiple tasks. E.g. a business chat bot. This is not true when it comes to testing, because tests are supposed to ensure that the system succeeds in the use case.  If you have a background in machine learning, you might be familiar with the concept of evaluation metrics. These are used to measure the performance of a model. But LLMs are different. They are used for higher-level, knowledge-based tasks, like question answering, summarization, and generation. These tasks are more complicated and require more nuanced evaluation metrics.  But even though they are probably overkill, language models are still useful for traditional tasks, like classification. Some traditional evaluation metrics include: accuracy, recall, precision, F-score, etc.  You can consider these metrics the low-level building blocks of evaluation.  The Problem With Traditional Metrics  But LLMs are mostly used for higher-level, knowledge-based tasks. These tasks require evaluation metrics that calculate the quality of the output as much as the correctness. If you dig into each of these metrics, you’ll find that they are related to low-level metrics. For example, relevance is related to recall and precision, correctness is related to accuracy, and so on. For example:  Question Answering : relevance, hallucination, correctness, etc.  : relevance, hallucination, correctness, etc. Summarization : alignment, coverage, relevance, etc.  : alignment, coverage, relevance, etc. Generation : functional correctness, syntactic correctness, etc.  : functional correctness, syntactic correctness, etc. and so on.  But these are still highly task specific. For example, consider the difference between generating JSON and SQL. The evaluation procedure for these two tasks would be very different. So while you could create metrics for each of these tasks, doing so for the sake of it isn’t scalable.  The Need For Aggregations  Since there’s so many ways that you could evaluate a response, aggregations of metrics have emerged to capture specific human-like traits or specific use cases.  For example:  BLUE : evaluates “quality” compared to a reference text, which is built from an average of precisions  : evaluates “quality” compared to a reference text, which is built from an average of precisions ROUGE: is an average recall-like metric for summarization  And then others then combine the aggregations into even higher level abstractions!  HellaSwag : evaluates for “common sense”  : evaluates for “common sense” TruthfulQA: evaluates for “truth”  And so on.  You end up with a tree of metrics that you could use. But they’re all rooted in the task.  Practical Approach  Ideally, you want to browse a list of all possible metrics. Then you rank and pick metrics from that list that correlate with your task. You could then use that to benchmark several different models over those metrics.  If you can’t you’ll need to create your own metrics. For example, for one of our recent clients the golden metric was “percent of predictions that were correct to within +/- 3 minutes” since in that domain, within three minutes was equivalent to perfection.  This changed the focus of the development of the model. We then concentrated more on outliers than we did on average performance.  If that doesn’t sound like fun, or if you’re short on time, then it might be easier to use off-the-shelf metrics. There’s a variety of tools that provide various industry standard metrics that you can plug in and use as a proxy.  This is a reasonable approximation because if you improve those metrics, it’s quite likely that you will also improve your business metric.  Here are a few evaluation frameworks that might help you in your journey:  https://github.com/confident-ai/deepeval - a collection of test-oriented helper methods to do evaluation  https://github.com/openai/evals - more of a benchmark, but has the code to do the evaluation  https://docs.arize.com/phoenix/evaluation/llm-evals - more of a platform, but again has the capability to do the evaluation  What Are LLM Benchmarks?  Benchmarks are combinations of metrics and testing data. Typically they include a variety of high-abstraction metrics like “truthfulness” and “task understanding” and a large number of human curated answers.  Benchmarks are typically aimed at people that are developing, training, or fine-tuning models. More and more people are fine-tuning models and benchmarks are a great way to ensure you’re not destroying the general capabilities of your base model.  LLM Leaderboards  Benchmarks are most commonly used to power LLM leaderboards. These help you decide which base model is better.  But benchmarks are publicly available, which means that they are available to the model developers too. All of the models at the top of the leaderboards are likely to have been trained upon the benchmarks that are trying to evaluate them.  The benchmarks attempt to fight back with obfuscations and randomisation, but the general idea is that public evaluations gamify model development.  Hugging face has a leaderboard that presents many metrics for opens source language models.  The Need For Human Evaluations  The problem of overfitting the benchmarks has lead to a new era of human-powered performance benchmarks which pit one model against another in a battle. A human (we assume!) then gets to decide which model has given the best response. The results are aggregated using an Elo rating system to produce a final “skill level” metric.  LMSYS Chatbot arena is one of the most famous implementations of this. You can see the current LLM leaderboard here. As of writing, GPT-4o just pips Claude 3.5 and Gemini-1.5 with a score of 1287.  For reference, Llama-3-8b-Instruct is in 31st place with a score of 1152, just behind GPT-4-0613 . This is my most commonly used general purpose open source model that I use in my work.  The problem with this metric is that it’s too course and it’s not expressive enough to distinguish capability. The metric is used to provide a ranking, not measure any objective performance measure. For example, what does it mean for Llama-8b to be “10” behind GPT-4 . Is that good? Or is that a long way off?  Evaluation Conclusion  Evaluation is rooted in the choice of metric you choose for your particular task. But use cases are often more complicated that simply achieving the best average performance for a particular task. You may have non-functional requirements regarding resource usage or latency. And the use case may have constraints that you must ensure.  Evaluation techniques are only appropriate for measuring aggregate performance. When developing an application you should be constraining the LLM to ensure it does what your use case requires it to do, and no more. Robustness is the key driver here and is not only task specific, but use case specific to boot.  Testing  To recap, testing aims to ensure that a system works as intended. It’s much more bespoke to your use case and domain. It is unlikely that there are publicly available test suites for you to use. When it comes to language models, testing is hard because we want the output to be diverse and capable. Testing that is not as simple as checking for equality.  Prompt Driven Development  To test a traditional software application you would write scenarios that must succeed, otherwise it is considered broken.  This is a simple rubric that sadly does not work with AI applications because the underlying data is inherently noisy. Instead, you must test a range of scenarios to increase your confidence that the solution is most likely working.  Engineers and managers alike often squirm at this definition because it leads to the obvious conclusion that your application is broken at least some of the time. Trying to communicate that to your users is difficult. An uncertain world produces uncertain results.  Still, you can develop your AI application in such a way to give you a high degree of confidence that it’s working well enough. And that often leads to usefulness and therefore value. So all is not lost.  If we constrain our analysis to just language models for a moment, what does this involve?  Prompt Engineering and Testing  Language models have a surprisingly simple interface. Text in, text out. Your goal is to ensure the text out produces the expected results for a given input. Sounds simple right? Wrong.  The input is unconstrained. Your input could be the sum of all human knowledge, which means the output could literally be anything.  The input is typically split into two key components: the system prompt and the user prompt. The system prompt is fully under your control. You potentially have some control over the user prompt but that is typically left to the mercy of an external system.  Constraining the user prompt is an undervalued technique. This could be as simple as some user experience that restricts what or how the user is expressing the data. For example you could constrain it to input only JSON. A more sophisticated approach might be to transform or mutate the input so that it better conforms to what you are expecting. You could even use another language model as a kind of gatekeeper.  This means that most of your time is spent refining the system prompt to pre-empt and counteract the damage the user could do.  You could develop your countermeasures through trial and error. But in the long term you should be properly testing your prompts and ideally developing using test driven (prompt) development.  How to Decide What to Test  One initial struggle with both software and language models is deciding what to test.  Like in software, I (controversially) like to develop the application/prompt first to gain ad-hoc experience. Only then do I go back to the use case and imagine how the user might use it.  This is because I often find that something pops out that I hadn’t thought of. There might be technical limitations, the problem might be unviable, or more likely, it highlights problems or a lack of a problem definition. If I don’t do this I often find that I spend a fair amount of time writing tests only for them to go unused.  Once you have honed in on the problem definition and the use case, then you need to generate some examples.  Ideally you want your users to do this for you, so you don’t add your biases. Quite often that’s not possible. An LLM is also often quite good and helping you generate or find alternative use cases.  Things You Might Want To Test  The tests that are right for your application entirely depend on your use case. To help you get started, the list below presents a selection of tests that might be important to you:  latency  contains X (e.g. a string, some json/sql, a name, etc.)  confidence in its own output – a.k.a. logprobs greater than or perplexity less than  factuality (usually via another model)  relevance/recall (usually via another model, but you could use a direct metric)  toxicity/harm/moderation (as above)  similarity  calls a function  resultant code is syntactically correct/compiles/works/passes/etc.  classifies correctly  does not contain personally identifiable information  These are known as test properties. To test a property you use an eval designed to evaluate that property. An eval is a metric designed to test a specific property.  Note that all of this is very closely related to monitoring. Everything we are talking about here could be applied to monitoring too. You should look at consolidating both approaches so you don’t repeat yourself too much. See related topics for more discussion.  LLMs Are Better At Classification Than They Are Generation  Language models are better at classification than generation. The reason for this, in a nutshell, is that the the task is much narrower in scope compared to generation. And narrow problems are easier to solve. Thinking of this in reverse, generating an example of a class requires a large amount of contextual information and planning. So unless you have a highly detailed and prescriptive prompt, it’s difficult for a language model to generate perfect examples.  I haven’t found a good citation for this, but I have seen this mentioned.  What this means is that even though a language model might not be great at doing the job you really want it to do, it probably is good enough at judging the output. There are exceptions to this of course. Just make sure you consider how hard the task is. The easier it is for you to do, the easier it will be for the language model.  In summary, you can use a language model to evaluate the result of a test.  How to Test Prompts  Now we have all the tools to build a suite of tests to help develop and test your prompt. To do this begin by:  Convert the business problem definition into distinct use cases For each use case, split it into tasks. Quite often there is only one task per use case, but more complicated use cases may require multiple tasks. For each task, test properties that fulfil the requirements of the task. For each property, create a test input to use in a test case. Use a language model to generate examples.  Organize these test cases in a hierarchy to help you manage them.  Now that you have a list of test cases to evaluate, decide on the best mechanism to run it. Ideally, tests should be as fast as possible, because they directly affect development velocity.  Using code to do the evaluation is the fastest way of achieving this. This is perfect for properties that don’t require any interpretation like latency. But many of the more interesting properties require a more comprehensive assessment.  Human evaluations are the gold standard, but don’t scale well. You might want to consider doing a few as a smoke test.  The slowest, but most flexible automated approach is to use another language model to programmatically evaluate whether a test case passes.  Anthropic have a nice notebook example of these three methods.  Double Down on Failures  For every failure, try to improve the system as a whole to make that test pass. This might involve improving the system prompt or it could be a matter of mutating the input. Either way, the idea here is to slowly improve your solution to fulfil your requirements.  If you do have persistent failures, it’s worth digging down to find more examples that error that are slightly different. So for each failure, ask a language model to generate another example that your system will fail on.  You can also prompt the language model to tell you why it’s failing. Use that information to make your system even more robust.  Ultimately you will want to test a range of these these properties in a systematic way. There’s a few tools out there that help you do that, but they’re all basically domain specific libraries or languages that allow you specify an input and the expected output.  The summary in the previous section was inspired by Microsoft’s adaptive testing app (AdaTest). This is an OpenAI tool with a frontend to help you manage and develop tests.  PromtFoo is a very similar tool and equally as useful.  If you’re building applications, not just prompts, you probably also want to write your own scripts to run your tests in an automated fashion, for CI/CD purposes. Keep your script and tests close to your codebase and easy (and fast!) to run.  You’ll obviously need access to a large language model and a playground to do prompt development. You can take advantage of a hosted provider or build a solution yourself from the likes of: langchain, llamaindex, something for end-user-focused like Chainlit, or something more MLOps-y like Arize.  Related to the playground idea is the use of spreadsheets to organize prompt experiments and API connections to commercial model providers. For example, claude or gpt for sheets for Google Sheets.  If you’re part of a larger team, then you might want to start unifying or organizing your prompts centrally with something like PromptLayer or W&B Prompts.  Conclusion  Evaluation is most useful for those that are developing, training, or fine-tuning models. You can use benchmarks to ensure that, on average, your model performs well, or at least doesn’t perform much worse.  But if you are building a production application on top of a language model, then you need to have confidence that it works. You can do this by developing tests that take a wide range of user inputs and evaluating the output.  Continuous Integration  The logical extreme of both of these is a CI pipeline. The first stage in the pipeline evaluates the actual model you use in production to ensure that performance doesn’t change over time due to model updates or benchmark changes. The second stage tests your use case against that model to ensure that the system as a whole (including system prompts, input guards, etc.) works as expected.  This process is not cheap, both in terms of development and execution time, but it offers unparalleled production quality confidence in your application.  Related Topics  I decided not to include monitoring to reduce the scope of the article. But there’s obvious parallels between evaluating and testing offline whilst building an application and doing something similar online when the application is in production.  I’ll leave this for another day but here’s one link that begins to discuss the nuances when attempting to evaluate online.  Further Reading","model, evaluating, models, applications, evaluation, case, metrics, system, test, prompt, testing, language, large, ai",2024-07-17 16:30:00+01:00
85,How to Win at AI - Insurances at the Forefront of the Data Revolution,,https://www.linkedin.com/pulse/ai-insurance-industry-winners-losers-coming-revolution-kainz,"Machine learning and Artificial Intelligence (AI) sound futuristic and lead to associations of programs that develop consciousness. In reality machine learning and AI are firmly rooted in statistical mathematics. The whole operation had only 50 employees in 2018, as receipts are also analyzed and handled by machine learning software. Reducing Customer Service Costs with ChatbotsFacing rising demands for cost-cutting, insurances are turning to AI and machine learning for customer facing interfaces. According to an investigation by the markup Allstate insurance used machine learning to find the maximum premium they could charge.","Machine learning and Artificial Intelligence (AI) sound futuristic and lead to associations of programs that develop consciousness. In reality machine learning and AI are firmly rooted in statistical mathematics. Insurances have employed these methods to calculate risk longer than computers existed.  But Machine learning combines statistical techniques these with innovations such as neural networks, natural language processing and big data. That opens up new and fascinating use-cases. In this article I’ll examine 5 ways how to use machine learning and AI in a way that profits insurance and customer and one that doesn’t.  Read below to see how to use machine learning and AI for the profit of the customer and how to use it to maximize profit and face regulatory and customer backlash.  Health Insurance Pools  Alibaba the Chinese e-commerce giant turned its payment juggernaut AliPay into ANT Financial to enable financial services beyond payments.  Only 5 years later, ANT Financial has a valuation of $150 billion and is the most valuable startup in the world.  Among payments and investments, ANT Financial offers an insurance product that uses machine learning to offer health insurance at a fraction of traditional insurances.  Launched in 2018, by the end of 2019 the product, called Xiang Hu Bao, already had a 100 million users. Xiang Hu Bao is not an actual health insurance but a claims pool that offers protection against 100 types of critical illnesses.  Members of the pool get their medical expenses paid, up to 300,000 RMB ($45.000), when falling critically ill. The amount is shared equally by members of the pool. This means when an incident occurs, members of the pool pay after the fact. Disputes are managed by volunteers that vote on the validity of the claim.  This product is not a traditional insurance, but it fills a similar niche, financial protection in the case of serious illnesses. With low overhead costs and transparent pricing, as fees are set a 8% management fee.  To increase the quality of the pool members, the pool requires a credit score of 650 or more to join. The score, called Zhima or Sesame Credit, is calculated using machine learning from “hundreds of sources — ranging from purchases on Alibaba’s Taobao marketplace to subway fares”.  The whole operation had only 50 employees in 2018, as receipts are also analyzed and handled by machine learning software.  For end users the end result is an health insurance that is more affordable than other options. Capped at RMB 188 ($27) per month, this opens up health insurance for poorer people. More than two thirds of the pool participants make less than 100,000 RMB ($14.000).  Reducing Customer Service Costs with Chatbots  Facing rising demands for cost-cutting, insurances are turning to AI and machine learning for customer facing interfaces.  The major touch points with the customer are branches, call centers, web sites, and mobile apps.  Branches and call centers are costly as they require personal and real estate.  Web sites and mobile apps are impersonal and require some technology affinity when used.  To solve this conundrum insurances are building what are called conversational interfaces.  Well known conversational interfaces are Apple Siri, Amazon Alexa, and Google Assistant.  While some insurances rely on textual chatbots others use voice-activated bots similar to the well-known examples above.  But current chatbots are facing criticism, they “have no memory” they offer “Cold Experiences” and “Low error tolerance”.  Chatbot looks great in demos, if the natural text follows the playbook. When typing in “I want a health insurance for my trip to Italy” leads to an answer “A trip to Italy is always a good idea”.  But misspell Italy as Italie and the chatbot is stumped.  Also the storyboard like process of most chatbots may lead to a loop, if the question doesn’t fit into the current storyboard. This leads to the frustrating experience that a lot of people associate with automated menu systems.  Often chatbots are used as a complicated way to search the Frequently Asked Questions (FAQ) or a set of prepared responses.  But with new AI-driven platforms like Google Dialogflow and Amazons Lex, chatbots may be able to solve much more complicated requests. The experience these companies have with Google Assistant and Amazons Alexa, will allow a new generation of chatbots. With sophisticated machine learning algorithms and deep pockets for research, this new generation will likely allow more natural conversations.  For example KLM is offering an chatbot to book flights based on Google Dialogflow. It is whimsical and doesn’t take itself to serious which makes it fun and less frustrating to interact with. This encourages people to explore more than a strict automated phone menu in chatbot form. It seems to be also better at handling misspellings and deviations from the storybook, while still keeping a memory of the conversation.  Optimize Risk prediction by Analyzing Telematics  Fairness in insurance means charging every customer the exact premium related to the risk (and cost) of a claim. The higher the potential for a payment the higher the premium. In a fair insurance the uncertainty would be zero and everyone would pay the exact premium to match the risk. In reality however there is always a level of uncertainty.  Car insurances use any number of factors to find the correct risk level for an individual. Factor such as age, driving history, marital status and so on. But while each factor reduces the level of error, there still remains a lot of uncertainty. Maybe a 25 year old single man is a careful and safe driver, but due to his age and marital status he will be seen as high risk.  Some insurances have offered reducing premiums, if the customer is willing to share his behavior on the road. For example State Farm, the largest property and casualty insurance provider in the United States offers a program called Drive Safe and Save. The program tracks and analyses driver behavior on the road. This way the company does not have only have to rely on statistical data, but can directly relate the driver behavior to his risk. That way the careful 25 year old can get a quote that is related to his personal risk level and not the risk level of his peers.  State Farm says that the customer receives up to 5% discount on his insurance just for signing up and “some customers could see a discount of up to 50%.”.  One reason why insurances can offer 5% is the process of self-selection. Just like the people how respond to a Nigerian spam mail are more likely to fall for it, the people who sign up for telematics are more likely to be safe drivers.  In general a driver would consider itself a safe-driver to sign up. That’s the 5% signup bonus. Only 5% because of the everyone-else-on-the-road-is-an-idiot- syndrome(76% of Americans think they are good drivers while 93% commit bad driving behaviors). For the rest of the insurance discount, machine learning anaylzes the driving behavior and produces a risk profile for the customer. Currently that is mostly based on the usage of the car, which is why this model is also called Usage Based Insurance or (UBI). But better model can produce risk profiles that are exactly tailored to the driver in the future.  Not everyone is excited about having every move on the road being tracked by the insurance. A hack on the size of Equifax could expose the driving data of millions of people. Even the most secure data may be used for law enforcement or state security purposes.  Data privacy concerns aside, telematics result in fairer insurance pricing for all the safe and the unsafe drivers.  Predict Estimated Car Damages with “The Box”  Munich Re is the 2nd largest re-insurance company in the world. Munich Re specializes on insuring insurances. By taking risk out of other insurances hands, the risk to the individual insurance company is reduced. To accurately estimate the risk, Munich Re models risk for floods, earthquakes but also pandemics and cyber crime.  These models are a perfect fit for data science and machine learning.  “In the past we had statisticians and mathematicians, that build our models. Now we need data scientists and data engineers.”  One of the areas that will change dramatically is motor insurance. Not only because of telematics or usage based solution, but also because of assisted driving technologies. “94% of vehicle crashes are due to human arror. 80% of these crashes may be reduces or eliminated by automation.”  Munich Re provides machine learning tools to the customer facing insurers for motor insurance. “The Box” can be used by these insurance companies to accurately estimate taking into account the changing motor insurance market. It can take weather information, accident statistics or socio-economic information into account.  “[Data analytics and Artificial intelligence mean] faster claims estimates and handling, and better pricing as a result of improved accuracy in risk assessment. Not to mention better loss prevention in the first place.”      Preventing Fraudulent Claims  According to the Coalition against Insurance Frau “Conservatively, fraud steals $80 billion a year across all lines of insurance”.  Machine Learning and Artificial intelligence are ideally suited to find hidden patterns in data and help with Fraud prevention.  A human analyst or a human reviewer can only look at a handful of signals at a time and make a determination. But there is enough data out there and that’s really when machine learning comes into play. Because it’s literally able to crunch thousands of signals and look at probabilities of abuse or probabilities or fraud. That’s really where the industry is going from a machine learning viewpoint.  The Insurtech 100 2019 list lists 6 vendors that are tacking fraud with machine learning and artificial intelligence. Among them the Shift Technology at number 4 and the only fraud prevention vendor in the top 10.  FWD, is the leading Pan-Asean insurance provider with offices in Hong Kong, Malaysia, Singapore, Indonesia, Japan, Vietnam, the Philippines and Thailand.  FWD Group has an artificial intelligence and machine learning center in Singapore. Since the end of 2019, FWD Singapore is using Shift Force for fraud detection and is the first user in Southeast Asia. According to Shift, FORCE achieves a 75% fraud detection rate versus the industry standard of 35%.  Overcharge The Customers Least Likely to Leave  Earlier examples in this article show how insurance companies use machine learning to model the risk as exactly as possible. Both, insurance and customer benefit, the better the risk model the fairer the pricing.  Allstate, one of the largest insurance providers in the US however has used machine learning for a not so customer friendly purpose.  According to an investigation by the markup Allstate insurance used machine learning to find the maximum premium they could charge. The insurer claimed to examine each of the individual policies to make sure that the rates are still accurate.  [We] found that, despite the purported complexity of Allstate’s price-adjustment algorithm, it was actually simple: It resulted in a suckers list of Maryland customers who were big spenders and would squeeze more money out of them than others.  To measure the likeliness of a customer leaving, it seems the algorithm looked at how much the customers were already being charged and calculated the risk of leaving to be less if the customer was already paying a huge premium.  And according to the markup the algorithm did not seem to be equally distributed  In Maryland, seniors were overrepresented among those customers who were owed discounts but would not have gotten them. Allstate proposed giving those Maryland customers over the age of 62 a median discount of $1.64, far less than many deserved, according to its new risk calculations.  This illustrates a danger of using machine learning for the customers. They can be opaque and hard to understand. Some algorithms require knowledge and understanding of statistics. Other algorithms, such as neural networks, are black boxes that make ithard to explain the results even for experts.  Conclusion  Used in the customers interest, machine learning and AI can benefit both the customer and the insurance company. By opening health insurance up to lower income populations through health insurance pools, by improving customer service through chatbots, by pricing car insurance fairer through telematics and by preventing fraud. But used in the interest solely of the insurance company the use of these techniques can lead to maximal extraction of profit from the customer. The danger of using machine learning in that way is a backlash from customers and regulators that is in no one’s interest. Customers already are weary about privacy issues and feeling reluctance towards artificial intelligence. Insurances will be well advised to make sure that machine learning and AI are used transparently and create value, but not to extract maximum profit.","revolution, insurances, machine, customers, fraud, risk, customer, forefront, win, data, insurance, learning, used, ai",
86,Announcement: AI alignment prize winners and next round — LessWrong,,https://www.lesserwrong.com/posts/4WbNGQMvuFtY3So7s/announcement-ai-alignment-prize-winners-and-next-round,,,"announcement, prize, lesswrong, alignment, round, winners, ai",
87,AI that picked Oscar winners could predict the next US president,,http://www.engadget.com/2016/06/01/ai-that-picked-oscar-winners-could-predict-the-next-president/,"Now the ""artificial swarm intelligence"" is hosting its first AMA on Reddit, where it will respond to questions pertaining to the US presidential elections. Unlike robotic AIs that are being built to emulate the human brain, UNU works with existing human intelligence instead of replicating it. UNU's human-based group intelligence follows the natural workings of insects like bees that tend to find solutions in swarms. ""They negotiate in real time through body vibrations and converge on a solution for the whole group,"" Rosenberg tells Engadget. While the human swarm system sounds safer than its robotic peers, its accuracy and superiority has yet to make an impact beyond the predictive format in which it's being tested.","There's strength -- and intelligence -- in numbers. Unanimous A.I., a Silicon Valley startup, has built a platform that taps into the collective knowledge of a group of people to form its own opinions, preferences and surprisingly accurate predictions. The software, dubbed UNU, successfully guessed last year's Oscar winners (11 out of 15 categories) and most recently predicted the winning horses in the Kentucky Derby. Now the ""artificial swarm intelligence"" is hosting its first AMA on Reddit, where it will respond to questions pertaining to the US presidential elections.  Unlike robotic AIs that are being built to emulate the human brain, UNU works with existing human intelligence instead of replicating it. The platform, which is open to the public, allows a group of people to converge on an answer in real time. While a swarm of seven predicted the Oscars, the Derby decision came from 20 people. For the AMA at 1 p.m. EDT today, the group that will make political predictions is expected to range from 100 to 200 people.  The participants will come from UNU's user base, which has already answered fantasy football and cooking queries. The decision-making process, which will be live on YouTube through the AMA, is straightforward. When the users sign in, they are presented with a question and its potential answers. The group has 60 seconds to drag a puck toward a chosen answer with a graphical magnet. While each participant can see only his or her own puck, the group arrives at a collective decision that best represents the intelligence of the swarm.  UNU's human-based group intelligence follows the natural workings of insects like bees that tend to find solutions in swarms. There's a lot to learn from these hard-working honey producers that thrive in togetherness, live in well-organized colonies and work in groups to find solutions for their community. As supporting evidence from nature, Louis Rosenberg, CEO of Unanimous A.I., points to a study that looked into the decision-making tactics of honeybees.  Around early summer every year, a couple of hundred bee scouts split from a hive of thousands in search of a new home. When they spot a branch in a tree or a hole in the wall with the potential to make a good nest, they bring that information back to the colony, where instead of casting buzz votes, the bees break into a ""waggle dance"" to communicate their preferences.    Based on the bodily movements, the swarm reaches a decision. ""They negotiate in real time through body vibrations and converge on a solution for the whole group,"" Rosenberg tells Engadget. ""Biologists have found that bees will pick the optimal site almost 80 percent of the time. No individual bee can understand the question or find and evaluate the site. But when a group of bees form swarm intelligence, it functions as a brain of brains. It [operates] like a neural network.""  Like the bees that swarm or birds that flock, the artificial intelligence of UNU is based on the idea that people can be smarter together. ""Nature has shown that by allowing groups of similar organisms to function in a closed-loop system they can become smarter than any single individual in that system,"" says Rosenberg. ""In fact, it becomes a super-organism."" The collective wisdom gives the swarm the ability to make decisions, have opinions and even take on a unique personality.  Since the system relies entirely on human knowledge and even instinct, it's easy to think of it as a kind of crowdsourcing platform for opinions and intelligence. But according to Rosenberg, UNU doesn't work like a poll or a survey that finds the average of the opinions in a group. Instead, it creates an artificial swarm that amplifies a group's intelligence to create its own. For instance, when predicting the Derby winners, the group picked the first four horses accurately to win $11,000 in a grand bet called Superfecta. But when asked individually to make the same predictions, none of the participants had more than one winning horse.  ""Collectively, they make an expert,"" says Rosenberg. ""The individuals who participate in the swarm don't have the answers that the swarm does. They got one horse right on an average. But even though no single individual in that swarm came close, the swarm was right.""  Experiments in predictions, like picking Derby winners or Trump's potential running mate, validate the premise of UNU: The intelligence of the swarm is greater than the sum of its participants. It builds the evidence for the effectiveness of collective thinking. But it also highlights the fact that an intelligent system can be built with human knowledge at its core.  ""We see swarm intelligence as a way to build systems that keep humans in the loop,"" says Rosenberg. ""Unlike AI, [where] we're creating an intelligence from scratch that we have no reason to believe will share human values or interests or even act in ways that we as humans understand. When we're building artificial neural networks we're building an alien intelligence. But with swarm intelligence, we're amplifying what's most human about us rather than replacing it. UNU inherently has human values and interests in the system.""  While the human swarm system sounds safer than its robotic peers, its accuracy and superiority has yet to make an impact beyond the predictive format in which it's being tested. But if the idea sticks and takes on a large number of global users, UNU eventually could turn into a ""superexpert."" Rosenberg presents the idea of a medical swarm of doctors that can accurately diagnose a disease. ""Instead of relying on a single expert you'd have a group of experts who can collectively come up with answers that are significantly more insightful than individuals working on their own,"" he says. ""What we'll see is the ability to democratize expertise.""","president, rosenberg, oscar, swarm, unu, picked, system, predictions, group, winners, intelligence, predict, opinions, bees, human, ai",2016-06-01 00:00:00
88,A machine-learning system may have predicted the World Series winner,,https://www.engadget.com/2019/10/22/Swarm-AI-World-Series-2019-winner-picks/,"Back East, the Astros won out against the Yankees for American League dominance and a ticket to the World Series. You may remember the Silicon Valley startup Unanimous AI from 2016, when its human-augmented AI platform, UNU, correctly guessed 11 of 15 Academy Award winners. In 2018, researchers from Stanford University School of Medicine teamed up with Unanimous AI to study whether the system could help improve pneumonia diagnosis. Of course, the Swarm AI system is not infallible, especially when predicting the outcome of a game as quirky as baseball. Each of these factors contributed to an outcome that neither sports fans nor machine-learning systems could have predicted.","The 2019 MLB postseason has been a wild one. The Dodgers imploded in the first round, clearing the way for the Washington Nationals to rout a listless Cardinals team and win their first NLCS since 1933. Back East, the Astros won out against the Yankees for American League dominance and a ticket to the World Series. All of these upsets have made the postseason a joy for fans but a nightmare for the gambling community. However, one company thinks it has the answer: Combine the collective wisdom of crowds with modern machine-learning methods into a symbiotic sports-betting hive mind.  You may remember the Silicon Valley startup Unanimous AI from 2016, when its human-augmented AI platform, UNU, correctly guessed 11 of 15 Academy Award winners. The company followed that feat in 2017 when it successfully predicted the top four winners of that year's Kentucky Derby -- a Superfecta that returned 540-to-1 odds. This year, the company has turned its predictive services toward Major League Baseball.  ""We're really very different than how other companies work,"" Unanimous founder and chief scientist Dr. Louis Rosenberg told Engadget. ""Most AI companies and researchers are focused on replacing people with algorithms, where they do deep learning on big data sets and find patterns in the data."" Unanimous, conversely, puts humans at the center of the decision making.  ""We want people to be part of the process,"" he said, ""because people have knowledge and wisdom and insight and intuition that does not exist in just the raw data. People also have a really good sense of the context. ... They have intuition that's just not present if you're just looking at the historical data.""  To generate a prediction, the company gathers a bunch of serious fans -- for the MLB postseason, they recruited 50 -- and connects them in real-time through the company's Swarm AI platform, which as you can see above, looks like a cross between a Ouija board and a web poll.  This platform enables the participants to combine their knowledge and intuition, becoming a ""super expert"" capable of making predictions with a far higher degree of accuracy than any one of them could do alone.  Specifically, the system works through an online portal where group members can log on with a PC or mobile device. When prompted, the group attempts to collectively move a cursor towards a displayed answer to the question while the AI's algorithms interpret each person's actions to determine their conviction and assign relative weight to their input.  Using the previous Kentucky Derby results as an example, Rosenberg explains that ""as a swarm, [these participants] all have different views and different perspectives ... some people feel like the most important factor is the jockey, other people think that the weather conditions and other people think that the pole position is most important."" Rosenberg also points out that none of the 20 horse-racing fans recruited for the project correctly guessed the four winning horses on their own. Only through combining their expertise were they able to correctly predict the Superfecta.  Rosenberg likens this effect to the group decision-making processes found in nature. It's ""the reason why birds flock or fish school or bees swarm,"" he said. ""They become significantly smarter together. You can almost think of it as this invisible tug of war, where they're all pulling in different directions and they're converging on one answer that basically combines the strength of their conviction.""  Interestingly, this effect can be scaled only so far. If you can get improved predictive results from 100 people, then using 1,000 people should be even better, right? Not so, says Rosenberg. Turns out that this system sees quickly diminishing returns as more members move to the group. But even with small numbers, this system can provide powerful insights.  In 2018, researchers from Stanford University School of Medicine teamed up with Unanimous AI to study whether the system could help improve pneumonia diagnosis. The study recruited five radiologists and had them decide whether patients had pneumonia based on a series of chest X-rays.  ""When doctors worked together, we reduced errors [by] 33 percent compared to diagnoses done by doctors alone, and [by] 22 percent compared to a Stanford software called CheXNet,"" Rosenberg told the Stanford Daily.  Of course, the Swarm AI system is not infallible, especially when predicting the outcome of a game as quirky as baseball. When Engadget interviewed Dr. Rosenberg, only the first game of divisional series (four games in total, two from each league) had been played.  ""We see that the most likely winner in the American League is the Astros, the most likely winner in the National League is the Dodgers,"" Rosenberg said at the time. ""And we see that the most likely winner if they face each other in the World Series, we see the Astros win it all.""  ""We're currently 75 percent accurate,"" he continued. ""There's only been four games played, but we're 75 percent accurate in predicting those four games.""  Unanimous has also since revised its estimates throughout the playoffs, accounting for upsets. While the Nats' have seen a 24 percent jump (from 6 percent to nearly 30 percent) in their chances of winning the Series since the start of the playoffs, the system still holds a comfortable 33 percent chance of taking home the Commissioner's Trophy this year.  But nobody could have foreseen the Dodgers' spectacular first-round upset to a wild card team after dominating the regular season with 104 wins or have seen Clayton Kershaw choke in game 5 of the NLDS coming (even though he keeps doing it), or even predict the impact of the MLB ""unjuicing"" balls used in the postseason. Each of these factors contributed to an outcome that neither sports fans nor machine-learning systems could have predicted.  Still, Rosenberg remains confident of the system's potential. ""We've been predicting the whole baseball season so far,"" he said. ""We track ourselves against Vegas, which is what everybody cares about, and anyone who's been following our pics would have a positive ROI for the season against Vegas.""  Rosenberg doesn't see either Swarm AI or traditional machine-learning systems wiping out the other in a VHS vs BetaMax format fight, but rather each occupying a complementary niche. ""There are really good powerful uses of machine-learning,"" he said. ""The issue that, while the technology of machine-learning works well, the data that you need to train the system doesn't exist, or it does exist, but gets out of date very quickly.""  Swarm AIs on the other hand, rely on a continuously updated database of the participants' experience. ""That database is really good at solving many problems, especially issues that involve human sentiment or human perspective,"" Rosenberg said. ""It solves a different class of problems that the traditional systems aren't good at.""","machinelearning, unanimous, rosenberg, different, predicted, swarm, system, world, league, systems, winner, series, ai",2019-10-22 00:00:00
89,Announcing the 2019 AI for Good Idea Challenge winners,Mitra Azizirad,https://blogs.microsoft.com/ai-for-business/2019/07/29/ai-for-good-challenge-winners/,"Announcing the 2019 AI for Good Idea Challenge winnersThanks to developers, the world runs on software! To that end, in 2018 Microsoft created the initial AI Idea Challenge to explore how developers were applying AI in meaningful and fascinating ways. Voilá, a new developer challenge called the AI for Good Idea Challenge was born! Great winning projects from our first AI Idea Challenge, like Angel Eyes and Clean Water AI, helped to inspire more developers than ever to get involved and think up new ways to leverage Microsoft AI to tackle societal issues. We were incredibly fortunate to have four stellar judges for our first AI for Good Idea Challenge who evaluated the projects and had the tough job of selecting the winners.","Announcing the 2019 AI for Good Idea Challenge winners  Thanks to developers, the world runs on software!  Developers have played a strong role in many of the massive transformations and technology shifts of the past few decades. And now that AI is redefining how software is created with the capability to learn through data and experiences and to perceive the world around us through vision, speech and understanding, we’re excited to see a whole new level of transformation and innovation coming to the forefront. We love learning from and showcasing the cool developers behind these innovations.  To that end, in 2018 Microsoft created the initial AI Idea Challenge to explore how developers were applying AI in meaningful and fascinating ways. As we explored the projects that poured in, it was clear that developers were incredibly inspired by the ability to use AI to positively impact society. This desire is directly aligned with our vision to use AI to empower people to take on some of society’s toughest issues. Voilá, a new developer challenge called the AI for Good Idea Challenge was born!  This challenge is focused on using developer creativity and skills specifically around AI for Good scenarios. Great winning projects from our first AI Idea Challenge, like Angel Eyes and Clean Water AI, helped to inspire more developers than ever to get involved and think up new ways to leverage Microsoft AI to tackle societal issues. I have been amazed by the sheer numbers and pure creativity of the ideas from developers across the globe. Ready to be inspired? Take a look at the highlight video to get an idea of the breadth of ideas that came in.  YouTube Video Click here to load media  To help us select winners of the AI for Good Idea Challenge, we brought together a team of judges to evaluate each entry against three criteria: originality of impact, complexity to implement and solution feasibility. With such passion and creativity represented in work of these talented developers, it was very hard to pick the winners. My deepest gratitude goes out to all who entered. Thank you!  And now, I’m thrilled to share the winners of the AI for Good Idea Challenge:  First place: CardioVision, by Bohdan Petryshak and his student team from Ukrainian Catholic University, is a solution that can help patients at risk of coronary artery disease live a better life. This AI-guided web application will assist doctors to localize and classify the stenosis score on the CT scans and MPR images of the coronary artery. By detecting artery defects and disease up to three times faster, patients are equipped for better heart attack prevention.  Second place: LeafAI, by Maanasa Mendu, identifies 38 classes of biotic plant disease from a basic smartphone picture – with an astounding 90% accuracy rate. The homogeneity of the current agriculture system combined with the effects of climate change has led to a growing threat of plant disease, contributing to malnutrition, which affects 700,000 people around the world. LeafAI’s technology can help identify plant diseases and provide information about treatment, taking us one step closer to better economic and food security.  Third place: OrganSecure, by Pratik Mohapatra, is a sophisticated set of machine learning algorithms that can quickly match organ donors and provide real-time updates with people in need of a transplant. Using health parameters such as blood group and antigen type, it becomes possible to predict the match of an organ and estimate the rank and time required for an awaiting recipient. Not only would this help people waiting for organs, but it would also make the host-donor matching process more transparent.  You can learn more about the winning projects here.  We were incredibly fortunate to have four stellar judges for our first AI for Good Idea Challenge who evaluated the projects and had the tough job of selecting the winners. Thank you to our judges: Stephen Ibaraki, Wendy Chisholm, Alma Cardena and David Carmona.  CONGRATULATIONS to each of our winners! Thank you to all the developers who took the time to share and submit their great ideas for the challenge. We can’t wait to do it again!  Related:","announcing, 2019, idea, projects, world, developers, help, challenge, disease, winners, good, ai",2019-07-29 00:00:00
90,Microsoft’s AI for Accessibility grant winners: ‘You want to be seen as the person you are’,".Wp-Block-Co-Authors-Plus-Coauthors.Is-Layout-Flow, Class, Wp-Block-Co-Authors-Plus, Display Inline, .Wp-Block-Co-Authors-Plus-Avatar, Where Img, Height Auto Max-Width, Vertical-Align Bottom .Wp-Block-Co-Authors-Plus-Coauthors.Is-Layout-Flow .Wp-Block-Co-Authors-Plus-Avatar, Vertical-Align Middle .Wp-Block-Co-Authors-Plus-Avatar Is .Alignleft .Alignright, Display Table .Wp-Block-Co-Authors-Plus-Avatar.Aligncenter Display Table Margin-Inline Auto",https://news.microsoft.com/features/microsofts-ai-for-accessibility-grant-winners-you-want-to-be-seen-as-the-person-you-are/,"And why his organization, Our Ability, is among seven new recipients of Microsoft’s AI for Accessibility grants to people using AI-powered technology to make the world a more inclusive place. The one-year grants provide use of the Azure AI platform through Azure compute credits and can also include Azure compute credits plus engineering-related costs. AI for Accessibility has three focus areas: communication and connection; employment; and daily life. “You want to be seen as the person you are, in total, not just the shell that you are on the outside,” he says. The research being done by all of the AI for Accessibility grantees “is an important step in scaling accessible technology across the globe,” says Bellard of Microsoft.","John Robinson was born without the extensions of his arms or legs. As a child, and an adult, he rejected wearing prostheses. They worked, but they were uncomfortable, and he never felt like himself with them on. And that’s all Robinson really wanted to be, just himself.  It’s why he understands the frustrations many other people with disabilities face in trying to get people to see who they really are – especially when it comes to looking for employment. And why his organization, Our Ability, is among seven new recipients of Microsoft’s AI for Accessibility grants to people using AI-powered technology to make the world a more inclusive place.  In 2018, when the $25 million program was announced, nine organizations were given grants to work on a variety of projects, some from scratch, some already underway.  The new grantees, announced in conjunction with Global Accessibility Awareness Day May 16, are: University of California, Berkeley; Massachusetts Eye and Ear, a teaching hospital of Harvard Medical School; Voiceitt in Israel; Birmingham City University in the United Kingdom; University of Sydney in Australia; Pison Technology of Boston; and Our Ability, of Glenmont, New York.  Their projects may differ, but the people behind them share a passion for how technology can improve the lives of their fellow human beings.  “What stands out the most about this round of grantees is how so many of them are taking standard AI capabilities, like a chatbot or data collection, and truly revolutionizing the value of technology in typical scenarios for a person with a disability like finding a job, being able to use a computer mouse or anticipating a seizure,” says Mary Bellard, Microsoft senior accessibility architect.  The one-year grants provide use of the Azure AI platform through Azure compute credits and can also include Azure compute credits plus engineering-related costs. AI for Accessibility has three focus areas: communication and connection; employment; and daily life.  Robinson of Our Ability knows about all of those. But it was his employment journey after college that always remained with him. He went through a discouraging, 4-1/2-year search to find a job – the right job. During those years, he sent out hundreds of resumes, and had 20 to 25 interviews for work in ad sales at TV stations. He also married and started a family.  “If I were really going to give up, I would have given up after the first 10 or 20 interviews,” he says. “That’s not who I am.”  He vowed, someday, to find a way to improve the process for others who have disabilities. “You want to be seen as the person you are, in total, not just the shell that you are on the outside,” he says.  After working at TV stations for many years, Robinson founded Our Ability in 2011 to bring businesses with employment opportunities together with people with disabilities looking for jobs.  Now, with the AI for Accessibility grant, and working with students from Syracuse University, Our Ability wants to create an accessible and intuitive AI-powered chatbot to help businesses find workers, and to help people with disabilities find employment that’s meaningful to them.  “So many of them are taking standard AI capabilities, like a chatbot or data collection, and truly revolutionizing the value of technology in typical scenarios for a person with a disability.”  And with good reason: The unemployment rate among people with disabilities is about twice as high – 7.9 percent – as for those without disabilities, according to the U.S. Department of Labor’s Office of Disability Employment Policy (ODEP). Robinson says the rate is actually closer to 65 percent when you factor in another significant number: Only one out of every five people with disabilities is in the labor force, according to ODEP.  Of course, human job coaches are helpful, Robinson says, but even some of them still tend to look at the disability first, and not the person.  “The internet is a more level playing field” that way, he says. “The individual with a disability will talk to the chatbot about who they really are, and maybe more importantly, who they want to be.”  Our Ability has 20,000 unique users that visit its website every year, but not all of them go through the manual process of filling out the forms on the site; the chatbot will help with that, Robinson says. It will also help individuals “find the skills they need for the jobs that they want.”  The Our Ability website now includes an area where individuals can build their profile, companies can search the database, and individuals can search job openings, and “they can find each other organically that way, or they can reach out to us and we can make an email introduction as best we can for opportunities that we know about,” he says. That’s not ideal, though.  The chatbot, he says, “will provide a much more rapid way of getting more people to connect with one another. By creating a place where we assess real-life skills, train real-life skills and match them with employment – that’s every disability job coach’s goal in the last 50 years,” he says.  “We’re going to be able to do it with technology a lot faster and a lot better.”  Every day, Dexter Ang was growing more frustrated as he watched his mother deal with the indignities of ALS, amyotrophic lateral sclerosis. She was diagnosed with it in 2014.  The progressive disease attacks nerve cells that control muscles throughout the body. Over time – for many, anywhere from a year to decades – it robs people of their ability to walk, to use their arms and hands, to talk and ultimately, to breathe independently.  Ang had been working in Chicago in the financial world of high-frequency trading before his mother was diagnosed. He decided to move to Boston to help take care of her. Over the course of a year, there were fewer activities she could do with her hands. Eventually, she couldn’t use utensils to eat. She couldn’t dress herself. And she couldn’t maneuver the mouse to use her laptop, which she relied on for reading e-books from the library.  “I asked her when the last time was that she had read a book, and she said six weeks — because she couldn’t click a mouse,” Ang says. “That just made me tremendously sad, because that was one of the only things that she could still enjoy, and that was just gone.”  Ang, a graduate of Massachusetts Institute of Technology (MIT) with a degree in mechanical engineering, spent months meeting with experts and poring over information about existing technologies to see if there were any that could help his mother.  “Ultimately, a lot of it was not useful — it was complicated and not well-designed,” he says. He credits his mother for inspiring him with a question she asked: What if a person could use their nerve signals to help control a mouse?  Ang wanted to learn more, returned to MIT as a graduate student and co-founded Pison Technology. The company, one of the AI for Accessibility grantees, is developing a nerve-sensing wearable, similar in appearance to a watch, to control digital devices using small, micro-movements of the hands and arms.  “Our proprietary technology can sense nerve signals on the surface of the skin,” Ang says. “Our machine-learning algorithms can classify those voltage signals into discernable actions,” such as simulating a mouse click to help interact with a computer.  In 2016, the ALS Association awarded an ALS Assistive Technology prize to Ang and Pison co-founder David Cipoletta.  “People are looking for products or services to make things easier, and AI might be able to help.”  “They blew the judges away with their easy-to-use, self-contained communication system,” the ALS Association said on its blog. “People living with ALS are able to learn and use the system to communicate in minutes. We observed first hand as participants (and) were thrilled with its comfort and usability while testing out their technology.”  The device, which is now undergoing testing, can help improve communication for people with other neuromuscular disabilities, including multiple sclerosis, Ang says. He wants it to be made widely available, around the world, at a low cost, and easy to purchase online. He believes his mother, who died in 2015, would be proud of the work he and his team have done.  When a person’s “physical world shrinks” because of a disease like ALS, for many “the digital world is the only world that is still out there for people to express themselves, and to connect with others,” Ang says. “To be able to maintain and increase access to that digital world is exceptionally important for people with disabilities.”  For people with epilepsy, one of the biggest dangers is having a seizure while driving. In some places, patients need to prove they’ve been seizure-free for a year in order to be allowed behind the wheel, which can create psychological and economic stress of its own.  It’s a problem that got Omid Kavehei thinking: What if there was a way to warn drivers who have epilepsy that a seizure could be coming, so that they have time to safely pull off the road?  Kavehei, a senior lecturer at the Faculty of Engineering and Information Technologies, and his colleagues at the University of Sydney – also a new AI for Accessibility grantee – are working on a potential way to help. They have been using deep learning to develop an analytical tool that can read a person’s electroencephalogram (EEG) data via a wearable cap, then communicate that data back and forth to the cloud to provide seizure monitoring and alerts.  It’s a very new approach to a very old disorder. Awareness and writings about epilepsy date back to ancient times, but so do myths and social stigma about it that continue to persist.  “Some people think epilepsy is contagious — it’s not — and we hear stories of misbelief and misguidance on social media almost every day,” says Kavehei. “I’ve read it in tweets — ‘My child can’t be enrolled in such-and-such primary school because there’s a student there with epilepsy.’ We are living in 2019, but still, you can’t believe some of the stories you hear.”  More than 50 million people worldwide live with epilepsy, making it one of the most common neurological disorders globally, according to the World Health Organization.  “Some people think epilepsy is contagious — it’s not — and we hear stories of misbelief and misguidance on social media.”  “To have a non-surgical device available for those living with epilepsy will make a significant difference to many, including family members, friends, and of course those impacted by epilepsy,” says Carol Ireland, CEO of Epilepsy Action Australia, which is among the groups working with the university on the project.  “Such a device would take away the fear element of when and if a seizure may occur, ensuring that the person living with epilepsy can get into a safe place quickly.”  Kavehei and his colleagues want to first test a wearable cap on epilepsy patients using driving simulations. They will leverage Azure Machine Learning to attempt to predict seizures from human signals.  The research being done by all of the AI for Accessibility grantees “is an important step in scaling accessible technology across the globe,” says Bellard of Microsoft. “People are looking for products or services to make things easier and AI might be able to help.”  Lead image: Leanne Strong talks with John Robinson at a recent job fair hosted by CDPHP and Living Resources in Albany, New York. (Photo by Scott Eklund/Red Box Pictures)","job, ability, technology, seen, world, ai, disabilities, help, microsofts, person, winners, epilepsy, accessibility, robinson, grant",
